{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IBM Business Automation Open Edition 8.0 Enablement Your Journey Begins Here Get the tools First Kogito Project Setup IBM Business Automation Open Edition 8.0 Decision Manager Exercises Learn DMN Implement a On-push CI/CD IBM Business Automation Open Edition 8.0 Process Automation Exercises Getting started with IBAMOE Event-driven processes More topics Extra Learning IBAMOE on OpenShift About the guides Getting Started with IBAMOE: Order Management A great guide for users who are trying IBAMOE for the first time. Recommended getting started guide. Learn DMN In this hands-on workshop, users can learn about the DMN specification, author decisions, and deploy it using IBM Decision Manager with basic, intermediate and advanced exercises. Event-driven processes This hands-on workshop allows the user to validate the experience of creating loan approval workflow with decision automation. The user will also have an introduction to Fuse, develop an endpoint and consume it using the workflow. Learn the tools In this hands-on workshop, create new decision services using Kogito tooling and the VSCode extension , create unit tests and deploy them on KIE Server. Learn how to deploy IBAMOE on OpenShift using Operators Try out the business automation operator and learn how to manage your KIE Apps on OpenShift. Check the Learn more section for more guides and references about business automation with the projects under the KIE umbrella.","title":"IBM Business Automation Open Edition 8.0 Enablement"},{"location":"#ibm-business-automation-open-edition-80-enablement","text":"","title":"IBM Business Automation Open Edition 8.0 Enablement"},{"location":"#your-journey-begins-here","text":"Get the tools First Kogito Project Setup","title":"Your Journey Begins Here"},{"location":"#ibm-business-automation-open-edition-80-decision-manager-exercises","text":"Learn DMN Implement a On-push CI/CD","title":"IBM Business Automation Open Edition 8.0 Decision Manager Exercises"},{"location":"#ibm-business-automation-open-edition-80-process-automation-exercises","text":"Getting started with IBAMOE Event-driven processes","title":"IBM Business Automation Open Edition 8.0 Process Automation Exercises"},{"location":"#more-topics","text":"Extra Learning IBAMOE on OpenShift","title":"More topics"},{"location":"#about-the-guides","text":"","title":"About the guides"},{"location":"#getting-started-with-ibamoe-order-management","text":"A great guide for users who are trying IBAMOE for the first time. Recommended getting started guide.","title":"Getting Started with IBAMOE: Order Management"},{"location":"#learn-dmn","text":"In this hands-on workshop, users can learn about the DMN specification, author decisions, and deploy it using IBM Decision Manager with basic, intermediate and advanced exercises.","title":"Learn DMN"},{"location":"#event-driven-processes","text":"This hands-on workshop allows the user to validate the experience of creating loan approval workflow with decision automation. The user will also have an introduction to Fuse, develop an endpoint and consume it using the workflow.","title":"Event-driven processes"},{"location":"#learn-the-tools","text":"In this hands-on workshop, create new decision services using Kogito tooling and the VSCode extension , create unit tests and deploy them on KIE Server.","title":"Learn the tools"},{"location":"#learn-how-to-deploy-ibamoe-on-openshift-using-operators","text":"Try out the business automation operator and learn how to manage your KIE Apps on OpenShift. Check the Learn more section for more guides and references about business automation with the projects under the KIE umbrella.","title":"Learn how to deploy IBAMOE on OpenShift using Operators"},{"location":"more/","text":"Learn more Videos KIE Live events are live streams designed to facilitate knowledge sharing about the Business Automation topic, including business rules, decisions, processes, resource planning, tooling, and AI. They're community events and anyone is welcome to attend. Check out all the KIE Lives at: https://red.ht/kielives Red Hat Scholars You can refer to Red Hat Scholars to find several up-to-date guides about business automation topics, including Kogito. Check out all the great guides available at: https://redhat-scholars.github.io/cloud-native-business-automation/ Guides There are several guided exercises and workshops that are not yet part of this guide. You can find them below: Loan Approval Workshop with DMN This workshop is aimed at providing hands on experience creating DMN assets. This lab will implement a Loan Approval workflow. Check out this guide here . Loan Approval Workshop This workshop is aimed at providing hands-on experience creating Decision and Process Assets. This lab will implement a Loan Approval workflow. Check out this guide here . KIE Learning: IBAMOE and RHDM This repository is a set of explanation and hands-on labs which you can try on your environment and follow at your own pace. The step-by-step guides covers different topics, since basics to more advanced and specific features. The content is not sequential and it covers Decision Manager, Business Optimizer, Process Automation Manager and Kogito. DMN Handbook This handbook is a vademecum for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine.","title":"Learn more"},{"location":"more/#learn-more","text":"","title":"Learn more"},{"location":"more/#videos","text":"KIE Live events are live streams designed to facilitate knowledge sharing about the Business Automation topic, including business rules, decisions, processes, resource planning, tooling, and AI. They're community events and anyone is welcome to attend. Check out all the KIE Lives at: https://red.ht/kielives","title":"Videos"},{"location":"more/#red-hat-scholars","text":"You can refer to Red Hat Scholars to find several up-to-date guides about business automation topics, including Kogito. Check out all the great guides available at: https://redhat-scholars.github.io/cloud-native-business-automation/","title":"Red Hat Scholars"},{"location":"more/#guides","text":"There are several guided exercises and workshops that are not yet part of this guide. You can find them below: Loan Approval Workshop with DMN This workshop is aimed at providing hands on experience creating DMN assets. This lab will implement a Loan Approval workflow. Check out this guide here . Loan Approval Workshop This workshop is aimed at providing hands-on experience creating Decision and Process Assets. This lab will implement a Loan Approval workflow. Check out this guide here . KIE Learning: IBAMOE and RHDM This repository is a set of explanation and hands-on labs which you can try on your environment and follow at your own pace. The step-by-step guides covers different topics, since basics to more advanced and specific features. The content is not sequential and it covers Decision Manager, Business Optimizer, Process Automation Manager and Kogito. DMN Handbook This handbook is a vademecum for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine.","title":"Guides"},{"location":"guided_exercises/00_get_tools/env-setup/","text":"Environment Setup This section will cover the deployment of some of the tools locally that you will use in these exercises. IBM Business Automation Open Edition 8.0 In order to follow these guided labs, you should have IBAMOE 8.0+ in your local environment. IBAMOE is flexible in how you design. The developer runtime tools are predominantly focused in VSCode (download here ) with Maven builds targeted at a more cloud native strategy. There are also runtime options available through the utilization of the KIE Server (Knowledge is Everything) to execute. This enablement session will go through both, but the evolution of the jBPM/Drools projects are definitely more towards the Kogito runtimes versus the Java EE-based KIE Server. If you need to setup IBAMOE locally, you can use this repository to help you get up and running quickly: IBM Business Automation Open Edition 8.0 Environment Setup . This will provide the files as two different forms, a locally built environment that will persist, or the option to have an ephemeral container that will lose the data stored between sessions. Both forms will provide a Business Central and KIE Server environment for use. VS Code For running through the exercises, it is highly recommended that you get Visual Studio Code as this is the IDE that IBM Business Automation Open Edition 8.0 most supports for activities regarding both decisions and processes. To get Visual Studio Code go to the Visual Studio Code download page and download for your platform. Git, Maven and Java These labs are going to assume you are already running , a running version of git, Maven 3.6.2+ and OpenJDK 11+. To get these added to your environment, you can follow the steps in this section. IBM Business Automation Open Edition 8.0 is built around git source code control and Maven archetypes. You will need these tools to do most of the content in these quick walk throughs. Git Git is the open-source solution for source code management. To get git, go to here to install it if you don't have it already. All of the major source code management systems work with the git command line and is ultimately platform-agnostic to those. Maven With IBM Business Automation Open Edition 8.0, the components are built around a Maven architecture predominantly. What this ultimately means is your workstation needs to be able to communicate with one to many different Maven Repositories. These labs will use two in particular, the Red Hat General Availability repository and Maven Central . You could easily replace the two repositories with a local environment one hosting the Maven dependencies as a mirror or based in a disconnected installation, but this is the easiest developer workflow for acquiring new dependencies. The reason we are pointing to the Red Hat Maven repository, at least in the short term, is that the builds for IBM Business Automation Open Edition 8.0 are being deployed there as they are the same binaries used within both the IBM and Red Hat products during the transition of Red Hat Process Automation Manager (RHPAM)/Red Hat Decision Manager (RHDM) from Red Hat into IBM Automation under the name of IBM Business Automation Open Edition 8.0 ( IBAMOE). The settings.xml file included below will use the local Maven repository at your USER_HOME /.m2/repository, which when configuring Maven would be the default M2_HOME that's created. Example settings.xml file <?xml version=\"1.0\" encoding=\"UTF-8\"?> <settings xmlns= \"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\" > <localRepository> ${user.home}/.m2/repository </localRepository> <interactiveMode> true </interactiveMode> <usePluginRegistry> false </usePluginRegistry> <offline> false </offline> <profiles> <!-- Profile with online repositories required by IBAMOE --> <profile> <id> brms-bpms-online-profile </id> <repositories> <repository> <!-- Red Hat Maven Repository--> <id> jboss-ga-repository </id> <url> https://maven.repository.redhat.com/ga/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> jboss-ga-plugin-repository </id> <url> https://maven.repository.redhat.com/ga/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> <profile> <id> maven-https </id> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <!--Maven Central Repository--> <id> central </id> <url> https://repo1.maven.org/maven2 </url> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> central </id> <url> https://repo1.maven.org/maven2 </url> <snapshots> <enabled> false </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> </profiles> <activeProfiles> <!-- Activation of the BRMS/BPMS profile --> <activeProfile> brms-bpms-online-profile </activeProfile> <activeProfile> maven-https </activeProfile> </activeProfiles> </settings> Linux/Windows/Mac Follow the instructions at the Maven Community to download and update your path variables to incorporate it into your builds. Example Linux installation Download Maven 3.8.6 from here If using the Skytap image, change to the downloads folder. cd /home/pamadmin/Downloads Now you need to extract the tar tar xzvf apache-maven-3.8.6-bin.tar.gz Now if there's an existing Maven in your stack, you can move it out with this command. sudo mv /usr/share/maven /usr/share/maven-old Now move the new download into your /usr/share with the following command. sudo mv apache-maven-3.8.6 /usr/share/maven With this, Maven should be updated, validate with running a Maven version command. mvn -v Your console should return a log similar to below [pamadmin@host-1 maven]$ mvn -version Apache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) Maven home: /usr/share/maven Java version: 11.0.16.1, vendor: Red Hat, Inc., runtime: /usr/lib/jvm/java-11-openjdk-11.0.16.1.1-1.el8_6.x86_64 Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"4.18.0-372.26.1.el8_6.x86_64\", arch: \"amd64\", family: \"unix\" Mac Alternative The easiest way to acquire Maven is to use homebrew and run the command brew install maven , which at the time of writing will install Maven 3.8.6. Java The assumption in these exercises are that you will utilize a supported JDK. These were tested with openjdk 11.0.11 2021-04-20 , but other ones are supported. You should be using at least Java 11 with IBM Business Automation Open Edition 8.0 to ensure your best experience with the tooling. Kafka The Kafka setup requires docker-compose to run locally, to get this capability, you can use either Docker or Podman for this capability. To see which is right for you, read this article . Coming later in 2022 will be the labs shifted to using an OpenShift deployment of AMQ Streams to minimize the local installations required. Event-driven processes can react to the events that happens in the ecosystem. Kafka is an open-source even streaming platform, and currently, one of the most popular tools. In this type of architecture, we have the Kafka topics used as the communication layer in between the services. Each service can now be considered a consumer or a producer , in other words, each service can publish or consume events to/from the topics . IBM supports the integration between IBAMOE and AMQ Streams (Kafka). To follow the labs, you should have an accessible Kafka server. The KIE Server (process engine) will communicate with the topics that we will create in the Kafka server. If you don't have an environment available you can get a Kafka (Strimzi) server quickly running by using Docker. Let's clone the project to the enablement folder. Create a new folder named enablement and access it: mkdir ~/enablement && cd ~/enablement Clone this repository to your local machine. git clone https://github.com/hguerrero/amq-examples The docker-compose file available in this quickstart should bootstrap everything you need to have your Strimzi up and running: Zookeeper, Kafka server v2.5.0, Apicurio Registry and a Kafka Bridge. Access the amq-examples/strimzi-all-in-one/ folder: cd amq-examples/strimzi-all-in-one/ Start the Kafka environment: docker-compose up Docker will download the images and start the services for you. You now have a Kafka server running on localhost port 9092. Creating the Kafka topics In the upcoming labs we will need three topics: incoming-requests , requests-approved and requests-denied . Next, you need to create these topics in your Kafka server. If you are using the containerized option mentioned in this lab, you can create the topics using the kafka-topics.sh available in the kafka container. Open a new terminal and let's using the kafka container we have just started. Enter the Kafka folder: cd ~/enablement/amq-examples/strimzi-all-in-one / Create the following three topics: docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic incoming-requests docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-approved docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-denied Next steps At this point you should have IBAMOE, and Kafka on your environment. Before proceeding to the next steps, make sure you have both IBAMOE and Kafka server up and running.","title":"Environment Setup"},{"location":"guided_exercises/00_get_tools/env-setup/#environment-setup","text":"This section will cover the deployment of some of the tools locally that you will use in these exercises. IBM Business Automation Open Edition 8.0 In order to follow these guided labs, you should have IBAMOE 8.0+ in your local environment. IBAMOE is flexible in how you design. The developer runtime tools are predominantly focused in VSCode (download here ) with Maven builds targeted at a more cloud native strategy. There are also runtime options available through the utilization of the KIE Server (Knowledge is Everything) to execute. This enablement session will go through both, but the evolution of the jBPM/Drools projects are definitely more towards the Kogito runtimes versus the Java EE-based KIE Server. If you need to setup IBAMOE locally, you can use this repository to help you get up and running quickly: IBM Business Automation Open Edition 8.0 Environment Setup . This will provide the files as two different forms, a locally built environment that will persist, or the option to have an ephemeral container that will lose the data stored between sessions. Both forms will provide a Business Central and KIE Server environment for use.","title":"Environment Setup"},{"location":"guided_exercises/00_get_tools/env-setup/#vs-code","text":"For running through the exercises, it is highly recommended that you get Visual Studio Code as this is the IDE that IBM Business Automation Open Edition 8.0 most supports for activities regarding both decisions and processes. To get Visual Studio Code go to the Visual Studio Code download page and download for your platform.","title":"VS Code"},{"location":"guided_exercises/00_get_tools/env-setup/#git-maven-and-java","text":"These labs are going to assume you are already running , a running version of git, Maven 3.6.2+ and OpenJDK 11+. To get these added to your environment, you can follow the steps in this section. IBM Business Automation Open Edition 8.0 is built around git source code control and Maven archetypes. You will need these tools to do most of the content in these quick walk throughs.","title":"Git, Maven and Java"},{"location":"guided_exercises/00_get_tools/env-setup/#git","text":"Git is the open-source solution for source code management. To get git, go to here to install it if you don't have it already. All of the major source code management systems work with the git command line and is ultimately platform-agnostic to those.","title":"Git"},{"location":"guided_exercises/00_get_tools/env-setup/#maven","text":"With IBM Business Automation Open Edition 8.0, the components are built around a Maven architecture predominantly. What this ultimately means is your workstation needs to be able to communicate with one to many different Maven Repositories. These labs will use two in particular, the Red Hat General Availability repository and Maven Central . You could easily replace the two repositories with a local environment one hosting the Maven dependencies as a mirror or based in a disconnected installation, but this is the easiest developer workflow for acquiring new dependencies. The reason we are pointing to the Red Hat Maven repository, at least in the short term, is that the builds for IBM Business Automation Open Edition 8.0 are being deployed there as they are the same binaries used within both the IBM and Red Hat products during the transition of Red Hat Process Automation Manager (RHPAM)/Red Hat Decision Manager (RHDM) from Red Hat into IBM Automation under the name of IBM Business Automation Open Edition 8.0 ( IBAMOE). The settings.xml file included below will use the local Maven repository at your USER_HOME /.m2/repository, which when configuring Maven would be the default M2_HOME that's created. Example settings.xml file <?xml version=\"1.0\" encoding=\"UTF-8\"?> <settings xmlns= \"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\" > <localRepository> ${user.home}/.m2/repository </localRepository> <interactiveMode> true </interactiveMode> <usePluginRegistry> false </usePluginRegistry> <offline> false </offline> <profiles> <!-- Profile with online repositories required by IBAMOE --> <profile> <id> brms-bpms-online-profile </id> <repositories> <repository> <!-- Red Hat Maven Repository--> <id> jboss-ga-repository </id> <url> https://maven.repository.redhat.com/ga/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> jboss-ga-plugin-repository </id> <url> https://maven.repository.redhat.com/ga/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> <profile> <id> maven-https </id> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <!--Maven Central Repository--> <id> central </id> <url> https://repo1.maven.org/maven2 </url> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> central </id> <url> https://repo1.maven.org/maven2 </url> <snapshots> <enabled> false </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> </profiles> <activeProfiles> <!-- Activation of the BRMS/BPMS profile --> <activeProfile> brms-bpms-online-profile </activeProfile> <activeProfile> maven-https </activeProfile> </activeProfiles> </settings>","title":"Maven"},{"location":"guided_exercises/00_get_tools/env-setup/#linuxwindowsmac","text":"Follow the instructions at the Maven Community to download and update your path variables to incorporate it into your builds.","title":"Linux/Windows/Mac"},{"location":"guided_exercises/00_get_tools/env-setup/#example-linux-installation","text":"Download Maven 3.8.6 from here If using the Skytap image, change to the downloads folder. cd /home/pamadmin/Downloads Now you need to extract the tar tar xzvf apache-maven-3.8.6-bin.tar.gz Now if there's an existing Maven in your stack, you can move it out with this command. sudo mv /usr/share/maven /usr/share/maven-old Now move the new download into your /usr/share with the following command. sudo mv apache-maven-3.8.6 /usr/share/maven With this, Maven should be updated, validate with running a Maven version command. mvn -v Your console should return a log similar to below [pamadmin@host-1 maven]$ mvn -version Apache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) Maven home: /usr/share/maven Java version: 11.0.16.1, vendor: Red Hat, Inc., runtime: /usr/lib/jvm/java-11-openjdk-11.0.16.1.1-1.el8_6.x86_64 Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"4.18.0-372.26.1.el8_6.x86_64\", arch: \"amd64\", family: \"unix\"","title":"Example Linux installation"},{"location":"guided_exercises/00_get_tools/env-setup/#mac-alternative","text":"The easiest way to acquire Maven is to use homebrew and run the command brew install maven , which at the time of writing will install Maven 3.8.6.","title":"Mac Alternative"},{"location":"guided_exercises/00_get_tools/env-setup/#java","text":"The assumption in these exercises are that you will utilize a supported JDK. These were tested with openjdk 11.0.11 2021-04-20 , but other ones are supported. You should be using at least Java 11 with IBM Business Automation Open Edition 8.0 to ensure your best experience with the tooling.","title":"Java"},{"location":"guided_exercises/00_get_tools/env-setup/#kafka","text":"The Kafka setup requires docker-compose to run locally, to get this capability, you can use either Docker or Podman for this capability. To see which is right for you, read this article . Coming later in 2022 will be the labs shifted to using an OpenShift deployment of AMQ Streams to minimize the local installations required. Event-driven processes can react to the events that happens in the ecosystem. Kafka is an open-source even streaming platform, and currently, one of the most popular tools. In this type of architecture, we have the Kafka topics used as the communication layer in between the services. Each service can now be considered a consumer or a producer , in other words, each service can publish or consume events to/from the topics . IBM supports the integration between IBAMOE and AMQ Streams (Kafka). To follow the labs, you should have an accessible Kafka server. The KIE Server (process engine) will communicate with the topics that we will create in the Kafka server. If you don't have an environment available you can get a Kafka (Strimzi) server quickly running by using Docker. Let's clone the project to the enablement folder. Create a new folder named enablement and access it: mkdir ~/enablement && cd ~/enablement Clone this repository to your local machine. git clone https://github.com/hguerrero/amq-examples The docker-compose file available in this quickstart should bootstrap everything you need to have your Strimzi up and running: Zookeeper, Kafka server v2.5.0, Apicurio Registry and a Kafka Bridge. Access the amq-examples/strimzi-all-in-one/ folder: cd amq-examples/strimzi-all-in-one/ Start the Kafka environment: docker-compose up Docker will download the images and start the services for you. You now have a Kafka server running on localhost port 9092.","title":"Kafka"},{"location":"guided_exercises/00_get_tools/env-setup/#creating-the-kafka-topics","text":"In the upcoming labs we will need three topics: incoming-requests , requests-approved and requests-denied . Next, you need to create these topics in your Kafka server. If you are using the containerized option mentioned in this lab, you can create the topics using the kafka-topics.sh available in the kafka container. Open a new terminal and let's using the kafka container we have just started. Enter the Kafka folder: cd ~/enablement/amq-examples/strimzi-all-in-one / Create the following three topics: docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic incoming-requests docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-approved docker-compose exec kafka bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic requests-denied","title":"Creating the Kafka topics"},{"location":"guided_exercises/00_get_tools/env-setup/#next-steps","text":"At this point you should have IBAMOE, and Kafka on your environment. Before proceeding to the next steps, make sure you have both IBAMOE and Kafka server up and running.","title":"Next steps"},{"location":"guided_exercises/00_get_tools/introduction/","text":"Introduction In this section, we will look at the different toolings that are available with IBM Business Automation Open Edition 8.0. These will be both from what is in the community (and soon to be enterprise supported) in the KIE Sandbox to setting up your local workstation with the VSCode Plugins, Maven, GraalVM and more. This section will cover Business Central as it is what is enterprise supported (and what most customers will already be using), but our goal with this enablement will be to move the product towards Kogito and its Cloud Native strategies over the legacy runtimes found within KIE Server and Business Central.","title":"Introduction"},{"location":"guided_exercises/00_get_tools/introduction/#introduction","text":"In this section, we will look at the different toolings that are available with IBM Business Automation Open Edition 8.0. These will be both from what is in the community (and soon to be enterprise supported) in the KIE Sandbox to setting up your local workstation with the VSCode Plugins, Maven, GraalVM and more. This section will cover Business Central as it is what is enterprise supported (and what most customers will already be using), but our goal with this enablement will be to move the product towards Kogito and its Cloud Native strategies over the legacy runtimes found within KIE Server and Business Central.","title":"Introduction"},{"location":"guided_exercises/01_getting_started/01_deploy_local/","text":"Deploying the Project locally as a Quarkus runtime Now that we have seen a little of the DMN Decision, let's use the combined power of Quarkus ( Learn more here about the Kubernetes-native Java Stack that is the best place for Kogito to run! ) and Kogito to quickly build and deploy the service using Maven commands and also having the option to deploy it to OpenShift as well. The first thing we're going to do is open a terminal (either in VSCode or whatever method you prefer) and validate that you have Maven and Java installed. java -version Expected output should be similar to openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment AdoptOpenJDK-11.0.11+9 (build 11.0.11+9) OpenJDK 64-Bit Server VM AdoptOpenJDK-11.0.11+9 (build 11.0.11+9, mixed mode) mvn -version Expected output should be similar to: Apache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) Java version: 18.0.2, vendor: Homebrew, runtime: /opt/homebrew/Cellar/openjdk/18.0.2/libexec/openjdk.jdk/Contents/Home Maven home: /opt/homebrew/Cellar/maven/3.8.6/libexec Default locale: en_US, platform encoding: UTF-8 OS name: \"mac os x\", version: \"12.6\", arch: \"aarch64\", family: \"mac\" Now that our environment has the appropriate resources, let's build and run the service locally. Quarkus does this very simply with the command mvn quarkus:dev which will operate the container in development mode. By default it will run on 2 ports when running this command, 8080 and 5005 . These ports can be maintained in the application.properties file in src/main/resources/ or you can modify the mvn quarkus:dev command to load those properties from the command line. If you want to do the command line, you can do mvn quarkus:dev -Dquarkus.http.port=8085 -Ddebug=7007 which would setup your application to run off of port 8085 and be able to attach a remote debugger at 7007 . To change the default host in application.properties add the following line. quarkus . http . port = 8085 After the project's application.properties has been updated, let's start the Quarkus service by running the following command. This command will run the remote debugger (which we are not using in these labs) on port 6006 so that if you already have another application running on 5005 (the default port) it won't give an error. mvn quarkus:dev -Ddebug = 6006 When the service is ready, it will produce a log similar to the below: $ quick-kogito % mvn quarkus:dev -Ddebug=6006 [INFO] Scanning for projects... [INFO] [INFO] --------------------< com.ibm.sample:quick-kogito >--------------------- [INFO] Building quick-kogito 1.0.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- ... 2022-09-29 20:48:34,404 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Initializing DMN DT Validator... 2022-09-29 20:48:34,405 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) DMN DT Validator initialized. 2022-09-29 20:48:34,405 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Analysing decision tables in DMN Model 'pricing' ... 2022-09-29 20:48:34,410 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) analysis for decision table 'Base price': 2022-09-29 20:48:34,412 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Decision Table Analysis of table 'Base price' finished with no messages to be reported. 2022-09-29 20:48:34,494 INFO [org.kie.kog.qua.com.dep.KogitoAssetsProcessor] (build-38) reflectiveEfestoGeneratedClassBuildItem org.kie.kogito.quarkus.common.deployment.KogitoGeneratedSourcesBuildItem@415815e1 __ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\\ \\ --\\___\\_\\____/_/ |_/_/|_/_/|_|\\____/___/ 2022-09-29 20:48:35,728 INFO quick-kogito 1.0.0-SNAPSHOT on JVM started in 2.432s. 2022-09-29 20:48:35,728 INFO Listening on: http://localhost:8085 2022-09-29 20:48:35,728 INFO Profile dev activated. Live Coding activated. 2022-09-29 20:48:35,728 INFO Installed features: [cdi, kogito-decisions, kogito-predictions, kogito-processes, kogito-rules, resteasy-reactive, resteasy-reactive-jackson, smallrye-context-propagation, smallrye-openapi, swagger-ui, vertx] Using the project that is created Now that your application is created, you can start using it more for live development and change as you go. To access the Quarkus Development console, in the logs, you can Command/Ctrl+click the link in VSCode or go to ()[http://localhost:8085], directly. The development console will show various aspects of your deployment. A quick highlight will be: The list of end points that are available The produced Maven artifact name (in this case quick-kogito 1.0.0-SNAPSHOT) You can modify configurations of your running application from here The Swagger UI and OpenAPI pages to get more information and test out your end points. We will be clicking the Swagger UI link to open the Swagger page to try out our sample DMN. Here you can see your generated Quarkus endpoints that deal with the Decision model. When building a DMN Decision Service with IBM Business Automation Open Edition 8.0 in Kogito, it will be able to use the DMN Model, coupled with the data model created in it to generate the required end points and inputs. This creates a Domain Specific Decision Service , which is different than how KIE Server worked before in that you would interact with a generic API and insert the objects for execution. By going away from the classpath loading strategy in Kogito, Decision Services are ready to be deployed into a cloud native environment. Your payloads are simpler and easy to reuse across any services as required without having to know exactly how the service is deployed, the objects associated with the deployment and more. The service that was created for the Decision Service is called Pricing Resource and if you click any of the end points you will get more information on them. The first end point to look at is the Post /pricing . This endpoint can be used to execute a decision and return just the data associated with the decison (inputs and outputs). After clicking, you can edit the Domain Specific payload in input that matches your decision input nodes ( Age and Previous incidents ) with values you want to test. Notice that the endpoints use faces in the object names, this matches the expected values that were tied to the inputs in the model as opposed to camelCase or snake_case or PascalCase. This helps with a consistent view across the Kogito stack - from design to runtime. Your result should look similar to the one below. As you can see, the Swagger-UI page produces a few things of note, the first being a Curl command to reproduce locally, the URL you called, and then ultimately the response from the service. The next endpoint to evaluate is the /pricing/dmnresult endpoint which will tell you more about the decision that executed. You can use the same payload from the previous steps to show the differences of what is presented. That payload can be similar to the code below provided in JSON. Click Try it out and repeat the steps from above. The input for the Try it out can be used from below. { \"Age\" : 35 , \"Previous incidents?\" : true } Click Execute with the updated payload. The result will go into more detail about which Decision Nodes were executed against and more. When there is more than one Decision Node available, you can see the results from each node. Now you can play with the DMN model more and see how those changes impact your Quarkus service. Since you're in Dev mode, as you make changes, those changes should be available to you each time you save. Try it out!","title":"Deploy Locally"},{"location":"guided_exercises/01_getting_started/01_deploy_local/#deploying-the-project-locally-as-a-quarkus-runtime","text":"Now that we have seen a little of the DMN Decision, let's use the combined power of Quarkus ( Learn more here about the Kubernetes-native Java Stack that is the best place for Kogito to run! ) and Kogito to quickly build and deploy the service using Maven commands and also having the option to deploy it to OpenShift as well. The first thing we're going to do is open a terminal (either in VSCode or whatever method you prefer) and validate that you have Maven and Java installed. java -version Expected output should be similar to openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment AdoptOpenJDK-11.0.11+9 (build 11.0.11+9) OpenJDK 64-Bit Server VM AdoptOpenJDK-11.0.11+9 (build 11.0.11+9, mixed mode) mvn -version Expected output should be similar to: Apache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) Java version: 18.0.2, vendor: Homebrew, runtime: /opt/homebrew/Cellar/openjdk/18.0.2/libexec/openjdk.jdk/Contents/Home Maven home: /opt/homebrew/Cellar/maven/3.8.6/libexec Default locale: en_US, platform encoding: UTF-8 OS name: \"mac os x\", version: \"12.6\", arch: \"aarch64\", family: \"mac\" Now that our environment has the appropriate resources, let's build and run the service locally. Quarkus does this very simply with the command mvn quarkus:dev which will operate the container in development mode. By default it will run on 2 ports when running this command, 8080 and 5005 . These ports can be maintained in the application.properties file in src/main/resources/ or you can modify the mvn quarkus:dev command to load those properties from the command line. If you want to do the command line, you can do mvn quarkus:dev -Dquarkus.http.port=8085 -Ddebug=7007 which would setup your application to run off of port 8085 and be able to attach a remote debugger at 7007 . To change the default host in application.properties add the following line. quarkus . http . port = 8085 After the project's application.properties has been updated, let's start the Quarkus service by running the following command. This command will run the remote debugger (which we are not using in these labs) on port 6006 so that if you already have another application running on 5005 (the default port) it won't give an error. mvn quarkus:dev -Ddebug = 6006 When the service is ready, it will produce a log similar to the below: $ quick-kogito % mvn quarkus:dev -Ddebug=6006 [INFO] Scanning for projects... [INFO] [INFO] --------------------< com.ibm.sample:quick-kogito >--------------------- [INFO] Building quick-kogito 1.0.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- ... 2022-09-29 20:48:34,404 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Initializing DMN DT Validator... 2022-09-29 20:48:34,405 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) DMN DT Validator initialized. 2022-09-29 20:48:34,405 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Analysing decision tables in DMN Model 'pricing' ... 2022-09-29 20:48:34,410 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) analysis for decision table 'Base price': 2022-09-29 20:48:34,412 INFO [org.kie.kog.cod.dec.DecisionValidation] (build-23) Decision Table Analysis of table 'Base price' finished with no messages to be reported. 2022-09-29 20:48:34,494 INFO [org.kie.kog.qua.com.dep.KogitoAssetsProcessor] (build-38) reflectiveEfestoGeneratedClassBuildItem org.kie.kogito.quarkus.common.deployment.KogitoGeneratedSourcesBuildItem@415815e1 __ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\\ \\ --\\___\\_\\____/_/ |_/_/|_/_/|_|\\____/___/ 2022-09-29 20:48:35,728 INFO quick-kogito 1.0.0-SNAPSHOT on JVM started in 2.432s. 2022-09-29 20:48:35,728 INFO Listening on: http://localhost:8085 2022-09-29 20:48:35,728 INFO Profile dev activated. Live Coding activated. 2022-09-29 20:48:35,728 INFO Installed features: [cdi, kogito-decisions, kogito-predictions, kogito-processes, kogito-rules, resteasy-reactive, resteasy-reactive-jackson, smallrye-context-propagation, smallrye-openapi, swagger-ui, vertx]","title":"Deploying the Project locally as a Quarkus runtime"},{"location":"guided_exercises/01_getting_started/01_deploy_local/#using-the-project-that-is-created","text":"Now that your application is created, you can start using it more for live development and change as you go. To access the Quarkus Development console, in the logs, you can Command/Ctrl+click the link in VSCode or go to ()[http://localhost:8085], directly. The development console will show various aspects of your deployment. A quick highlight will be: The list of end points that are available The produced Maven artifact name (in this case quick-kogito 1.0.0-SNAPSHOT) You can modify configurations of your running application from here The Swagger UI and OpenAPI pages to get more information and test out your end points. We will be clicking the Swagger UI link to open the Swagger page to try out our sample DMN. Here you can see your generated Quarkus endpoints that deal with the Decision model. When building a DMN Decision Service with IBM Business Automation Open Edition 8.0 in Kogito, it will be able to use the DMN Model, coupled with the data model created in it to generate the required end points and inputs. This creates a Domain Specific Decision Service , which is different than how KIE Server worked before in that you would interact with a generic API and insert the objects for execution. By going away from the classpath loading strategy in Kogito, Decision Services are ready to be deployed into a cloud native environment. Your payloads are simpler and easy to reuse across any services as required without having to know exactly how the service is deployed, the objects associated with the deployment and more. The service that was created for the Decision Service is called Pricing Resource and if you click any of the end points you will get more information on them. The first end point to look at is the Post /pricing . This endpoint can be used to execute a decision and return just the data associated with the decison (inputs and outputs). After clicking, you can edit the Domain Specific payload in input that matches your decision input nodes ( Age and Previous incidents ) with values you want to test. Notice that the endpoints use faces in the object names, this matches the expected values that were tied to the inputs in the model as opposed to camelCase or snake_case or PascalCase. This helps with a consistent view across the Kogito stack - from design to runtime. Your result should look similar to the one below. As you can see, the Swagger-UI page produces a few things of note, the first being a Curl command to reproduce locally, the URL you called, and then ultimately the response from the service. The next endpoint to evaluate is the /pricing/dmnresult endpoint which will tell you more about the decision that executed. You can use the same payload from the previous steps to show the differences of what is presented. That payload can be similar to the code below provided in JSON. Click Try it out and repeat the steps from above. The input for the Try it out can be used from below. { \"Age\" : 35 , \"Previous incidents?\" : true } Click Execute with the updated payload. The result will go into more detail about which Decision Nodes were executed against and more. When there is more than one Decision Node available, you can see the results from each node. Now you can play with the DMN model more and see how those changes impact your Quarkus service. Since you're in Dev mode, as you make changes, those changes should be available to you each time you save. Try it out!","title":"Using the project that is created"},{"location":"guided_exercises/01_getting_started/01_deploy_openshift/","text":"Under construction This content will be completed soon! Deploying your service to OpenShift as a Kogito Quarkus Service Now that we have seen a little of the DMN Decision, let's use the combined power of Quarkus ( Learn more here about the Kubernetes-native Java Stack that is the best place for Kogito to run! ) and Kogito to quickly build and deploy the service to OpenShift. If you have already done the deploy locally, you will be able to skip some of these steps and start at Getting Ready for OpenShift Deployment . When deploying to OpenShift you will need to take the following things into account: The application with your Kogito microservices is in a Git repository that is reachable from your OpenShift environment. You have access to the OpenShift web console with the necessary permissions to create and edit KogitoBuild and KogitoRuntime. If implementing Quarkus, update the pom.xml file of your project contains the following dependency for the Quarkus smallrye-health extension. This extension enables the liveness and readiness probes that are required for Quarkus-based projects on OpenShift. If you created the project like in 01_walk_through.md , you will already have this extension added to your deployment. Deploying the Kogito Operator First create a namespace for this to deployed into from the OpenShift console. For example tim-kogito-on-openshift . Next, we need to install the Kogito Operator from the OperatorHub. To do this, login to the OpenShift Cluster console and login as an admin. From here, go to Operator ==> OperatorHub to open the OperatorHub. Search for Kogito and click the IBM BAMOE Kogito Operator tile. From the Operator Form, click Install at the top left to install the Operator. Install it to your namespace you created earlier. This way if there's a version already installed, you're not colliding with it and it is contained to just your namespace. The drawback is that it is limited to this namespace. The installation may take a few minutes, but once it is completed, you will be ready to deploy your service. Click View Operator Success Deploying your first Kogito Application on OpenShift Now that we've got the Operator installed, we're going to see how it works. This lab will walk through the deployment of a Quarkus deployment of the Kogito Service. There will be a collapsed section to refer to if the service was a Spring Boot service instead so there's a reference to it to show how easy it both types of deployments are utilizing the operator. Once the operator is installed, you will click the Kebab icon and Create KogitoBuild to create the build. Since we're developing in Quarkus, our implemetation details handled in yaml are fairly minimal. We will point to a git repository where the service is deployed. In the previous section you created a repository, e.g. github.com/timwuthenow/quick-openshift-kogito .","title":"Deploy in OpenShift"},{"location":"guided_exercises/01_getting_started/01_deploy_openshift/#under-construction","text":"This content will be completed soon!","title":"Under construction"},{"location":"guided_exercises/01_getting_started/01_deploy_openshift/#deploying-your-service-to-openshift-as-a-kogito-quarkus-service","text":"Now that we have seen a little of the DMN Decision, let's use the combined power of Quarkus ( Learn more here about the Kubernetes-native Java Stack that is the best place for Kogito to run! ) and Kogito to quickly build and deploy the service to OpenShift. If you have already done the deploy locally, you will be able to skip some of these steps and start at Getting Ready for OpenShift Deployment . When deploying to OpenShift you will need to take the following things into account: The application with your Kogito microservices is in a Git repository that is reachable from your OpenShift environment. You have access to the OpenShift web console with the necessary permissions to create and edit KogitoBuild and KogitoRuntime. If implementing Quarkus, update the pom.xml file of your project contains the following dependency for the Quarkus smallrye-health extension. This extension enables the liveness and readiness probes that are required for Quarkus-based projects on OpenShift. If you created the project like in 01_walk_through.md , you will already have this extension added to your deployment.","title":"Deploying your service to OpenShift as a Kogito Quarkus Service"},{"location":"guided_exercises/01_getting_started/01_deploy_openshift/#deploying-the-kogito-operator","text":"First create a namespace for this to deployed into from the OpenShift console. For example tim-kogito-on-openshift . Next, we need to install the Kogito Operator from the OperatorHub. To do this, login to the OpenShift Cluster console and login as an admin. From here, go to Operator ==> OperatorHub to open the OperatorHub. Search for Kogito and click the IBM BAMOE Kogito Operator tile. From the Operator Form, click Install at the top left to install the Operator. Install it to your namespace you created earlier. This way if there's a version already installed, you're not colliding with it and it is contained to just your namespace. The drawback is that it is limited to this namespace. The installation may take a few minutes, but once it is completed, you will be ready to deploy your service. Click View Operator Success","title":"Deploying the Kogito Operator "},{"location":"guided_exercises/01_getting_started/01_deploy_openshift/#deploying-your-first-kogito-application-on-openshift","text":"Now that we've got the Operator installed, we're going to see how it works. This lab will walk through the deployment of a Quarkus deployment of the Kogito Service. There will be a collapsed section to refer to if the service was a Spring Boot service instead so there's a reference to it to show how easy it both types of deployments are utilizing the operator. Once the operator is installed, you will click the Kebab icon and Create KogitoBuild to create the build. Since we're developing in Quarkus, our implemetation details handled in yaml are fairly minimal. We will point to a git repository where the service is deployed. In the previous section you created a repository, e.g. github.com/timwuthenow/quick-openshift-kogito .","title":"Deploying your first Kogito Application on OpenShift"},{"location":"guided_exercises/01_getting_started/01_walk_through/","text":"Using Maven to create the project workspace In the previous section, you setup Maven locally in your environment, so you should now have access to all of the mvn commands that are associated with running it. The first thing we're going to create is a project using the proceeding steps: We're going to create the service in Quarkus with the Maven commands below, this will create a Quarkus project called quick-kogito that will be versioned 1.0.0-SNAPSHOT including the extensions kogito-quarkus, dmn, resteasy-reactive-jackson, quarkus-smallrye-openapi, quarkus-smallrye-health which will create a Quarkus DMN project with the openapi components to get the OpenAPI end points easily with health checks when deploying to OpenShift. mvn io.quarkus:quarkus-maven-plugin:create \\ -DprojectGroupId = com.ibm.sample -DprojectArtifactId = quick-kogito \\ -DprojectVersion = 1 .0.0-SNAPSHOT -Dextensions = kogito-quarkus,dmn,resteasy-reactive-jackson,quarkus-smallrye-openapi,quarkus-smallrye-health When you create this project you should get a bunch of Maven artifacts start to stream in your console that are being pulled and ultimately are left with a console message like the below: [INFO] [INFO] ======================================================================================== [INFO] Your new application has been created in /Users/developer/quick-kogito [INFO] Navigate into this directory and launch your application with mvn quarkus:dev [INFO] Your application will be accessible on http://localhost:8080 [INFO] ======================================================================================== [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 24.548 s [INFO] Finished at: 2022-09-27T10:22:31-04:00 [INFO] ------------------------------------------------------------------------ If you installed VSCode to your PATH variables, you can open the workspace by doing the below command, otherwise open VSCode and navigate to where you ran the command for quick-kogito to be created at: cd quick-kogito code . From here we can see the workspace's contents and if we expand the contents of /src/main you will see the creation of several artifacts. Within java you will have a GreetingResource.java and within resources you will have an application.properties and pricing.dmn file. These are sample files that can be later modified or deleted, but we will be explore them first in this section, but will do more in later labs around the various end points. Let's first click the pricing.dmn file to open it. When you do so you may be greeted with a message similar to This diagram does not have layout information. Click 'Yes' to compute optimal layout, it takes time according to the diagram size. Click 'No' to proceed without layout. Please save the layout changes once diagram is opened. - if so click Yes to automap the DMN locations. When the diagram opens you will see something similar to below, so we will start exploring it. The DMN is made up of two inputs Age and Previous incidents? , which are used to make the decision, Base price . If you click Age and then click the Properties icon on the right, you will open a pane for the input. Within this pane, you can see information about the input Age , this includes that it is a number and what the input name is. More can be changed around this object, including changing the color of the node, font size, etc. To view the Decision, click the square decision node and select the Edit button to enter the decision for Base Price . From here you will see the Decision Table that is associated with the Base Price decision. From here you will see two (2) input columns ( Age and Previous Incidents ), as well as one output column ( Base price ) all with their types below them. These types are controlled from the properties panel similarly to how they were opened when looking at Age a few steps ago. This decision has 4 different rows that could fire, with a Hit Policy of UNIQUE signified by the U in the top left corner of the table. A decision writer could make any comments they want to the table and have them saved towards the decision here Create a GitHub repository for the project One last thing we're going to do is to create a GitHub repository for this service, so you have somewhere to store our changes and also take advantage of building it into a cloud service running on OpenShift. You can create a repository on GitHub . To do this login to your GitHub username (or create one if you don't have one!) and from your home page and click the green New icon near the top left of the page (or you can navigate directly to here ) New Repo Fill out the form with your values Repository Name : quick-openshift-kogito Select Public for now Add a Description if you want Click Create Repository Copy the command for ...or create a new repository on the command line as we're going to take exactly what's in our repository add a mostly empty README.md and push those changes to GitHub. The below command is an example and will not be the exact same as what you have on your repository . Copy yours . When this is done, you are finsihed with this section. Proceed to either deploying locally or deploying on OpenShift","title":"Project Creation and Walk through"},{"location":"guided_exercises/01_getting_started/01_walk_through/#using-maven-to-create-the-project-workspace","text":"In the previous section, you setup Maven locally in your environment, so you should now have access to all of the mvn commands that are associated with running it. The first thing we're going to create is a project using the proceeding steps: We're going to create the service in Quarkus with the Maven commands below, this will create a Quarkus project called quick-kogito that will be versioned 1.0.0-SNAPSHOT including the extensions kogito-quarkus, dmn, resteasy-reactive-jackson, quarkus-smallrye-openapi, quarkus-smallrye-health which will create a Quarkus DMN project with the openapi components to get the OpenAPI end points easily with health checks when deploying to OpenShift. mvn io.quarkus:quarkus-maven-plugin:create \\ -DprojectGroupId = com.ibm.sample -DprojectArtifactId = quick-kogito \\ -DprojectVersion = 1 .0.0-SNAPSHOT -Dextensions = kogito-quarkus,dmn,resteasy-reactive-jackson,quarkus-smallrye-openapi,quarkus-smallrye-health When you create this project you should get a bunch of Maven artifacts start to stream in your console that are being pulled and ultimately are left with a console message like the below: [INFO] [INFO] ======================================================================================== [INFO] Your new application has been created in /Users/developer/quick-kogito [INFO] Navigate into this directory and launch your application with mvn quarkus:dev [INFO] Your application will be accessible on http://localhost:8080 [INFO] ======================================================================================== [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 24.548 s [INFO] Finished at: 2022-09-27T10:22:31-04:00 [INFO] ------------------------------------------------------------------------ If you installed VSCode to your PATH variables, you can open the workspace by doing the below command, otherwise open VSCode and navigate to where you ran the command for quick-kogito to be created at: cd quick-kogito code . From here we can see the workspace's contents and if we expand the contents of /src/main you will see the creation of several artifacts. Within java you will have a GreetingResource.java and within resources you will have an application.properties and pricing.dmn file. These are sample files that can be later modified or deleted, but we will be explore them first in this section, but will do more in later labs around the various end points. Let's first click the pricing.dmn file to open it. When you do so you may be greeted with a message similar to This diagram does not have layout information. Click 'Yes' to compute optimal layout, it takes time according to the diagram size. Click 'No' to proceed without layout. Please save the layout changes once diagram is opened. - if so click Yes to automap the DMN locations. When the diagram opens you will see something similar to below, so we will start exploring it. The DMN is made up of two inputs Age and Previous incidents? , which are used to make the decision, Base price . If you click Age and then click the Properties icon on the right, you will open a pane for the input. Within this pane, you can see information about the input Age , this includes that it is a number and what the input name is. More can be changed around this object, including changing the color of the node, font size, etc. To view the Decision, click the square decision node and select the Edit button to enter the decision for Base Price . From here you will see the Decision Table that is associated with the Base Price decision. From here you will see two (2) input columns ( Age and Previous Incidents ), as well as one output column ( Base price ) all with their types below them. These types are controlled from the properties panel similarly to how they were opened when looking at Age a few steps ago. This decision has 4 different rows that could fire, with a Hit Policy of UNIQUE signified by the U in the top left corner of the table. A decision writer could make any comments they want to the table and have them saved towards the decision here","title":"Using Maven to create the project workspace"},{"location":"guided_exercises/01_getting_started/01_walk_through/#create-a-github-repository-for-the-project","text":"One last thing we're going to do is to create a GitHub repository for this service, so you have somewhere to store our changes and also take advantage of building it into a cloud service running on OpenShift. You can create a repository on GitHub . To do this login to your GitHub username (or create one if you don't have one!) and from your home page and click the green New icon near the top left of the page (or you can navigate directly to here ) New Repo Fill out the form with your values Repository Name : quick-openshift-kogito Select Public for now Add a Description if you want Click Create Repository Copy the command for ...or create a new repository on the command line as we're going to take exactly what's in our repository add a mostly empty README.md and push those changes to GitHub. The below command is an example and will not be the exact same as what you have on your repository . Copy yours . When this is done, you are finsihed with this section. Proceed to either deploying locally or deploying on OpenShift","title":"Create a GitHub repository for the project"},{"location":"guided_exercises/01_getting_started/introduction/","text":"Introduction This section will focus on how to setup your first Kogito projects from scratch. The method we're going to use for this is through the use of Maven to create it from an empty workspace, produce a DMN model and make it available both locally and how to use deploy it to OpenShift. Initial project setup and walk through Deploy locally Deploy to OpenShift","title":"Introduction"},{"location":"guided_exercises/01_getting_started/introduction/#introduction","text":"This section will focus on how to setup your first Kogito projects from scratch. The method we're going to use for this is through the use of Maven to create it from an empty workspace, produce a DMN model and make it available both locally and how to use deploy it to OpenShift. Initial project setup and walk through Deploy locally Deploy to OpenShift","title":"Introduction"},{"location":"guided_exercises/03_dmn/advanced-lab-authoring/","text":"Authoring the decisioning for the Call Center The problem statement describes a number of different inputs to our decision: Call : the incoming call into the call-centre Employees**: the employees of certain office. Office : an office to which the call could potentially be routed. Furthermore, the problem statement describes that phone numbers could be banned. So, also banned numbers can be regarded as an input to our model (although we will not implement it as an input in this lab). With the given input, we need to make the following decisions: Accept Call : the final decision we need to make is whether the given office will accept the call. Can Handle Call : whether the office can actually accept the call. As defined in the problem statement, this depends on: whether the phone number has been banned; the purpose of the phone call (\u201chelp\u201d or \u201cobjection\u201d). . Accept Call Decision Structure Accept Call : the final decision we need to make is whether the given office will accept the call. Add a Decision node to the diagram by clicking on the Decision node icon and placing it in the DRD. Double-click on the node to set the name. We will name this node Accept Call . With the Accept Call node selected, open the property panel. Set the Output data type to boolean . The input of this decision is the incoming call , office and employee . Create these 3 input nodes and connect them to the Accept Call decision. We can now set data types of our input nodes. Click on the incoming call node, open the property panel and in the Output data type section and click on the Manage button. This will open the Custom Data Types window. In the Custom Data Types window, click on the + Add button. Define the data type tPhoneNumber as follows: Define another data type tCall as follows. Note that this datatype has a field that is of type tPhoneNumber , the type we defined earlier: When you\u2019ve created the tCall type, go back to the DRD by clicking on the Model tab. Select the incoming call node, and in the property panel, set the node\u2019s Output data type to tCall Next, define the following data type and set it as the Output data type of the office input as such: Define the data type for employees as follows. Note that we\u2019ve first defined the type tEmployee , and afterwards we\u2019ve defined tEmployees as a List of tEmployee . Decision Service With the main structure defined, we can now look at the requirements of the decision whether the office can actually accept the call. As defined in the problem statement, this depends on: whether the phone number has been banned. the purpose of the phone call (\"help\" or \"objection\"). We will model this decision as a DMN Decision Service that can be called by our main decision Accept Call . First, model the Decision Service in the DRD and give it the name Can Handle Call . Set it\u2019s Output data type to boolean . Add a Decision Node to the Decision Service . Name it Call Can Be Handled and set it\u2019s Output data type to boolean . Add 2 additional Decision Nodes and name them Is Banned and Call Purpose Accepted . Both should have an Output data type of type boolean . Connect the 2 Decision Nodes to the Call Can Be Handled node. The input to both the Is Banned and Call Purpose Accepted decisions is a call . Connect the existing node \"incoming call\" to the 2 decision nodes. The Is Banned decision also needs a collection of banned phone numbers. Instead of implementing this as an Input node, we will implement this as a DMN Relation Decision . Create a new Decision Node and name it Banned Phone Numbers . Connect it to the Is Banned decision node. The Ouput data type of this nodes is a new custom data type, which is a list of tPhoneNumber . We\u2019ll name this type tPhoneNumbers : Click on the Edit button of the Banned Phone Numbers node. Set the logic type of the decision to Relation . Create the following table: We can now implement the logic of the Is Banned decision. Click on the Edit button of the decision node. We will implement the logic as a Literal Expression . Define the following FEEL expression: list contains ( Banned Phone Numbers , call . phone ) The next node for which we want to implement the decision logic is Call Purpose Accepted . Click on the node, and click on the Edit button. Implement the following logic as a Decision Table : We can now implement the decision of Call Can Be Handled . Click on the node and click on the node\u2019s Edit button. In the decision editor, set the logic type to Decision Table and implement the following table: Create a DMN Knowledge Requirement from the Can Handle Call decision service to the Accept Call decision. \"Accept Call\" Decision Logic Implement the Accept Call decision logic as follows. Notice that the line 1 is the invocation of the decision service \"Can Handle Call\". This is an Invocation of the Can Handle Call service, passing the incoming call input as the variable call . The output of this invocation will be the boolean variable Call can be handled . The Call can be handled variable as then used to validate the decision result in the last line. Next steps We're done. Next, we should deploy the project in KIE Server and test the model using the REST and Java API.","title":"Call Centre - Authoring Decisions"},{"location":"guided_exercises/03_dmn/advanced-lab-authoring/#authoring-the-decisioning-for-the-call-center","text":"The problem statement describes a number of different inputs to our decision: Call : the incoming call into the call-centre Employees**: the employees of certain office. Office : an office to which the call could potentially be routed. Furthermore, the problem statement describes that phone numbers could be banned. So, also banned numbers can be regarded as an input to our model (although we will not implement it as an input in this lab). With the given input, we need to make the following decisions: Accept Call : the final decision we need to make is whether the given office will accept the call. Can Handle Call : whether the office can actually accept the call. As defined in the problem statement, this depends on: whether the phone number has been banned; the purpose of the phone call (\u201chelp\u201d or \u201cobjection\u201d).","title":"Authoring the decisioning for the Call Center"},{"location":"guided_exercises/03_dmn/advanced-lab-authoring/#accept-call-decision-structure","text":"Accept Call : the final decision we need to make is whether the given office will accept the call. Add a Decision node to the diagram by clicking on the Decision node icon and placing it in the DRD. Double-click on the node to set the name. We will name this node Accept Call . With the Accept Call node selected, open the property panel. Set the Output data type to boolean . The input of this decision is the incoming call , office and employee . Create these 3 input nodes and connect them to the Accept Call decision. We can now set data types of our input nodes. Click on the incoming call node, open the property panel and in the Output data type section and click on the Manage button. This will open the Custom Data Types window. In the Custom Data Types window, click on the + Add button. Define the data type tPhoneNumber as follows: Define another data type tCall as follows. Note that this datatype has a field that is of type tPhoneNumber , the type we defined earlier: When you\u2019ve created the tCall type, go back to the DRD by clicking on the Model tab. Select the incoming call node, and in the property panel, set the node\u2019s Output data type to tCall Next, define the following data type and set it as the Output data type of the office input as such: Define the data type for employees as follows. Note that we\u2019ve first defined the type tEmployee , and afterwards we\u2019ve defined tEmployees as a List of tEmployee .","title":". Accept Call Decision Structure"},{"location":"guided_exercises/03_dmn/advanced-lab-authoring/#decision-service","text":"With the main structure defined, we can now look at the requirements of the decision whether the office can actually accept the call. As defined in the problem statement, this depends on: whether the phone number has been banned. the purpose of the phone call (\"help\" or \"objection\"). We will model this decision as a DMN Decision Service that can be called by our main decision Accept Call . First, model the Decision Service in the DRD and give it the name Can Handle Call . Set it\u2019s Output data type to boolean . Add a Decision Node to the Decision Service . Name it Call Can Be Handled and set it\u2019s Output data type to boolean . Add 2 additional Decision Nodes and name them Is Banned and Call Purpose Accepted . Both should have an Output data type of type boolean . Connect the 2 Decision Nodes to the Call Can Be Handled node. The input to both the Is Banned and Call Purpose Accepted decisions is a call . Connect the existing node \"incoming call\" to the 2 decision nodes. The Is Banned decision also needs a collection of banned phone numbers. Instead of implementing this as an Input node, we will implement this as a DMN Relation Decision . Create a new Decision Node and name it Banned Phone Numbers . Connect it to the Is Banned decision node. The Ouput data type of this nodes is a new custom data type, which is a list of tPhoneNumber . We\u2019ll name this type tPhoneNumbers : Click on the Edit button of the Banned Phone Numbers node. Set the logic type of the decision to Relation . Create the following table: We can now implement the logic of the Is Banned decision. Click on the Edit button of the decision node. We will implement the logic as a Literal Expression . Define the following FEEL expression: list contains ( Banned Phone Numbers , call . phone ) The next node for which we want to implement the decision logic is Call Purpose Accepted . Click on the node, and click on the Edit button. Implement the following logic as a Decision Table : We can now implement the decision of Call Can Be Handled . Click on the node and click on the node\u2019s Edit button. In the decision editor, set the logic type to Decision Table and implement the following table: Create a DMN Knowledge Requirement from the Can Handle Call decision service to the Accept Call decision.","title":"Decision Service"},{"location":"guided_exercises/03_dmn/advanced-lab-authoring/#accept-call-decision-logic","text":"Implement the Accept Call decision logic as follows. Notice that the line 1 is the invocation of the decision service \"Can Handle Call\". This is an Invocation of the Can Handle Call service, passing the incoming call input as the variable call . The output of this invocation will be the boolean variable Call can be handled . The Call can be handled variable as then used to validate the decision result in the last line.","title":"\"Accept Call\" Decision Logic"},{"location":"guided_exercises/03_dmn/advanced-lab-authoring/#next-steps","text":"We're done. Next, we should deploy the project in KIE Server and test the model using the REST and Java API.","title":"Next steps"},{"location":"guided_exercises/03_dmn/advanced-lab-deployment/","text":"Deploying and testing the Decision Service With our decision model completed, we can now package our DMN model in a Deployment Unit (KJAR), deploy it on the Execution Server and test our decision. Deploying the decision service To deploy your business application, follow these steps: In the bread-crumb navigation in the upper-left corner, click on call-centre-decisions to go back to the project\u2019s Library View. Click on the Deploy button in the upper-right corner of the screen. This will package our DMN mode in a Deployment Unit (KJAR) and deploy it onto the Execution Server (KIE-Server). Go to the Execution Servers perspective by clicking on \"Menu \u2192 Deploy \u2192 Execution Servers\". You will see the Deployment Unit deployed on the Execution Server. Testing DMN Solution In this section, you will test the DMN solution with Execution Server\u2019s Swagger interface and via Java KIE Client API. Testing the Decision Service via the REST API The Swagger interface provides the description and documentation of the Execution Server\u2019s RESTful API. At the same time, it allows the APIs to be called from the UI. This enables developers and users to quickly test a, in this case, a deployed DMN Service . Navigate to KIE Server swagger docs Locate the DMN Models section. The DMN API provides the DMN model as a RESTful resources, which accepts 2 operations: GET : Retrieves the DMN model. POST : Evaluates the decisions for a given input. Expand the GET operation by clicking on it. Click on the Try it out button. Set the containerId field to call-centre-decisions and set the Response content type to application/json and click on Execute If requested, provide the username and password of your Business Central and KIE-Server user. The response will be the model-description of your DMN model. Evaluate the Model Next, we will evaluate our model with some input data. We need to provide our model with the incoming call , list of employees and office location . Expand the POST operation and click on the Try it out button Set the containerId field to call-centre-decisions . Set the Parameter content type and Response content type fields to application/json . Pass the following request to evaluate whether the given call is accepted by the call-centre. IMPORTANT : We\u2019re explicitly specifying the decision-name of the decision we want to evaluate. If we would not specify this, the engine will evaluate the full model, and hence will also require us to pass the call input. When we only evaluate the Accept Call decision, we only need to specify the inputs of Accept Call . In the decision service invocation in the Accept Call logic, the input incoming call is passed to the call parameter of the decision service. { \"decision-name\" : \"Accept Call\" , \"dmn-context\" :{ \"incoming call\" :{ \"phone\" : { \"country prefix\" : \"+420\" , \"phone number\" : \"1234\" }, \"purpose\" : \"help\" }, \"employees\" : [{ \"name\" : \"Duncan\" , \"office location\" : \"Rome\" }], \"office\" : { \"location\" : \"Rome\" } } } Click on Execute . The result value of the Accept Call should be true . Test the service with a number of other values. For example, specify a banned phone number like: +421 92000001 Using the KIE-Server Client IBM Decision Manager provides a KIE-Server Client API that allows the user to interact with the KIE-Server from a Java client using a higher level API. It abstracts the data marshalling and unmarshalling and the creation and execution of the RESTful commands from the developer, allowing him/her to focus on developing business logic. In this section we will create a simple Java client for our DMN model. IMPORTANT: If your KIE Server is exposed via https you need to configure the `javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword in the Java client code using the Remote Java API. If not, you may get a rest.NoEndpointFoundException`. For more information check this solution Red Hat's knowledge base. Create a new Maven Java JAR project in your favourite IDE (e.g. IntelliJ, Eclipse, Visual Studio Code). Add the following dependency to your project: <dependency> <groupId> org.kie.server </groupId> <artifactId> kie-server-client </artifactId> <version> 7.67.0.Final-redhat-00008 </version> <scope> compile </scope> </dependency> Create a Java package in your src/main/java folder with the name org.kie.dmn.lab . In the package you\u2019ve just created, create a Java class called Main . Add a public static void main(String[] args) method to your main class. Before we implement our method, we first define a number of constants that we will need when implementing our method (note that the values of your constants can be different depending on your environment, model namespace, etc.): private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"call-centre-decisions\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; private static final String DMN_MODEL_NAMESPACE = \"https://kiegroup.org/dmn/_2E9DCCE2-8C2B-496E-AC37-103694E51940\" ; private static final String DMN_MODEL_NAME = \"call-centre\" ; \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"call-centre-decisions\" ; private static final String USERNAME = \"pamadmin\" ; private static final String PASSWORD = \"pamadm1n\" ; private static final String DMN_MODEL_NAMESPACE = \"https://kiegroup.org/dmn/_2E9DCCE2-8C2B-496E-AC37-103694E51940\" ; private static final String DMN_MODEL_NAME = \"call-centre\" ; KIE-Server client API classes can mostly be retrieved from the KieServicesFactory class. We first need to create a KieServicesConfiguration instance that will hold our credentials and defines how we want our client to communicate with the server: KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , new EnteredCredentialsProvider ( USERNAME , PASSWORD )); kieServicesConfig . setMarshallingFormat ( MarshallingFormat . JSON ); Next, we create the KieServicesClient : KieServicesClient kieServicesClient = KieServicesFactory . newKieServicesClient ( kieServicesConfig ); From this client we retrieve our DMNServicesClient: DMNServicesClient dmnServicesClient = kieServicesClient . getServicesClient ( DMNServicesClient . class ); To pass the input values to our model to the Execution Server, we need to create a DMNContext : DMNContext dmnContext = dmnServicesClient . newContext (); We pass the input variables to the DMNContext . We define the following three methods that create the data inputs: private static Map < String , Object > getIncomingCall () { Map < String , Object > incomingCall = new HashMap <> (); Map < String , Object > phone = new HashMap <> (); phone . put ( \"country prefix\" , \"+420\" ); phone . put ( \"phone number\" , \"1234\" ); incomingCall . put ( \"phone\" , phone ); incomingCall . put ( \"purpose\" , \"help\" ); return incomingCall ; } private static List < Map < String , Object >> getEmployees () { List < Map < String , Object >> employees = new ArrayList <> (); Map < String , Object > employee = new HashMap <> (); employee . put ( \"name\" , \"Duncan\" ); employee . put ( \"office location\" , \"Rome\" ); employees . add ( employee ); return employees ; } private static Map < String , Object > getOffice () { Map < String , Object > office = new HashMap <> (); office . put ( \"location\" , \"Rome\" ); return office ; } We can now add the data to the DMNContext as follows: dmnContext . set ( \"incoming call\" , getIncomingCall ()); dmnContext . set ( \"employees\" , getEmployees ()); dmnContext . set ( \"office\" , getOffice ()); We now have defined all the required instances needed to send a DMN evaluation request to the server. We explicitly specify which decision we want to evaluate, in this case the Accept Call decision, by using the evaluateDecisionByName of the DMNServiceClient . ServiceResponse < DMNResult > dmnResultResponse = dmnServicesClient . evaluateDecisionByName ( CONTAINER_ID , DMN_MODEL_NAMESPACE , DMN_MODEL_NAME , \"Accept Call\" , dmnContext ); Finally we can retrieve the DMN evaluation result and print it in the console: DMNDecisionResult decisionResult = dmnResultResponse . getResult (). getDecisionResultByName ( \"Accept Call\" ); System . out . println ( \"Is the call accepted?: \" + decisionResult . getResult ()); Compile your project and run it. Observe the output in the console, which should say: Is the call accepted?: true The complete project can be found here: https://github.com/kmacedovarela/dmn-workshop-labs/tree/master/call-centre-dmn-lab-client","title":"Call Centre - Consuming Decisions"},{"location":"guided_exercises/03_dmn/advanced-lab-deployment/#deploying-and-testing-the-decision-service","text":"With our decision model completed, we can now package our DMN model in a Deployment Unit (KJAR), deploy it on the Execution Server and test our decision.","title":"Deploying and testing the Decision Service"},{"location":"guided_exercises/03_dmn/advanced-lab-deployment/#deploying-the-decision-service","text":"To deploy your business application, follow these steps: In the bread-crumb navigation in the upper-left corner, click on call-centre-decisions to go back to the project\u2019s Library View. Click on the Deploy button in the upper-right corner of the screen. This will package our DMN mode in a Deployment Unit (KJAR) and deploy it onto the Execution Server (KIE-Server). Go to the Execution Servers perspective by clicking on \"Menu \u2192 Deploy \u2192 Execution Servers\". You will see the Deployment Unit deployed on the Execution Server.","title":"Deploying the decision service"},{"location":"guided_exercises/03_dmn/advanced-lab-deployment/#testing-dmn-solution","text":"In this section, you will test the DMN solution with Execution Server\u2019s Swagger interface and via Java KIE Client API.","title":"Testing DMN Solution"},{"location":"guided_exercises/03_dmn/advanced-lab-deployment/#testing-the-decision-service-via-the-rest-api","text":"The Swagger interface provides the description and documentation of the Execution Server\u2019s RESTful API. At the same time, it allows the APIs to be called from the UI. This enables developers and users to quickly test a, in this case, a deployed DMN Service . Navigate to KIE Server swagger docs Locate the DMN Models section. The DMN API provides the DMN model as a RESTful resources, which accepts 2 operations: GET : Retrieves the DMN model. POST : Evaluates the decisions for a given input. Expand the GET operation by clicking on it. Click on the Try it out button. Set the containerId field to call-centre-decisions and set the Response content type to application/json and click on Execute If requested, provide the username and password of your Business Central and KIE-Server user. The response will be the model-description of your DMN model.","title":"Testing the Decision Service via the REST API"},{"location":"guided_exercises/03_dmn/advanced-lab-deployment/#evaluate-the-model","text":"Next, we will evaluate our model with some input data. We need to provide our model with the incoming call , list of employees and office location . Expand the POST operation and click on the Try it out button Set the containerId field to call-centre-decisions . Set the Parameter content type and Response content type fields to application/json . Pass the following request to evaluate whether the given call is accepted by the call-centre. IMPORTANT : We\u2019re explicitly specifying the decision-name of the decision we want to evaluate. If we would not specify this, the engine will evaluate the full model, and hence will also require us to pass the call input. When we only evaluate the Accept Call decision, we only need to specify the inputs of Accept Call . In the decision service invocation in the Accept Call logic, the input incoming call is passed to the call parameter of the decision service. { \"decision-name\" : \"Accept Call\" , \"dmn-context\" :{ \"incoming call\" :{ \"phone\" : { \"country prefix\" : \"+420\" , \"phone number\" : \"1234\" }, \"purpose\" : \"help\" }, \"employees\" : [{ \"name\" : \"Duncan\" , \"office location\" : \"Rome\" }], \"office\" : { \"location\" : \"Rome\" } } } Click on Execute . The result value of the Accept Call should be true . Test the service with a number of other values. For example, specify a banned phone number like: +421 92000001","title":"Evaluate the Model"},{"location":"guided_exercises/03_dmn/advanced-lab-deployment/#using-the-kie-server-client","text":"IBM Decision Manager provides a KIE-Server Client API that allows the user to interact with the KIE-Server from a Java client using a higher level API. It abstracts the data marshalling and unmarshalling and the creation and execution of the RESTful commands from the developer, allowing him/her to focus on developing business logic. In this section we will create a simple Java client for our DMN model. IMPORTANT: If your KIE Server is exposed via https you need to configure the `javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword in the Java client code using the Remote Java API. If not, you may get a rest.NoEndpointFoundException`. For more information check this solution Red Hat's knowledge base. Create a new Maven Java JAR project in your favourite IDE (e.g. IntelliJ, Eclipse, Visual Studio Code). Add the following dependency to your project: <dependency> <groupId> org.kie.server </groupId> <artifactId> kie-server-client </artifactId> <version> 7.67.0.Final-redhat-00008 </version> <scope> compile </scope> </dependency> Create a Java package in your src/main/java folder with the name org.kie.dmn.lab . In the package you\u2019ve just created, create a Java class called Main . Add a public static void main(String[] args) method to your main class. Before we implement our method, we first define a number of constants that we will need when implementing our method (note that the values of your constants can be different depending on your environment, model namespace, etc.): private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"call-centre-decisions\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; private static final String DMN_MODEL_NAMESPACE = \"https://kiegroup.org/dmn/_2E9DCCE2-8C2B-496E-AC37-103694E51940\" ; private static final String DMN_MODEL_NAME = \"call-centre\" ; \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"call-centre-decisions\" ; private static final String USERNAME = \"pamadmin\" ; private static final String PASSWORD = \"pamadm1n\" ; private static final String DMN_MODEL_NAMESPACE = \"https://kiegroup.org/dmn/_2E9DCCE2-8C2B-496E-AC37-103694E51940\" ; private static final String DMN_MODEL_NAME = \"call-centre\" ; KIE-Server client API classes can mostly be retrieved from the KieServicesFactory class. We first need to create a KieServicesConfiguration instance that will hold our credentials and defines how we want our client to communicate with the server: KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , new EnteredCredentialsProvider ( USERNAME , PASSWORD )); kieServicesConfig . setMarshallingFormat ( MarshallingFormat . JSON ); Next, we create the KieServicesClient : KieServicesClient kieServicesClient = KieServicesFactory . newKieServicesClient ( kieServicesConfig ); From this client we retrieve our DMNServicesClient: DMNServicesClient dmnServicesClient = kieServicesClient . getServicesClient ( DMNServicesClient . class ); To pass the input values to our model to the Execution Server, we need to create a DMNContext : DMNContext dmnContext = dmnServicesClient . newContext (); We pass the input variables to the DMNContext . We define the following three methods that create the data inputs: private static Map < String , Object > getIncomingCall () { Map < String , Object > incomingCall = new HashMap <> (); Map < String , Object > phone = new HashMap <> (); phone . put ( \"country prefix\" , \"+420\" ); phone . put ( \"phone number\" , \"1234\" ); incomingCall . put ( \"phone\" , phone ); incomingCall . put ( \"purpose\" , \"help\" ); return incomingCall ; } private static List < Map < String , Object >> getEmployees () { List < Map < String , Object >> employees = new ArrayList <> (); Map < String , Object > employee = new HashMap <> (); employee . put ( \"name\" , \"Duncan\" ); employee . put ( \"office location\" , \"Rome\" ); employees . add ( employee ); return employees ; } private static Map < String , Object > getOffice () { Map < String , Object > office = new HashMap <> (); office . put ( \"location\" , \"Rome\" ); return office ; } We can now add the data to the DMNContext as follows: dmnContext . set ( \"incoming call\" , getIncomingCall ()); dmnContext . set ( \"employees\" , getEmployees ()); dmnContext . set ( \"office\" , getOffice ()); We now have defined all the required instances needed to send a DMN evaluation request to the server. We explicitly specify which decision we want to evaluate, in this case the Accept Call decision, by using the evaluateDecisionByName of the DMNServiceClient . ServiceResponse < DMNResult > dmnResultResponse = dmnServicesClient . evaluateDecisionByName ( CONTAINER_ID , DMN_MODEL_NAMESPACE , DMN_MODEL_NAME , \"Accept Call\" , dmnContext ); Finally we can retrieve the DMN evaluation result and print it in the console: DMNDecisionResult decisionResult = dmnResultResponse . getResult (). getDecisionResultByName ( \"Accept Call\" ); System . out . println ( \"Is the call accepted?: \" + decisionResult . getResult ()); Compile your project and run it. Observe the output in the console, which should say: Is the call accepted?: true The complete project can be found here: https://github.com/kmacedovarela/dmn-workshop-labs/tree/master/call-centre-dmn-lab-client","title":"Using the KIE-Server Client"},{"location":"guided_exercises/03_dmn/advanced-lab-intro/","text":"Call Centre - Intro and Use Case This is an advanced Decision Model & Notation lab that introduces DMN Decision Services, Relations, nested boxed expressions, etc. It also explores a number of different FEEL constructs and expressions like, for example, list contains . Goals Implement a DMN model using the Red Hat DM/PAM DMN editor Deploy the existing DMN project to Decision Server Problem Statement In this lab we will create a decision that determines if a call-centre can take an incoming call. Whether a call will be accepted by a certain office depends on: The office accepts the call. There are employees currently available at the office. Whether the office can accepts a call depends on: whether the phone number has been banned. the purpose of the phone call (\"help\" or \"objection\"). Create a Decision Project To define and deploy a DMN decision model, we first need to create a new project in which we can store the model. To create a new project: Navigate to Business Central Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name call-centre-decisions , and the description \"Call Centre Decisions\". With the project created, we can now create our DMN model. Click on the blue Add Asset button. In the Add Asset page, select Decision in the dropdown filter selector. Click on the DMN tile to create a new DMN model. Give it the name call-centre . This will create the asset and open the DMN editor. Next Steps You can do this lab in 2 ways: If you already have (some) DMN knowledge, we would like to challenge you to build the solution by yourself. After you\u2019ve built solution, you can verify your answer by going to the next module in which we will explain the solution and will deploy it onto the runtime. Follow this step-by-step guide which will guide you through the implementation.","title":"Advanced DMN Exercises - Call Centre - Introduction"},{"location":"guided_exercises/03_dmn/advanced-lab-intro/#call-centre-intro-and-use-case","text":"This is an advanced Decision Model & Notation lab that introduces DMN Decision Services, Relations, nested boxed expressions, etc. It also explores a number of different FEEL constructs and expressions like, for example, list contains .","title":"Call Centre - Intro and Use Case"},{"location":"guided_exercises/03_dmn/advanced-lab-intro/#goals","text":"Implement a DMN model using the Red Hat DM/PAM DMN editor Deploy the existing DMN project to Decision Server","title":"Goals"},{"location":"guided_exercises/03_dmn/advanced-lab-intro/#problem-statement","text":"In this lab we will create a decision that determines if a call-centre can take an incoming call. Whether a call will be accepted by a certain office depends on: The office accepts the call. There are employees currently available at the office. Whether the office can accepts a call depends on: whether the phone number has been banned. the purpose of the phone call (\"help\" or \"objection\").","title":"Problem Statement"},{"location":"guided_exercises/03_dmn/advanced-lab-intro/#create-a-decision-project","text":"To define and deploy a DMN decision model, we first need to create a new project in which we can store the model. To create a new project: Navigate to Business Central Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name call-centre-decisions , and the description \"Call Centre Decisions\". With the project created, we can now create our DMN model. Click on the blue Add Asset button. In the Add Asset page, select Decision in the dropdown filter selector. Click on the DMN tile to create a new DMN model. Give it the name call-centre . This will create the asset and open the DMN editor.","title":"Create a Decision Project"},{"location":"guided_exercises/03_dmn/advanced-lab-intro/#next-steps","text":"You can do this lab in 2 ways: If you already have (some) DMN knowledge, we would like to challenge you to build the solution by yourself. After you\u2019ve built solution, you can verify your answer by going to the next module in which we will explain the solution and will deploy it onto the runtime. Follow this step-by-step guide which will guide you through the implementation.","title":"Next Steps"},{"location":"guided_exercises/03_dmn/getting-started/","text":"Getting Started with Decision Model and Notation This lab introduces you to the deployment of an existing Decision Model and Notation (DMN) and validation of it's decisions. Explore an existing DMN file created using Business Central Deploy the existing DMN project to Decision Server Test the deployed DMN Examine Existing DMN Diagram The following example describes an insurance price calculator based on an applicant\u2019s age and accident history. This is a simple decision process based on the following decision table: DMN Decision Table The decision table was designed without using Business Central tools, but could be imported seemlesly due to the conformity with DMN specification. The DMN decision table includes a hit policy , inputs , and outputs . Unlike the drl based decision tables that can be created in Business Central, input and output names in DMN decision tables accept spaces. The conditions for the Age input is defined using the Friendly Enough Expression Language (FEEL). The decision can also be represented by the following decision requirements diagram: In this decision requirements diagram, note that the applicant\u2019s age and accident history are the required inputs for the decision table \"Insurance Total Price\". The DMN component is currently stored in the DMN GitHub repository . Import the DMN File into KIE Sandbox In this section, you will import the GitHub repository to KIE Sandbox directly. Copy to this Raw DMN file and click the Raw Button or simply copy the link from the below command. https://raw.githubusercontent.com/timwuthenow/dmn-workshop-labs/master/policy-price/insurance-pricing.dmn With this link navigate to the KIE Sandbox and under Import From URL paste the link from the previous step and click Import . When the project is imported, you will see the DMN Editor with the insurance-pricing DMN model displayed. If you instead of pointing to a particular DMN, pointed to an entire project, any DMN/BPMN models associated with it would be able to viewed/edited within KIE Sandbox. You can then click the Run button to get a local copy of this DMN running within the browser session itself. This will have a section of the browser turn into a form and you can run the model right there. Modifying the checkbox based on the boolean of had previous incidents and set an Age based on the data type being a number. \ud83d\udcd8 INFO: There is a known issue around the DMN Runner with forms that does not initially assume a non-checked box is false on the first successful execution. To quickly get around this, check and uncheck the checkbox and move forward with your testing If you link your OpenShift login to the gear icon, you can even do a sample deployment of this as a service into OpenShift. For now this will conclude this section. Importing a DMN in Business Central If you want to try this in Business Central, you can go through the following steps, but it is not required. From the GitHub web page, click Clone or download on the right and then select Download ZIP : Using your favorite file system navigation tool, locate the downloaded ZIP file and unzip it to a directory in your file system. From this point forward, this location is referred to as $PROJECT_HOME . Log in to Business Central. You can use either bamAdmin:ibmpam1! or pamadmin:pamadm1n to do so or whatever login you have created on your instance. Create a project in Business Central called policy-price . In the empty project library view for the policy-price project, click Import Asset . In the Create new Uploaded file dialog, enter insurance-pricing.dmn in the Uploaded file field: Using the browse button at the far right of the field labeled Please select a file to upload , navigate with the file browser to the $PROJECT_HOME directory where the unzipped Git repository is located. Select the $PROJECT_HOME\\policy-price\\insurance-pricing.dmn file. Click Ok to import the DMN asset. The diagram will open and you will be able to see the DRD. Explore the diagram nodes to check the decision policies of this diagram. Close the diagram. You should now be on the library view for the policy-price project. You should see the insurance-pricing asset is added to your project assets: From the policy-price project\u2019s library view, click Build , then Deploy to deploy the project to the execution server. After receiving the build confirmation, navigate to the container deployment list by clicking the \" View deployment details \" link in the confirmation pop-up, or by selecting Menu \u2192 Deploy \u2192 Execution Servers . Verify that policy-price_2.0.0 shows a green status: Testing the Decision Service on KIE Server In this section, you test the DMN solution using the REST endpoints available in the Decision Server (a.k.a. KIE Server). Open your Decision Server (a.k.a KIE Server) on the url \"/docs\". You should see something like this: Next, under DMN Models , click on the POST /server/containers/{containerId}/dmn\" and select \"Try it out\": Now use the following data: Container ID: policy-price Body (dmn context): {\"dmn-context\": {\"Age\": 20, \"had previous incidents\": false}} Parameter content type: application/json Click on the execute button. You should see the server response 200 and the results of the decision. Try out the Decision with different values for the age and accident history, and compare the results with the decision table: Conclusion Congratulations, you've finished the getting started exercise. Next, you will have an intermediate level exercise that will guide you through the implementation, deployment and testing of the Vacation Days use case.","title":"Getting Started DMN Exercises - Insurance Price - Getting Started"},{"location":"guided_exercises/03_dmn/getting-started/#getting-started-with-decision-model-and-notation","text":"This lab introduces you to the deployment of an existing Decision Model and Notation (DMN) and validation of it's decisions. Explore an existing DMN file created using Business Central Deploy the existing DMN project to Decision Server Test the deployed DMN","title":"Getting Started with Decision Model and Notation"},{"location":"guided_exercises/03_dmn/getting-started/#examine-existing-dmn-diagram","text":"The following example describes an insurance price calculator based on an applicant\u2019s age and accident history. This is a simple decision process based on the following decision table: DMN Decision Table The decision table was designed without using Business Central tools, but could be imported seemlesly due to the conformity with DMN specification. The DMN decision table includes a hit policy , inputs , and outputs . Unlike the drl based decision tables that can be created in Business Central, input and output names in DMN decision tables accept spaces. The conditions for the Age input is defined using the Friendly Enough Expression Language (FEEL). The decision can also be represented by the following decision requirements diagram: In this decision requirements diagram, note that the applicant\u2019s age and accident history are the required inputs for the decision table \"Insurance Total Price\". The DMN component is currently stored in the DMN GitHub repository .","title":"Examine Existing DMN Diagram"},{"location":"guided_exercises/03_dmn/getting-started/#import-the-dmn-file-into-kie-sandbox","text":"In this section, you will import the GitHub repository to KIE Sandbox directly. Copy to this Raw DMN file and click the Raw Button or simply copy the link from the below command. https://raw.githubusercontent.com/timwuthenow/dmn-workshop-labs/master/policy-price/insurance-pricing.dmn With this link navigate to the KIE Sandbox and under Import From URL paste the link from the previous step and click Import . When the project is imported, you will see the DMN Editor with the insurance-pricing DMN model displayed. If you instead of pointing to a particular DMN, pointed to an entire project, any DMN/BPMN models associated with it would be able to viewed/edited within KIE Sandbox. You can then click the Run button to get a local copy of this DMN running within the browser session itself. This will have a section of the browser turn into a form and you can run the model right there. Modifying the checkbox based on the boolean of had previous incidents and set an Age based on the data type being a number. \ud83d\udcd8 INFO: There is a known issue around the DMN Runner with forms that does not initially assume a non-checked box is false on the first successful execution. To quickly get around this, check and uncheck the checkbox and move forward with your testing If you link your OpenShift login to the gear icon, you can even do a sample deployment of this as a service into OpenShift. For now this will conclude this section.","title":"Import the DMN File into KIE Sandbox"},{"location":"guided_exercises/03_dmn/getting-started/#importing-a-dmn-in-business-central","text":"If you want to try this in Business Central, you can go through the following steps, but it is not required. From the GitHub web page, click Clone or download on the right and then select Download ZIP : Using your favorite file system navigation tool, locate the downloaded ZIP file and unzip it to a directory in your file system. From this point forward, this location is referred to as $PROJECT_HOME . Log in to Business Central. You can use either bamAdmin:ibmpam1! or pamadmin:pamadm1n to do so or whatever login you have created on your instance. Create a project in Business Central called policy-price . In the empty project library view for the policy-price project, click Import Asset . In the Create new Uploaded file dialog, enter insurance-pricing.dmn in the Uploaded file field: Using the browse button at the far right of the field labeled Please select a file to upload , navigate with the file browser to the $PROJECT_HOME directory where the unzipped Git repository is located. Select the $PROJECT_HOME\\policy-price\\insurance-pricing.dmn file. Click Ok to import the DMN asset. The diagram will open and you will be able to see the DRD. Explore the diagram nodes to check the decision policies of this diagram. Close the diagram. You should now be on the library view for the policy-price project. You should see the insurance-pricing asset is added to your project assets: From the policy-price project\u2019s library view, click Build , then Deploy to deploy the project to the execution server. After receiving the build confirmation, navigate to the container deployment list by clicking the \" View deployment details \" link in the confirmation pop-up, or by selecting Menu \u2192 Deploy \u2192 Execution Servers . Verify that policy-price_2.0.0 shows a green status:","title":"Importing a DMN in Business Central"},{"location":"guided_exercises/03_dmn/getting-started/#testing-the-decision-service-on-kie-server","text":"In this section, you test the DMN solution using the REST endpoints available in the Decision Server (a.k.a. KIE Server). Open your Decision Server (a.k.a KIE Server) on the url \"/docs\". You should see something like this: Next, under DMN Models , click on the POST /server/containers/{containerId}/dmn\" and select \"Try it out\": Now use the following data: Container ID: policy-price Body (dmn context): {\"dmn-context\": {\"Age\": 20, \"had previous incidents\": false}} Parameter content type: application/json Click on the execute button. You should see the server response 200 and the results of the decision. Try out the Decision with different values for the age and accident history, and compare the results with the decision table:","title":"Testing the Decision Service on KIE Server"},{"location":"guided_exercises/03_dmn/getting-started/#conclusion","text":"Congratulations, you've finished the getting started exercise. Next, you will have an intermediate level exercise that will guide you through the implementation, deployment and testing of the Vacation Days use case.","title":"Conclusion"},{"location":"guided_exercises/03_dmn/intermediate-lab-authoring/","text":"Vacation Days - Authoring Decisions Let's work on the decision model. Input Nodes The problem statement describes a number of different inputs to our decision: Age of the employee Years of Service of the employee Therefore, we should create two input nodes, one for each input: Add an Input node to the diagram by clicking on the Input node icon and placing it in the DRD. Double-click on the node to set the name. We will name this node Age . With the Age node selected, open the property panel. Set the data type to number . In the same way, create an Input node for Years of Service . This node should also have its data type set to number . Save the model. Constants The problem statement describes that every employee receives at least 22 days. So, if no other decisions apply, an employee receives 22 days. This is can be seen as a constant input value into our decision model. In DMN we can model such constant inputs with a Decision node with a Literal boxed expression that defines the constant value: Add a Decision node to the DRD Give the node the name Base Vacation Days . Click on the node to select it and open the property panel. Set the node\u2019s data type to number . Click on the node and click on the Edit icon to open the expression editor. In the expression editor, click on the box that says Select expression and select Literal expression . Simply set the Literal Expression to 22 , the number of base vacation days defined in the problem statement. Save the model. Decisions The problem statement defines 3 decisions which can cause extra days to be given to employees based on various criteria. Let\u2019s simply call these decision: Extra days case 1 Extra days case 2 Extra days case 3 Although these decisions could be implemented in a single decision node, we\u2019ve decided, in order to improve maintainability of the solution, to define these decisions in 3 separate decision nodes. In your DRD, create 3 decision nodes with these given names. Set their data types to number . We need to attach both input nodes, Age and Years of Service to all 3 decision nodes. We can do this by clicking on an Input node, clicking on its arrow icon, and attaching the arrow to the Decision node. Select the Extra days case 1 node and open its expression editor by clicking on the Edit button. Select the expression Decision Table to create a boxed expression implemented as a decision table. The first case defines 2 decisions which can be modelled with 2 rows in our decision table as such: employees younger than 18 or at least 60 years will receive 5 extra days, or \u2026 employees with at least 30 years of service will receive 5 extra days To add new lines to your table, right click the first column and select \"Insert below\" Note that the hit-policy of the decision table is by default set to U , which means Unique . This implies that only one rule is expected to fire for a given input. In this case however, we would like to set it to Collect Max , as, for a given input, multiple decisions might match, but we would like to collect the output from the rule with the highest number of additional vacation days. To do this, click on the U in the upper-left corner of the decision table. Now, set the Hit Policy to Collect and the Builtin Aggregator to MAX . Finally, we need to set the default result of the decision. This is the result that will be returned when none of the rules match the given input. This is done as follows: .. Select the output/result column of the decision table. In this case this is the column Extra days case 1 .. Open the properties panel on the right-side of the editor. .. Expand the Default output section. .. Set the Default output property to 0 . Save the model The other two decisions can be implemented in the same way. Now, implement the following two decision tables: Case 2: Case 3: Total Vacation Days The total vacation days needs to be determined from the base vacation days and the decisions taken by our 3 decision nodes. As such, we need to create a new Decision node, which takes the output of our 4 Decision nodes (3 decision tables and a literal expression) as input and determines the final output. To do this, we need to: Create a new Decision node in the model. Give the node the name Total Vacation Days and set its data type to number . Connect the 4 existing Decision nodes to the node. This defines that the output of these nodes will be the input of the next node. Click on the Total Vacation Days node and click on Edit to open the expression editor. Configure the expression as a literal expression. We need to configure the following logic: Everyone gets the Base Vacation Days. If both case 1 and case 3 add extra days, only the extra days of one of this decision is added. So, in that case we take the maximum. If case 2 adds extra days, add them to the total. The above logic can be implemented with the following FEEL expression: Save the completed model. Next steps We're done. Next, we should deploy the project in KIE Server and test the model using the REST and Java API.","title":"Vacation Days - Authoring Decisions"},{"location":"guided_exercises/03_dmn/intermediate-lab-authoring/#vacation-days-authoring-decisions","text":"Let's work on the decision model.","title":"Vacation Days - Authoring Decisions"},{"location":"guided_exercises/03_dmn/intermediate-lab-authoring/#input-nodes","text":"The problem statement describes a number of different inputs to our decision: Age of the employee Years of Service of the employee Therefore, we should create two input nodes, one for each input: Add an Input node to the diagram by clicking on the Input node icon and placing it in the DRD. Double-click on the node to set the name. We will name this node Age . With the Age node selected, open the property panel. Set the data type to number . In the same way, create an Input node for Years of Service . This node should also have its data type set to number . Save the model.","title":"Input Nodes"},{"location":"guided_exercises/03_dmn/intermediate-lab-authoring/#constants","text":"The problem statement describes that every employee receives at least 22 days. So, if no other decisions apply, an employee receives 22 days. This is can be seen as a constant input value into our decision model. In DMN we can model such constant inputs with a Decision node with a Literal boxed expression that defines the constant value: Add a Decision node to the DRD Give the node the name Base Vacation Days . Click on the node to select it and open the property panel. Set the node\u2019s data type to number . Click on the node and click on the Edit icon to open the expression editor. In the expression editor, click on the box that says Select expression and select Literal expression . Simply set the Literal Expression to 22 , the number of base vacation days defined in the problem statement. Save the model.","title":"Constants"},{"location":"guided_exercises/03_dmn/intermediate-lab-authoring/#decisions","text":"The problem statement defines 3 decisions which can cause extra days to be given to employees based on various criteria. Let\u2019s simply call these decision: Extra days case 1 Extra days case 2 Extra days case 3 Although these decisions could be implemented in a single decision node, we\u2019ve decided, in order to improve maintainability of the solution, to define these decisions in 3 separate decision nodes. In your DRD, create 3 decision nodes with these given names. Set their data types to number . We need to attach both input nodes, Age and Years of Service to all 3 decision nodes. We can do this by clicking on an Input node, clicking on its arrow icon, and attaching the arrow to the Decision node. Select the Extra days case 1 node and open its expression editor by clicking on the Edit button. Select the expression Decision Table to create a boxed expression implemented as a decision table. The first case defines 2 decisions which can be modelled with 2 rows in our decision table as such: employees younger than 18 or at least 60 years will receive 5 extra days, or \u2026 employees with at least 30 years of service will receive 5 extra days To add new lines to your table, right click the first column and select \"Insert below\" Note that the hit-policy of the decision table is by default set to U , which means Unique . This implies that only one rule is expected to fire for a given input. In this case however, we would like to set it to Collect Max , as, for a given input, multiple decisions might match, but we would like to collect the output from the rule with the highest number of additional vacation days. To do this, click on the U in the upper-left corner of the decision table. Now, set the Hit Policy to Collect and the Builtin Aggregator to MAX . Finally, we need to set the default result of the decision. This is the result that will be returned when none of the rules match the given input. This is done as follows: .. Select the output/result column of the decision table. In this case this is the column Extra days case 1 .. Open the properties panel on the right-side of the editor. .. Expand the Default output section. .. Set the Default output property to 0 . Save the model The other two decisions can be implemented in the same way. Now, implement the following two decision tables: Case 2: Case 3:","title":"Decisions"},{"location":"guided_exercises/03_dmn/intermediate-lab-authoring/#total-vacation-days","text":"The total vacation days needs to be determined from the base vacation days and the decisions taken by our 3 decision nodes. As such, we need to create a new Decision node, which takes the output of our 4 Decision nodes (3 decision tables and a literal expression) as input and determines the final output. To do this, we need to: Create a new Decision node in the model. Give the node the name Total Vacation Days and set its data type to number . Connect the 4 existing Decision nodes to the node. This defines that the output of these nodes will be the input of the next node. Click on the Total Vacation Days node and click on Edit to open the expression editor. Configure the expression as a literal expression. We need to configure the following logic: Everyone gets the Base Vacation Days. If both case 1 and case 3 add extra days, only the extra days of one of this decision is added. So, in that case we take the maximum. If case 2 adds extra days, add them to the total. The above logic can be implemented with the following FEEL expression: Save the completed model.","title":"Total Vacation Days"},{"location":"guided_exercises/03_dmn/intermediate-lab-authoring/#next-steps","text":"We're done. Next, we should deploy the project in KIE Server and test the model using the REST and Java API.","title":"Next steps"},{"location":"guided_exercises/03_dmn/intermediate-lab-deployment/","text":"Vacation Days - Consuming Decisions The first thing we should do is deploy the project. We'll deploy it in KIE Server using Business Central. Deploying the Decision Service With our decision model completed, we can now package our DMN model in a Deployment Unit (KJAR) and deploy it on the Execution Server. To do this: In the bread-crumb navigation in the upper-left corner, click on vacation-days-decisions to go back to the project\u2019s Library View. Click on the Deploy button in the upper-right corner of the screen. This will package our DMN mode in a Deployment Unit (KJAR) and deploy it onto the Execution Server (KIE-Server). Go to the Execution Servers perspective by clicking on \"Menu \u2192 Deploy \u2192 Execution Servers\". You will see the Deployment Unit deployed on the Execution Server. Testing DMN Solution In this section, you will test the DMN solution with Execution Server\u2019s Swagger interface and via Java KIE Client API. Testing the solution via REST API In this section, you will test the DMN solution with KIE Server\u2019s Swagger interface. The Swagger interface provides the description and documentation of the Execution Server\u2019s RESTful API. At the same time, it allows the APIs to be called from the UI. This enables developers and users to quickly test, in this case, a deployed DMN Service. Navigate to KIE Server Locate the DMN Models section. The DMN API provides the DMN model as a RESTful resources, which accepts 2 operations: GET : Retrieves the DMN model. POST : Evaluates the decisions for a given input. Expand the GET operation by clicking on it. Click on the Try it out button. Set the containerId field to vacation-days-decisions and set the Response content type to application/json and click on Execute If requested, provide the username and password of your Business Central and KIE-Server user. The response will be the model-description of your DMN model. Next, we will evaluate our model with some input data. We need to provide our model with the age of an employee and the number of years of service . Let\u2019s try a number of different values to test our deicions. Expand the POST operation and click on the Try it out button Set the containerId field to vacation-days-decisions . Set the Parameter content type and Response content type fields to application/json . Pass the following request to lookup the number of vacation days for an employee of 16 years old with 1 year of service (note that the namespace of your model is probably different as it is generated. You can lookup the namespace of your model in the response/result of the GET operation you executed ealier, which returned the model description). { \"dmn-context\" :{ \"Age\" : 16 , \"Years of Service\" : 1 } } Click on Execute . The result value of the Total Vacation Days should be 27. Test the service with a number of other values. See the following table for some sample values and expected output. Age Years of Service Total Vacation Days 16 1 27 25 5 22 44 20 24 44 30 30 50 20 24 50 30 30 60 20 30 Using the KIE Java Client IBM Decision Manager provides a KIE Java Client API that allows the user to interact with the KIE-Server from a Java client using a higher level API. It abstracts the data marshalling and unmarshalling and the creation and execution of the RESTful commands from the developer, allowing him/her to focus on developing business logic. In this section we will create a simple Java client for our DMN model. IMPORTANT: If your KIE Server is exposed via https you need to configure the `javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword in the Java client code using the Remote Java API. If not, you may get a rest.NoEndpointFoundException`. For more information check this solution Red Hat's knowledge base. Create a new Maven Java JAR project in your favourite IDE (e.g. IntelliJ, Eclipse, Visual Studio Code). Add the following dependency to your project: <dependency> <groupId> org.kie.server </groupId> <artifactId> kie-server-client </artifactId> <version> 7.67.0.Final-redhat-00008 </version> <scope> compile </scope> </dependency> Create a Java package in your src/main/java folder with the name org.kie.dmn.lab . In the package you\u2019ve just created, create a Java class called Main.java . Add a public static void main(String[] args) method to your main class. Before we implement our method, we first define a number of constants that we will need when implementing our method (note that the values of your constants can be different depending on your environment, model namespace, etc.): private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"vacation-days-decisions\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. ~~~java private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\"; private static final String CONTAINER_ID = \"vacation-days-decisions\"; private static final String USERNAME = \"pamadmin\"; private static final String PASSWORD = \"pamadm1n\"; ~~~ KIE-Server client API classes can mostly be retrieved from the KieServicesFactory class. We first need to create a KieServicesConfiguration instance that will hold our credentials and defines how we want our client to communicate with the server: KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , new EnteredCredentialsProvider ( USERNAME , PASSWORD )); Next, we create the KieServicesClient : KieServicesClient kieServicesClient = KieServicesFactory . newKieServicesClient ( kieServicesConfig ); From this client we retrieve our DMNServicesClient: DMNServicesClient dmnServicesClient = kieServicesClient . getServicesClient ( DMNServicesClient . class ); To pass the input values to our model to the Execution Server, we need to create a DMNContext : DMNContext dmnContext = dmnServicesClient . newContext (); dmnContext . set ( \"Age\" , 16 ); dmnContext . set ( \"Years of Service\" , 1 ); We now have defined all the required instances needed to send a DMN evaluation request to the server: ServiceResponse < DMNResult > dmnResultResponse = dmnServicesClient . evaluateAll ( CONTAINER_ID , dmnContext ); Finally we can retrieve the DMN evaluation result and print it in the console: DMNDecisionResult decisionResult = dmnResultResponse . getResult (). getDecisionResultByName ( \"Total Vacation Days\" ); System . out . println ( \"Total vacation days: \" + decisionResult . getResult ()); Compile your project and run it. Observe the output in the console, which should say: Total vacation days: 27 The complete project can be found here package org.kie.dmn.lab ; import org.kie.api.builder.KieScannerFactoryService ; import org.kie.api.internal.weaver.KieWeaverService ; import org.kie.dmn.api.core.DMNContext ; import org.kie.dmn.api.core.DMNDecisionResult ; import org.kie.dmn.api.core.DMNResult ; import org.kie.server.api.model.ServiceResponse ; import org.kie.server.client.CredentialsProvider ; import org.kie.server.client.DMNServicesClient ; import org.kie.server.client.KieServicesClient ; import org.kie.server.client.KieServicesConfiguration ; import org.kie.server.client.KieServicesFactory ; import org.kie.server.client.credentials.EnteredCredentialsProvider ; /** * Vacation Days DMN Client */ public class Main { private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"vacation-days-decisions\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; // Comment out the above 2 lines if using the Skytap image and uncomment the two below to use those logins // private static final String USERNAME = \"pamadmin\"; // private static final String PASSWORD = \"pamadm1n\"; public static void main ( String [] args ) { CredentialsProvider credentialsProvider = new EnteredCredentialsProvider ( USERNAME , PASSWORD ); KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , credentialsProvider ); KieServicesClient kieServicesClient = KieServicesFactory . newKieServicesClient ( kieServicesConfig ); DMNServicesClient dmnServicesClient = kieServicesClient . getServicesClient ( DMNServicesClient . class ); DMNContext dmnContext = dmnServicesClient . newContext (); dmnContext . set ( \"Age\" , 16 ); dmnContext . set ( \"Years of Service\" , 1 ); ServiceResponse < DMNResult > dmnResultResponse = dmnServicesClient . evaluateAll ( CONTAINER_ID , dmnContext ); DMNDecisionResult decisionResult = dmnResultResponse . getResult (). getDecisionResultByName ( \"Total Vacation Days\" ); System . out . println ( \"Total vacation days: \" + decisionResult . getResult ()); } }","title":"Vacation Days - Consuming Decisions"},{"location":"guided_exercises/03_dmn/intermediate-lab-deployment/#vacation-days-consuming-decisions","text":"The first thing we should do is deploy the project. We'll deploy it in KIE Server using Business Central.","title":"Vacation Days - Consuming Decisions"},{"location":"guided_exercises/03_dmn/intermediate-lab-deployment/#deploying-the-decision-service","text":"With our decision model completed, we can now package our DMN model in a Deployment Unit (KJAR) and deploy it on the Execution Server. To do this: In the bread-crumb navigation in the upper-left corner, click on vacation-days-decisions to go back to the project\u2019s Library View. Click on the Deploy button in the upper-right corner of the screen. This will package our DMN mode in a Deployment Unit (KJAR) and deploy it onto the Execution Server (KIE-Server). Go to the Execution Servers perspective by clicking on \"Menu \u2192 Deploy \u2192 Execution Servers\". You will see the Deployment Unit deployed on the Execution Server.","title":"Deploying the Decision Service"},{"location":"guided_exercises/03_dmn/intermediate-lab-deployment/#testing-dmn-solution","text":"In this section, you will test the DMN solution with Execution Server\u2019s Swagger interface and via Java KIE Client API.","title":"Testing DMN Solution"},{"location":"guided_exercises/03_dmn/intermediate-lab-deployment/#testing-the-solution-via-rest-api","text":"In this section, you will test the DMN solution with KIE Server\u2019s Swagger interface. The Swagger interface provides the description and documentation of the Execution Server\u2019s RESTful API. At the same time, it allows the APIs to be called from the UI. This enables developers and users to quickly test, in this case, a deployed DMN Service. Navigate to KIE Server Locate the DMN Models section. The DMN API provides the DMN model as a RESTful resources, which accepts 2 operations: GET : Retrieves the DMN model. POST : Evaluates the decisions for a given input. Expand the GET operation by clicking on it. Click on the Try it out button. Set the containerId field to vacation-days-decisions and set the Response content type to application/json and click on Execute If requested, provide the username and password of your Business Central and KIE-Server user. The response will be the model-description of your DMN model. Next, we will evaluate our model with some input data. We need to provide our model with the age of an employee and the number of years of service . Let\u2019s try a number of different values to test our deicions. Expand the POST operation and click on the Try it out button Set the containerId field to vacation-days-decisions . Set the Parameter content type and Response content type fields to application/json . Pass the following request to lookup the number of vacation days for an employee of 16 years old with 1 year of service (note that the namespace of your model is probably different as it is generated. You can lookup the namespace of your model in the response/result of the GET operation you executed ealier, which returned the model description). { \"dmn-context\" :{ \"Age\" : 16 , \"Years of Service\" : 1 } } Click on Execute . The result value of the Total Vacation Days should be 27. Test the service with a number of other values. See the following table for some sample values and expected output. Age Years of Service Total Vacation Days 16 1 27 25 5 22 44 20 24 44 30 30 50 20 24 50 30 30 60 20 30","title":"Testing the solution via REST API"},{"location":"guided_exercises/03_dmn/intermediate-lab-deployment/#using-the-kie-java-client","text":"IBM Decision Manager provides a KIE Java Client API that allows the user to interact with the KIE-Server from a Java client using a higher level API. It abstracts the data marshalling and unmarshalling and the creation and execution of the RESTful commands from the developer, allowing him/her to focus on developing business logic. In this section we will create a simple Java client for our DMN model. IMPORTANT: If your KIE Server is exposed via https you need to configure the `javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword in the Java client code using the Remote Java API. If not, you may get a rest.NoEndpointFoundException`. For more information check this solution Red Hat's knowledge base. Create a new Maven Java JAR project in your favourite IDE (e.g. IntelliJ, Eclipse, Visual Studio Code). Add the following dependency to your project: <dependency> <groupId> org.kie.server </groupId> <artifactId> kie-server-client </artifactId> <version> 7.67.0.Final-redhat-00008 </version> <scope> compile </scope> </dependency> Create a Java package in your src/main/java folder with the name org.kie.dmn.lab . In the package you\u2019ve just created, create a Java class called Main.java . Add a public static void main(String[] args) method to your main class. Before we implement our method, we first define a number of constants that we will need when implementing our method (note that the values of your constants can be different depending on your environment, model namespace, etc.): private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"vacation-days-decisions\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. ~~~java private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\"; private static final String CONTAINER_ID = \"vacation-days-decisions\"; private static final String USERNAME = \"pamadmin\"; private static final String PASSWORD = \"pamadm1n\"; ~~~ KIE-Server client API classes can mostly be retrieved from the KieServicesFactory class. We first need to create a KieServicesConfiguration instance that will hold our credentials and defines how we want our client to communicate with the server: KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , new EnteredCredentialsProvider ( USERNAME , PASSWORD )); Next, we create the KieServicesClient : KieServicesClient kieServicesClient = KieServicesFactory . newKieServicesClient ( kieServicesConfig ); From this client we retrieve our DMNServicesClient: DMNServicesClient dmnServicesClient = kieServicesClient . getServicesClient ( DMNServicesClient . class ); To pass the input values to our model to the Execution Server, we need to create a DMNContext : DMNContext dmnContext = dmnServicesClient . newContext (); dmnContext . set ( \"Age\" , 16 ); dmnContext . set ( \"Years of Service\" , 1 ); We now have defined all the required instances needed to send a DMN evaluation request to the server: ServiceResponse < DMNResult > dmnResultResponse = dmnServicesClient . evaluateAll ( CONTAINER_ID , dmnContext ); Finally we can retrieve the DMN evaluation result and print it in the console: DMNDecisionResult decisionResult = dmnResultResponse . getResult (). getDecisionResultByName ( \"Total Vacation Days\" ); System . out . println ( \"Total vacation days: \" + decisionResult . getResult ()); Compile your project and run it. Observe the output in the console, which should say: Total vacation days: 27 The complete project can be found here package org.kie.dmn.lab ; import org.kie.api.builder.KieScannerFactoryService ; import org.kie.api.internal.weaver.KieWeaverService ; import org.kie.dmn.api.core.DMNContext ; import org.kie.dmn.api.core.DMNDecisionResult ; import org.kie.dmn.api.core.DMNResult ; import org.kie.server.api.model.ServiceResponse ; import org.kie.server.client.CredentialsProvider ; import org.kie.server.client.DMNServicesClient ; import org.kie.server.client.KieServicesClient ; import org.kie.server.client.KieServicesConfiguration ; import org.kie.server.client.KieServicesFactory ; import org.kie.server.client.credentials.EnteredCredentialsProvider ; /** * Vacation Days DMN Client */ public class Main { private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"vacation-days-decisions\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; // Comment out the above 2 lines if using the Skytap image and uncomment the two below to use those logins // private static final String USERNAME = \"pamadmin\"; // private static final String PASSWORD = \"pamadm1n\"; public static void main ( String [] args ) { CredentialsProvider credentialsProvider = new EnteredCredentialsProvider ( USERNAME , PASSWORD ); KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , credentialsProvider ); KieServicesClient kieServicesClient = KieServicesFactory . newKieServicesClient ( kieServicesConfig ); DMNServicesClient dmnServicesClient = kieServicesClient . getServicesClient ( DMNServicesClient . class ); DMNContext dmnContext = dmnServicesClient . newContext (); dmnContext . set ( \"Age\" , 16 ); dmnContext . set ( \"Years of Service\" , 1 ); ServiceResponse < DMNResult > dmnResultResponse = dmnServicesClient . evaluateAll ( CONTAINER_ID , dmnContext ); DMNDecisionResult decisionResult = dmnResultResponse . getResult (). getDecisionResultByName ( \"Total Vacation Days\" ); System . out . println ( \"Total vacation days: \" + decisionResult . getResult ()); } }","title":"Using the KIE Java Client"},{"location":"guided_exercises/03_dmn/intermediate-lab-intro/","text":"Vacation Days - Use case and project creation In this lab you'll try out the combination of DMN decision tables with literal expressions. You will also explore a number of different FEEL constructs and expressions like, for example, ranges. Finally, you'll learn how to use the KIE Java Client to consume decisions. Goal Implement a DMN model using the Red Hat DM/PAM DMN editor Deploy the existing DMN project to Decision Server Consume the DMN project using the REST API Consume the DMN project using a Java API Problem Statement In this lab we will create a decision that determines the number of vacation days assigned to an employee. The number of vacation days depends on age and years of service. Every employee receives at least 22 days. Additional days are provided according to the following criteria: Only employees younger than 18 or at least 60 years, or employees with at least 30 years of service will receive 5 extra days; Employees with at least 30 years of service and also employees of age 60 or more, receive 3 extra days, on top of possible additional days already given; If an employee has at least 15 but less than 30 years of service, 2 extra days are given. These 2 days are also provided for employees of age 45 or more. These 2 extra days can not be combined with the 5 extra days. Link KIE Sandbox to your GitHub account and create a project in GitHub using features coming soon If you use the following URL for the KIE Sandbox you can get an early look at the work that's going on to create a new repository around starting a model from the model out. To do this, you need to make sure your connection to GitHub is active with the token and from there you can jump right in! Link GitHub to your KIE Sandbox In this section we will link your GitHub account to the KIE Sandbox so we can easily synchronize changes in DMN with GitHub and our tooling, in this case KIE Sandbox. First click the Gear icon and validate that your GitHub token is in, if not, follow the instructions to connect your public GitHub account to the KIE Sandbox. Insert your token, if it is not already there to connect your account. There is a link on the page Create a new token that will bring you to the GitHub page to do so. Click Generate new token to create a new token that will be used by KIE Sandbox You can use similar properties to the token created below in the screenshot, but the main 2 to have right now are repo and gist - the others can be beneficial if you reuse this token for other purposes too, but not required. You can change the date to never expiring or be as short as you want. Once the token is generated though, that is the only time you will see the actual token value. Name : Name your token a unique name from any previously created Expiration : This can either be a set time period, up to 1 year or never expiring The checkboxes you need are repo and gist to get the full benefit of KIE Sandbox Use the copy button that's created with the Token to use in KIE Sandbox. Return to KIE Sandbox and insert the Token into the wizard. When your token is pasted, the KIE Sandbox will return a similar screen to below towards your GitHub account. Create a DMN Model and Create a Project in KIE Sandbox Now that our account is linked, let's go ahead and create a new DMN model and then later produce a project for it that will reside in our GitHub repositories. KIE Sandbox provides an excellent way to both push and pull from your GitHub repositories. Right now, the KIE Sandbox can just pull from public repositories. This is something that's being actively developed, so keep checking back into KIE Sandbox's Page for more recently released features frequently! When navigating from {{ no such element: dict object['experimental'] }} Create a Decision Project using Business Central (the Old Way) To define and deploy a DMN decision model, we first need to create a new project in which we can store the model. To create a new project: Navigate to Business Central Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. 1. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name vacation-days-decisions , and the description \"Vacation Days Decisions\". With the project created, we can now create our DMN model. Click on the blue Add Asset button. In the Add Asset page, select Decision in the dropdown filter selector. Click on the DMN tile to create a new DMN model. Give it the name vacation-days . This will create the asset and open the DMN editor. Next Steps You can do this lab in 2 ways: If you already have (some) DMN knowledge, we would like to challenge you to build the solution by yourself. After you\u2019ve built solution, you can verify your answer by going to the next module in which we will explain the solution and will deploy it onto the runtime. Follow the next step with contains a step-by-step guide and will guide you through the implementation.","title":"Intermediate DMN Exercises - Vacation Days - Introduction"},{"location":"guided_exercises/03_dmn/intermediate-lab-intro/#vacation-days-use-case-and-project-creation","text":"In this lab you'll try out the combination of DMN decision tables with literal expressions. You will also explore a number of different FEEL constructs and expressions like, for example, ranges. Finally, you'll learn how to use the KIE Java Client to consume decisions.","title":"Vacation Days - Use case and project creation"},{"location":"guided_exercises/03_dmn/intermediate-lab-intro/#goal","text":"Implement a DMN model using the Red Hat DM/PAM DMN editor Deploy the existing DMN project to Decision Server Consume the DMN project using the REST API Consume the DMN project using a Java API","title":"Goal"},{"location":"guided_exercises/03_dmn/intermediate-lab-intro/#problem-statement","text":"In this lab we will create a decision that determines the number of vacation days assigned to an employee. The number of vacation days depends on age and years of service. Every employee receives at least 22 days. Additional days are provided according to the following criteria: Only employees younger than 18 or at least 60 years, or employees with at least 30 years of service will receive 5 extra days; Employees with at least 30 years of service and also employees of age 60 or more, receive 3 extra days, on top of possible additional days already given; If an employee has at least 15 but less than 30 years of service, 2 extra days are given. These 2 days are also provided for employees of age 45 or more. These 2 extra days can not be combined with the 5 extra days.","title":"Problem Statement"},{"location":"guided_exercises/03_dmn/intermediate-lab-intro/#link-kie-sandbox-to-your-github-account-and-create-a-project-in-github-using-features-coming-soon","text":"If you use the following URL for the KIE Sandbox you can get an early look at the work that's going on to create a new repository around starting a model from the model out. To do this, you need to make sure your connection to GitHub is active with the token and from there you can jump right in!","title":"Link KIE Sandbox to your GitHub account and create a project in GitHub using features coming soon"},{"location":"guided_exercises/03_dmn/intermediate-lab-intro/#link-github-to-your-kie-sandbox","text":"In this section we will link your GitHub account to the KIE Sandbox so we can easily synchronize changes in DMN with GitHub and our tooling, in this case KIE Sandbox. First click the Gear icon and validate that your GitHub token is in, if not, follow the instructions to connect your public GitHub account to the KIE Sandbox. Insert your token, if it is not already there to connect your account. There is a link on the page Create a new token that will bring you to the GitHub page to do so. Click Generate new token to create a new token that will be used by KIE Sandbox You can use similar properties to the token created below in the screenshot, but the main 2 to have right now are repo and gist - the others can be beneficial if you reuse this token for other purposes too, but not required. You can change the date to never expiring or be as short as you want. Once the token is generated though, that is the only time you will see the actual token value. Name : Name your token a unique name from any previously created Expiration : This can either be a set time period, up to 1 year or never expiring The checkboxes you need are repo and gist to get the full benefit of KIE Sandbox Use the copy button that's created with the Token to use in KIE Sandbox. Return to KIE Sandbox and insert the Token into the wizard. When your token is pasted, the KIE Sandbox will return a similar screen to below towards your GitHub account.","title":"Link GitHub to your KIE Sandbox"},{"location":"guided_exercises/03_dmn/intermediate-lab-intro/#create-a-dmn-model-and-create-a-project-in-kie-sandbox","text":"Now that our account is linked, let's go ahead and create a new DMN model and then later produce a project for it that will reside in our GitHub repositories. KIE Sandbox provides an excellent way to both push and pull from your GitHub repositories. Right now, the KIE Sandbox can just pull from public repositories. This is something that's being actively developed, so keep checking back into KIE Sandbox's Page for more recently released features frequently! When navigating from {{ no such element: dict object['experimental'] }}","title":"Create a DMN Model and Create a Project in KIE Sandbox"},{"location":"guided_exercises/03_dmn/intermediate-lab-intro/#create-a-decision-project-using-business-central-the-old-way","text":"To define and deploy a DMN decision model, we first need to create a new project in which we can store the model. To create a new project: Navigate to Business Central Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. 1. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name vacation-days-decisions , and the description \"Vacation Days Decisions\". With the project created, we can now create our DMN model. Click on the blue Add Asset button. In the Add Asset page, select Decision in the dropdown filter selector. Click on the DMN tile to create a new DMN model. Give it the name vacation-days . This will create the asset and open the DMN editor.","title":"Create a Decision Project using Business Central (the Old Way)"},{"location":"guided_exercises/03_dmn/intermediate-lab-intro/#next-steps","text":"You can do this lab in 2 ways: If you already have (some) DMN knowledge, we would like to challenge you to build the solution by yourself. After you\u2019ve built solution, you can verify your answer by going to the next module in which we will explain the solution and will deploy it onto the runtime. Follow the next step with contains a step-by-step guide and will guide you through the implementation.","title":"Next Steps"},{"location":"guided_exercises/03_dmn/introduction/","text":"Introduction This is a series of guided exercises that will allow you to experiment the authoring of decisions using Decision Model and Notation - DMN. You will be able to experiment decision authoring in Business Central, along with the deployment and consumption of the decisions in the engine, KIE Server. What is DMN Take a look at the explanation of the DMN standard in the OMG website: \"DMN is a modeling language and notation for the precise specification of business decisions and business rules. DMN is easily readable by the different types of people involved in decision management. These include: business people who specify the rules and monitor their application; business analysts. DMN is designed to work alongside BPMN and/or CMMN, providing a mechanism to model the decision-making associated with processes and cases. While BPMN, CMMN and DMN can be used independently, they were carefully designed to be complementary. Indeed, many organizations require a combination of process models for their prescriptive workflows, case models for their reactive activities, and decision models for their more complex, multi-criteria business rules. Those organizations will benefit from using the three standards in combination, selecting which one is most appropriate to each type of activity modeling. This is why BPMN, CMMN and DMN really constitute the \u201ctriple crown\u201d of process improvement standards.\" IBM Business Automation Open Edition 8.0 and IBM Decision Manager bring a set of graphical tooling that allow you to author decisions using DMN and a lightweight engine that can execute these decisions. The engine and the authoring tooling set are decoupled and you can scale it independently. Tooling Set In IBM Business Automation Open Edition 8.0 you can author decisions in multiple ways - all using the exact same editor in different environments: KIE Sandbox found and import your project here or jump right into the editor at DMN.new for a light-weight browser experience Business Central found in IBAMOE Business Automation VSCode Extension A developer IDE ( Visual Studio Code ) extension that allows the visualization and editing of BPMN, DMN and Test Scenarios inside VSCode. There is also a set of community tooling that's also available for use. All the tools below are backed by IBM and Red Hat: DMN FEEL Handbook A handbook for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine. Learn DMN in 15 minutes A guided tour in a website through the elements of DMN GitHub Chrome Extension A browser extension that allows you to visualize and edit BPMN, DMN and Test Scenario files directly in GitHub. Online Editors BPMN.new - A free online editor for business processes; DMN.new - A free online editor for decision models; PMML.new - A free online editor for scorecards; Business Modeler Hub Allows for the download of the: VSCode extension, GitHub Chrome Extension, and Desktop App The DMN Editor The DMN Editor consists of a number of components: Decision Navigator : shows the nodes used in the Decision Requirements Diagram (DRD, the diagram), and the decisions behind the nodes. Allows for quick navigation through the model. Decision Requirements Diagram Editor : the canvas in which the model can be created. Palette : Contains all the DMN constructs that can be used in a DRD, e.g. Input Node, Decision Node, etc. Expression Editor : Editor in which DMN boxed expressions, like decision tables and literal expressions, can be created. Property Panel : provides access to the properties of the model (name, namespace, etc), nodes, etc. Data Types : allows the user to define (complex) datatypes. Guided Labs These are the labs you have available in this workshop: Insurance Price calculation: a getting started exercise. You will import an existing module, explore it, deploy it and test it using the decision engine's REST API. Vacation Days: an intermediate level exercise. You will author a model from scratch, use decision tables, work with different hit policies, different FEEL constructs and expressions. Finally, you will deploy it and test it using not only the decision engine's REST API but also the Java KIE Client API. Call Centre: an advanced level exercise. You will author a model from scratch, create data types, consume DMN decision services from within decision nodes, and more FEEL constructs and expressions. Finally, you will deploy it and test it using not only the decision engine's REST API but also the Java KIE Client API. These are independent guided exercises and you don't need to implement the previous use case to implement the next one.","title":"Introduction"},{"location":"guided_exercises/03_dmn/introduction/#introduction","text":"This is a series of guided exercises that will allow you to experiment the authoring of decisions using Decision Model and Notation - DMN. You will be able to experiment decision authoring in Business Central, along with the deployment and consumption of the decisions in the engine, KIE Server.","title":"Introduction"},{"location":"guided_exercises/03_dmn/introduction/#what-is-dmn","text":"Take a look at the explanation of the DMN standard in the OMG website: \"DMN is a modeling language and notation for the precise specification of business decisions and business rules. DMN is easily readable by the different types of people involved in decision management. These include: business people who specify the rules and monitor their application; business analysts. DMN is designed to work alongside BPMN and/or CMMN, providing a mechanism to model the decision-making associated with processes and cases. While BPMN, CMMN and DMN can be used independently, they were carefully designed to be complementary. Indeed, many organizations require a combination of process models for their prescriptive workflows, case models for their reactive activities, and decision models for their more complex, multi-criteria business rules. Those organizations will benefit from using the three standards in combination, selecting which one is most appropriate to each type of activity modeling. This is why BPMN, CMMN and DMN really constitute the \u201ctriple crown\u201d of process improvement standards.\" IBM Business Automation Open Edition 8.0 and IBM Decision Manager bring a set of graphical tooling that allow you to author decisions using DMN and a lightweight engine that can execute these decisions. The engine and the authoring tooling set are decoupled and you can scale it independently.","title":"What is DMN"},{"location":"guided_exercises/03_dmn/introduction/#tooling-set","text":"In IBM Business Automation Open Edition 8.0 you can author decisions in multiple ways - all using the exact same editor in different environments: KIE Sandbox found and import your project here or jump right into the editor at DMN.new for a light-weight browser experience Business Central found in IBAMOE Business Automation VSCode Extension A developer IDE ( Visual Studio Code ) extension that allows the visualization and editing of BPMN, DMN and Test Scenarios inside VSCode. There is also a set of community tooling that's also available for use. All the tools below are backed by IBM and Red Hat: DMN FEEL Handbook A handbook for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine. Learn DMN in 15 minutes A guided tour in a website through the elements of DMN GitHub Chrome Extension A browser extension that allows you to visualize and edit BPMN, DMN and Test Scenario files directly in GitHub. Online Editors BPMN.new - A free online editor for business processes; DMN.new - A free online editor for decision models; PMML.new - A free online editor for scorecards; Business Modeler Hub Allows for the download of the: VSCode extension, GitHub Chrome Extension, and Desktop App","title":"Tooling Set"},{"location":"guided_exercises/03_dmn/introduction/#the-dmn-editor","text":"The DMN Editor consists of a number of components: Decision Navigator : shows the nodes used in the Decision Requirements Diagram (DRD, the diagram), and the decisions behind the nodes. Allows for quick navigation through the model. Decision Requirements Diagram Editor : the canvas in which the model can be created. Palette : Contains all the DMN constructs that can be used in a DRD, e.g. Input Node, Decision Node, etc. Expression Editor : Editor in which DMN boxed expressions, like decision tables and literal expressions, can be created. Property Panel : provides access to the properties of the model (name, namespace, etc), nodes, etc. Data Types : allows the user to define (complex) datatypes.","title":"The DMN Editor"},{"location":"guided_exercises/03_dmn/introduction/#guided-labs","text":"These are the labs you have available in this workshop: Insurance Price calculation: a getting started exercise. You will import an existing module, explore it, deploy it and test it using the decision engine's REST API. Vacation Days: an intermediate level exercise. You will author a model from scratch, use decision tables, work with different hit policies, different FEEL constructs and expressions. Finally, you will deploy it and test it using not only the decision engine's REST API but also the Java KIE Client API. Call Centre: an advanced level exercise. You will author a model from scratch, create data types, consume DMN decision services from within decision nodes, and more FEEL constructs and expressions. Finally, you will deploy it and test it using not only the decision engine's REST API but also the Java KIE Client API. These are independent guided exercises and you don't need to implement the previous use case to implement the next one.","title":"Guided Labs"},{"location":"guided_exercises/03c_CICD/introduction/","text":"IBM Business Automation Open Edition 8.0 CI/CD Overview This lab was forked from the KIE Live session delived by Rafael Soares on YouTube found here . This lab has been updated to more current releases and also includes a provisioning of Nexus3 to deploy a Maven Repository in your environment. You do not need to do this if you have a Maven repository you can deploy your artifacts to. This lab will be using the OpenShift provided Pipelines built upon Tekton. With IBM Business Automation Open Edition 8.0, the projects are based on Maven artifact architectures, which provides an easy to build and deploy mechanism for all of your product needs. These can be done in Tekton as seen here, or with your favorite/enterprise provided CI/CD tools. The project in this lab is a Spring Boot hosted Decision Service that is utilizing the embedded KIE Server that will have a build and deploy triggered through the changes in your code base that are pushed to the specified build branch. With Decision Service projects, it is highly recommended that on all push events you consider a build of the project. This ensures that all the resources that are required are constantly available, breaking changes are identified immediately and if you want to release immediately, it's very easy to do so. Lab Overview Within this lab you will see how you can use OpenShift Pipelines (a.k.a Tekton) to automate the delivery of decision services implemented with IBAMOE. In this lab you will see: The automation of repeatable decisions using the DMN specification; Decision tables implementation using XLS. Usage of the rules engine based on KIE Server and running on top of SpringBoot CI/CD Pipeline implemented using Tekton How to configure webhooks in your pipeline to deploy based on changes on a git repository Automated tests for decisions (with Test Scenarios ) that are considered during the pipeline execution Deployment with zero downtime with OpenShift rolling deployment strategy Pre-requisites Java 8 OpenShift 4.10+ OpenShift Command Line client ( oc ) VSCode VSCode Business Automation Extension Installing on OpenShift Fork this repository, to get started . Clone your fork to your local machine. git clone https://github.com/ ${ yourgithubuser } /business-automation-showcase.git cd business-automation-showcase Run the provisioning script (Linux/MacOS): sh provision.sh When the script runs, you will be presented with a prompt to enter a namespace name, which is the namespace that will be created for your environment to be deployed in. business-automation-cicd-showcase % ./provision.sh Input a namespace root - first letter lowercase namespace: tim-demo When this completes you will get a console with a few links, as well as an admin password to the Nexus environment that was created. ****************************************************************** Use this URL in your GitHub Webhook configuration for automatic deployment http://el-ba-cicd-event-listener-tim-demo-rhdm-kieserver-cicd.openshift.io Use this URL to access the front-end application: http://decision-service-webclient-tim-demo-rhdm-kieserver-cicd.openshift.io Use this URL to access the Nexus Repository: http://nexus3-tim-demo-rhdm-kieserver-cicd.openshift.io Use this password for admin access to Nexus 3: long-strung-out-password-used-for-nexus-repo-admin ****************************************************************** Configuring the automatic deployment on GitHub To configure the webhook for automated deployment, open your fork in your GitHub. Next, add a new webhook by opening \" Settings -> Webhook -> Add webhook button\". Fill the form with the information below: Payload URL : provided after the provisioning. You can also get it using the command: echo $( oc get route el-ba-cicd-event-listener --template = 'http://{{.spec.host}}' ) Content type : application/json Secret : empty Which events would you like to trigger this webhook? : Just the push event . At this point, you should already have a fully automated integration and deployment lifecycle for the business application. Any changes pushed to your repository will trigger the pipeline in your OpenShift cluster. Maven Repository Setup Typically an environment would already have a Maven repository included, but if you wanted to set one up and use your own, this is a quick run through of what's required to get the Maven Repository to connect to the current location of the IBM Business Automation Open Edition 8.0 Maven artifacts which reside in the Red Hat Maven General Access Repository. From the console log you got earlier, you would have received two messages with routes and password infromation for the Nexus repository location (the 3rd item) and the admin password (the commands to get these are found within the provision.sh file). If you hold Control (Linux/Windows) or Command (Mac) and click the link in VSCode, you will go to the location of your newly provisioned Nexus3 repository. By default, the Nexus repository includes a Mirror of Maven Central which anyone can publish to following this documentation and is the typical environment for Community releases, but enterprise releases of Open Source Software can be found here as well. The practice within Red Hat, has been to publish artifacts to its own repository found here as these are the enterprise releases and outside of community. No access is required to pull from this repository. We will configure our newly minted Nexus repository to have a Mirror of the Red Hat Maven General Access Repository so that for any product releases that are used, it will pull them from Red Hat Maven General Access Repository and publish them in the Nexus repository as a proxy. This is common in most enterprise environments instead of a direct connection to the repositories to offset downtime, have security checks, etc. To setup the mirrow follow these steps: From the route that was produced from the console output (or you can run oc get route | grep nexus where you will need to paste the output) and use that link to get to the Nexus admin screen. From here, using the admin password that was a part of your console output. Click Sign In . Alternatively the command to run is below. echo \" $( oc exec $( oc get pod -o template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}' | grep nexus ) -- cat /nexus-data/admin.password ) \" Copying the password, login as Admin with the password from the previous command. You will now have a Wizard popup to walk you through some setup, we're going to do very minimal changes: you can change the password (e.g. password ). Typically in this development/throwaway environment, enabling anonymous access is the best way to do this, so select that option Enable anonymous access . Then click finish. Now that the repository is ready to be configured, we are going to create a mirror of the Red Hat Maven General Access Repository so that we can pull resources through the proxy instead of direct. To do this, click the Gear icon ( ) to open the settings screen. From here click Repositories to view the current repositories and also to be able to create another. Here you will see a list of default repositorues. By default, Nexus will include the connection to Maven_Central , but we'd also like to add the Red Hat Maven General Access Repository connection to pull the resources from this repository. To do so, click Create Repository to build a new repository. Scroll down and we're going to create a maven2 (proxy) . The reason we're doing a proxy is that we're going to allow our repository to reach out to the Red Hat Maven General Access Repository to retrieve artifacts it does not already have, but if it has previously pulled them, will be stored in the Nexus repository's cache. This way if there are new releases, you can pull as required, but are not taking the entire repository as required. On the form that opens, you will add the following information (the name red_hat_ga needs to be exact to match the lab expectations): Name : red_hat_ga What type of artifacts does this repository store : Release Remote Storage : https://maven.repository.redhat.com/ga Click Create repository at the bottom to deploy the proxy instance of the Red Hat Maven General Access Repository Your repositories should now be setup and look similar to the below. First run of the Pipeline to configure Persistent Volume Claims for the workspaces With the first run of the pipeline, you may have to run the pipeline manually. To do so, login to the OpenShift console for your project. You can see the console URL by running oc whoami --show-console and that will return the console link, Command/Control click it to open from VS Code. Once logged in, make sure to go to our project that you created in the provision.sh script. Follow these steps to run the pipeline for the first time. Make sure you change your project to the one you created in provision.sh . From the cluster home page, you can click Pipelines and then click Pipelines From the Pipelines screen, confirm your namespace matches what you created in the first part of the lab and after that, click the kebab icon to open the menu to Start Pipeline . Once you click start pipeline, a form will come up to set some settings for the pipeline. These should only have to be the first time you run this pipeline. You will modify the ones below if you followed the previous section for the workspaces to align to the ConfigMap of settings.xml used by Maven to build and the Persistent Volumes used for workspace data (git clones and the Maven artifacts). maven-local-repo : click the dropdown menu and select PersistentVolumeClaim and in the Select a PVC dropdown that appears, select maven-repo-pvc maven-settings : click the drop down and select Config Map and select from the new dropdown custom-maven-settings shared-workspace : click the dropdown menu and select PersistentVolumeClaim and in the Select a PVC dropdown, select source-workspace-pvc With this complete you can click the Start button to begin the pipeline Testing GitHub and Pipeline integration If you run this test, a new deployment should be triggered. The pipeline will deploy the decision service for the first time. In your terminal, access your project folder. Commit and push. You can use this empty commit sample if you need: git commit -m \"an empty commit to test the pipeline\" --allow-empty git push origin master In OpenShift, access: \" Pipelines -> ba-cicd-pipeline -> Pipeline Runs \" and check the progress of your application deployment. Using the web application The web application allows you to interact with the deployed rules and decisions in a specific Decision Server (KieServer or Kogito runtime). To use the deployed web app to interact with the deployed decisions, first you need to set the KIE Server URL in the web app settings. The deployed decision service is now deployed and accessible. Get your deployed KIE Server route. You can use the command: echo \"http://\" $( oc get route business-application-service-route -n rhdm-kieserver-cicd | awk 'FNR > 1 {print $2}' ) \"/rest/server\" Open your web application. The URL was provided in the installation step. If you lost it, use the command oc get route decision-service-webclient --template = 'http://{{.spec.host}}' -n rhdm-kieserver-cicd In the web application, click on the settings icon on the top right corner. In the field Kie Server Base URL , insert KIE Server URL. You can use the \"Test Connection\" button to validate the communication between the two services, then Save. You should be able to test the available decisions and rules. With this, the whole demo is now set up and ready to use. NOTE: If you get interested in see how this webapp was developed the src code is available here Extra information The provisioning script provision.sh will: Create a new namespace called rhdm-kieserver-cicd Install OpenShift Pipelines Create the pipeline resources Deploy a front-end application that you can use to interact with the decision service once you deploy it. At the moment there are 4 projects in this repository: decisions-showcase : Decision use cases using Business Rules (Drools) and Decision Logic (DMN) business-application-service : Spring Boot runtime based Kie Server exposing the API for Decisions provided with this Showcase demo cicd : Tekton Pipeline resources to implement a fully automated CI/CD pipeline for your Business Application Services monitoring : working in progress... To see a detailed instruction on each service and each deployment processes (with images), check: Provisioning and testing the CI/CD Pipeline Provisioning and testing the webclient application Interested in Kogito? Check out the kogito-quarkus branch to see this same demo but using Kogito based Decision Services instead of KieServer.","title":"IBM Business Automation Open Edition 8.0 CI/CD Overview"},{"location":"guided_exercises/03c_CICD/introduction/#ibm-business-automation-open-edition-80-cicd-overview","text":"This lab was forked from the KIE Live session delived by Rafael Soares on YouTube found here . This lab has been updated to more current releases and also includes a provisioning of Nexus3 to deploy a Maven Repository in your environment. You do not need to do this if you have a Maven repository you can deploy your artifacts to. This lab will be using the OpenShift provided Pipelines built upon Tekton. With IBM Business Automation Open Edition 8.0, the projects are based on Maven artifact architectures, which provides an easy to build and deploy mechanism for all of your product needs. These can be done in Tekton as seen here, or with your favorite/enterprise provided CI/CD tools. The project in this lab is a Spring Boot hosted Decision Service that is utilizing the embedded KIE Server that will have a build and deploy triggered through the changes in your code base that are pushed to the specified build branch. With Decision Service projects, it is highly recommended that on all push events you consider a build of the project. This ensures that all the resources that are required are constantly available, breaking changes are identified immediately and if you want to release immediately, it's very easy to do so.","title":"IBM Business Automation Open Edition 8.0 CI/CD Overview"},{"location":"guided_exercises/03c_CICD/introduction/#lab-overview","text":"Within this lab you will see how you can use OpenShift Pipelines (a.k.a Tekton) to automate the delivery of decision services implemented with IBAMOE. In this lab you will see: The automation of repeatable decisions using the DMN specification; Decision tables implementation using XLS. Usage of the rules engine based on KIE Server and running on top of SpringBoot CI/CD Pipeline implemented using Tekton How to configure webhooks in your pipeline to deploy based on changes on a git repository Automated tests for decisions (with Test Scenarios ) that are considered during the pipeline execution Deployment with zero downtime with OpenShift rolling deployment strategy","title":"Lab Overview"},{"location":"guided_exercises/03c_CICD/introduction/#pre-requisites","text":"Java 8 OpenShift 4.10+ OpenShift Command Line client ( oc ) VSCode VSCode Business Automation Extension","title":"Pre-requisites"},{"location":"guided_exercises/03c_CICD/introduction/#installing-on-openshift","text":"Fork this repository, to get started . Clone your fork to your local machine. git clone https://github.com/ ${ yourgithubuser } /business-automation-showcase.git cd business-automation-showcase Run the provisioning script (Linux/MacOS): sh provision.sh When the script runs, you will be presented with a prompt to enter a namespace name, which is the namespace that will be created for your environment to be deployed in. business-automation-cicd-showcase % ./provision.sh Input a namespace root - first letter lowercase namespace: tim-demo When this completes you will get a console with a few links, as well as an admin password to the Nexus environment that was created. ****************************************************************** Use this URL in your GitHub Webhook configuration for automatic deployment http://el-ba-cicd-event-listener-tim-demo-rhdm-kieserver-cicd.openshift.io Use this URL to access the front-end application: http://decision-service-webclient-tim-demo-rhdm-kieserver-cicd.openshift.io Use this URL to access the Nexus Repository: http://nexus3-tim-demo-rhdm-kieserver-cicd.openshift.io Use this password for admin access to Nexus 3: long-strung-out-password-used-for-nexus-repo-admin ******************************************************************","title":"Installing on OpenShift"},{"location":"guided_exercises/03c_CICD/introduction/#configuring-the-automatic-deployment-on-github","text":"To configure the webhook for automated deployment, open your fork in your GitHub. Next, add a new webhook by opening \" Settings -> Webhook -> Add webhook button\". Fill the form with the information below: Payload URL : provided after the provisioning. You can also get it using the command: echo $( oc get route el-ba-cicd-event-listener --template = 'http://{{.spec.host}}' ) Content type : application/json Secret : empty Which events would you like to trigger this webhook? : Just the push event . At this point, you should already have a fully automated integration and deployment lifecycle for the business application. Any changes pushed to your repository will trigger the pipeline in your OpenShift cluster.","title":"Configuring the automatic deployment on GitHub"},{"location":"guided_exercises/03c_CICD/introduction/#maven-repository-setup","text":"Typically an environment would already have a Maven repository included, but if you wanted to set one up and use your own, this is a quick run through of what's required to get the Maven Repository to connect to the current location of the IBM Business Automation Open Edition 8.0 Maven artifacts which reside in the Red Hat Maven General Access Repository. From the console log you got earlier, you would have received two messages with routes and password infromation for the Nexus repository location (the 3rd item) and the admin password (the commands to get these are found within the provision.sh file). If you hold Control (Linux/Windows) or Command (Mac) and click the link in VSCode, you will go to the location of your newly provisioned Nexus3 repository. By default, the Nexus repository includes a Mirror of Maven Central which anyone can publish to following this documentation and is the typical environment for Community releases, but enterprise releases of Open Source Software can be found here as well. The practice within Red Hat, has been to publish artifacts to its own repository found here as these are the enterprise releases and outside of community. No access is required to pull from this repository. We will configure our newly minted Nexus repository to have a Mirror of the Red Hat Maven General Access Repository so that for any product releases that are used, it will pull them from Red Hat Maven General Access Repository and publish them in the Nexus repository as a proxy. This is common in most enterprise environments instead of a direct connection to the repositories to offset downtime, have security checks, etc. To setup the mirrow follow these steps: From the route that was produced from the console output (or you can run oc get route | grep nexus where you will need to paste the output) and use that link to get to the Nexus admin screen. From here, using the admin password that was a part of your console output. Click Sign In . Alternatively the command to run is below. echo \" $( oc exec $( oc get pod -o template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}' | grep nexus ) -- cat /nexus-data/admin.password ) \" Copying the password, login as Admin with the password from the previous command. You will now have a Wizard popup to walk you through some setup, we're going to do very minimal changes: you can change the password (e.g. password ). Typically in this development/throwaway environment, enabling anonymous access is the best way to do this, so select that option Enable anonymous access . Then click finish. Now that the repository is ready to be configured, we are going to create a mirror of the Red Hat Maven General Access Repository so that we can pull resources through the proxy instead of direct. To do this, click the Gear icon ( ) to open the settings screen. From here click Repositories to view the current repositories and also to be able to create another. Here you will see a list of default repositorues. By default, Nexus will include the connection to Maven_Central , but we'd also like to add the Red Hat Maven General Access Repository connection to pull the resources from this repository. To do so, click Create Repository to build a new repository. Scroll down and we're going to create a maven2 (proxy) . The reason we're doing a proxy is that we're going to allow our repository to reach out to the Red Hat Maven General Access Repository to retrieve artifacts it does not already have, but if it has previously pulled them, will be stored in the Nexus repository's cache. This way if there are new releases, you can pull as required, but are not taking the entire repository as required. On the form that opens, you will add the following information (the name red_hat_ga needs to be exact to match the lab expectations): Name : red_hat_ga What type of artifacts does this repository store : Release Remote Storage : https://maven.repository.redhat.com/ga Click Create repository at the bottom to deploy the proxy instance of the Red Hat Maven General Access Repository Your repositories should now be setup and look similar to the below.","title":"Maven Repository Setup"},{"location":"guided_exercises/03c_CICD/introduction/#first-run-of-the-pipeline-to-configure-persistent-volume-claims-for-the-workspaces","text":"With the first run of the pipeline, you may have to run the pipeline manually. To do so, login to the OpenShift console for your project. You can see the console URL by running oc whoami --show-console and that will return the console link, Command/Control click it to open from VS Code. Once logged in, make sure to go to our project that you created in the provision.sh script. Follow these steps to run the pipeline for the first time. Make sure you change your project to the one you created in provision.sh . From the cluster home page, you can click Pipelines and then click Pipelines From the Pipelines screen, confirm your namespace matches what you created in the first part of the lab and after that, click the kebab icon to open the menu to Start Pipeline . Once you click start pipeline, a form will come up to set some settings for the pipeline. These should only have to be the first time you run this pipeline. You will modify the ones below if you followed the previous section for the workspaces to align to the ConfigMap of settings.xml used by Maven to build and the Persistent Volumes used for workspace data (git clones and the Maven artifacts). maven-local-repo : click the dropdown menu and select PersistentVolumeClaim and in the Select a PVC dropdown that appears, select maven-repo-pvc maven-settings : click the drop down and select Config Map and select from the new dropdown custom-maven-settings shared-workspace : click the dropdown menu and select PersistentVolumeClaim and in the Select a PVC dropdown, select source-workspace-pvc With this complete you can click the Start button to begin the pipeline","title":"First run of the Pipeline to configure Persistent Volume Claims for the workspaces"},{"location":"guided_exercises/03c_CICD/introduction/#testing-github-and-pipeline-integration","text":"If you run this test, a new deployment should be triggered. The pipeline will deploy the decision service for the first time. In your terminal, access your project folder. Commit and push. You can use this empty commit sample if you need: git commit -m \"an empty commit to test the pipeline\" --allow-empty git push origin master In OpenShift, access: \" Pipelines -> ba-cicd-pipeline -> Pipeline Runs \" and check the progress of your application deployment.","title":"Testing GitHub and Pipeline integration"},{"location":"guided_exercises/03c_CICD/introduction/#using-the-web-application","text":"The web application allows you to interact with the deployed rules and decisions in a specific Decision Server (KieServer or Kogito runtime). To use the deployed web app to interact with the deployed decisions, first you need to set the KIE Server URL in the web app settings. The deployed decision service is now deployed and accessible. Get your deployed KIE Server route. You can use the command: echo \"http://\" $( oc get route business-application-service-route -n rhdm-kieserver-cicd | awk 'FNR > 1 {print $2}' ) \"/rest/server\" Open your web application. The URL was provided in the installation step. If you lost it, use the command oc get route decision-service-webclient --template = 'http://{{.spec.host}}' -n rhdm-kieserver-cicd In the web application, click on the settings icon on the top right corner. In the field Kie Server Base URL , insert KIE Server URL. You can use the \"Test Connection\" button to validate the communication between the two services, then Save. You should be able to test the available decisions and rules. With this, the whole demo is now set up and ready to use. NOTE: If you get interested in see how this webapp was developed the src code is available here","title":"Using the web application"},{"location":"guided_exercises/03c_CICD/introduction/#extra-information","text":"The provisioning script provision.sh will: Create a new namespace called rhdm-kieserver-cicd Install OpenShift Pipelines Create the pipeline resources Deploy a front-end application that you can use to interact with the decision service once you deploy it. At the moment there are 4 projects in this repository: decisions-showcase : Decision use cases using Business Rules (Drools) and Decision Logic (DMN) business-application-service : Spring Boot runtime based Kie Server exposing the API for Decisions provided with this Showcase demo cicd : Tekton Pipeline resources to implement a fully automated CI/CD pipeline for your Business Application Services monitoring : working in progress... To see a detailed instruction on each service and each deployment processes (with images), check: Provisioning and testing the CI/CD Pipeline Provisioning and testing the webclient application","title":"Extra information"},{"location":"guided_exercises/03c_CICD/introduction/#interested-in-kogito","text":"Check out the kogito-quarkus branch to see this same demo but using Kogito based Decision Services instead of KieServer.","title":"Interested in Kogito?"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/","text":"Getting started with IBAMOE This guide shows you the experience of using IBM Business Automation Open Edition 8.0 to author, deploy, and execute your business automation applications. With three steps, this guide will get you from installation to deployment and testing of a business application: We will install IBAMOE locally, and it will run on top of Red Hat JBoss EAP (a.k.a. WildFly). Once we have it up and running, we will import an existing application, so that we have an overview of some capabilities by exploring the tool and the project itself. Finally, we'll wrap up by deploying the project and testing it. Pre-requisites We expect you to have installed in your machine: Java JDK 11 ( if you don't have it yet, you can download OpenJDK built by Red Hat https://developers.redhat.com/openjdk-install ) GIT client (https://git-scm.com/) IBM Business Automation Open Edition 8.0 Installation Demo : NOTE : You should use this installer to quickly install EAP, PAM and pre-configure the environment and user access you'll need. $ git clone https://github.com/timwuthenow/ibamoe-setup.git You should now have successfully installed IBM Business Automation Open Edition 8.0. You have two key components deployed in your Red Hat EAP right now: Business Central and KIE Server . Business Central is the component that allows you to develop business assets like processes and decisions, to manage projects, build and package them. Finally, you can deploy it in KIE Server. KIE Server is a lightweight engine capable of executing business assets like processes, cases and decisions. It can be easily integrated with your services, for example via REST or JMS. Luckily, IBM Business Automation Open Edition 8.0 comes with a number of out-of-the-box template and example applications that can be used to quickly build and deploy a process microservice. Explore the Asset Let's start by accessing Business Central. In your browser, access Business Central by navigating to http://localhost:8080/business-central Log in with the credentials: User: bamAdmin Password: ibmpam1! \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use pamadmin : pamadm1n as the username password Click on \"Design, create and modify projects and pages\" Select \"MySpace\", and next, click on \"Import Project\": Insert the following repository URL, and click on Import. https://github.com/jbossdemocentral/rhpam7-order-management-demo-repo.git Select the Order-Management project and click on OK. Once the project has been imported, notice it has 27 assets. Click on the filter button \"All\" and select Process. Open the order-management process. This is the automated process that determines the approval or denial of an order request. As you see below, it is implemented with the BPMN2 standard. The final element of this process, is a sub-process \"Place Order in ERP\". This subprocess includes advanced bpmn2 modeling concepts like compensation and event based gateways. Have in mind that PAM supports the modeling of advanced flows using the bpmn2 specification, but don't worry if you don't fully get what is happening in this subprocess. Notice this process tasks are aggregated in three lanes: Manager, Purchase and Supplier. The approval decision will be made based on multiple authors, but, in this process we even have the support of automated decision. The automated decision is made on the node \"Auto Approve Decision\", that references a DMN Model that is also part of this business project. Close the process modeler. Now, filter the assets by Decision. You should see a Test Scenario and a DMN model. Open the order-approval. It is a simple decision model that can define the \"Approve\" decision based on the data input \"Order Information\" and on the \"Price Tolerance\" business rules. Now, close the decision asset. In your project page, click on the Deploy button. Business Central will trigger the build of this maven project, that will be packaged in a KJAR (the deployment unit which contains the assets) and will be deployed on the KIE server. Once the build and deployment has finished, you'll see a successful deployment message. Click on the \"View deployment details\" link. The page will show a running \u201cdefault-kieserver\u201d with the \u201corder-management_1.1-SNAPSHOT\u201d container deployed. Our business project is now available to be consumed by client applications! Let's have a look at how we can consume this business application. Experience The engine, KIE Server, is the service which exposes the business project and also the one we use when integrating with client applications. It comes with a Swagger UI that allows us to test the RESTful endpoints of the engine and consume rules deployed on it. Another way to consume our business project is to use Business Central UI to interact with the engine and test our business assets. For this hello world, let's use Business Central process and task management capabilities. In Business Central, let's open the Menu in the top bar and navigate to \"Process Definitions\" We can see three different process definitions. We'll start a new process instance based on the \"order-management\" process. Click on the actions kebab, and select \"Start\" The form that opened is also part of our business process and we can customize it if needed. For now, let's just fill in the data required to start our process instance, and click the \"Submit\" button. Item Name: Laptop Dell XPS 15 Urgency: Medium A new process instance will start in the engine. In order to visualize the current status, click on \"Diagram\". Notice we currently have a Human Task named \"Request Offer\" waiting for human intervention. Now, let's work on this task. In the Menu, access the \"Task Inbox\": In the list you should see a list of tasks you have permission to see and work on. Let's claim the Request Offer task to our user, and start working on it. Click on the kebab and select the \"Claim and Work\" option: You'll see the task data available for your analysis, as a knowledge worker - someone responsible for executing the task. Click on the blue \"Start\" button to start working on the task.Based on this offer, we'll define our reply. Inform the following data and click on the blue \"Complete\" button: Category : optional Target Price: 250 Suplier list: supplier 1 According to our process, a new task will be created for the suppliers. The supplier should provide an offer - so let's do it. Still on the task list, claim and work the task \"Prepare Offer\": Click \"Start\" blue button, inform any date, and the best offer as 1000 . Click on complete. At this point, the automatic approval was already taken, and our request was not automatically approved. You can confirm this by visualizing the process instance. On the kebab, select \"View Process\" You'll be redirected to the list of process instances. Select the process instance with id 1, and then, choose the \"Diagram\" option: At this point, you have learned how you manage processes and tasks using Business Central. You know how to start new process instances, how to interact with the process tasks and how to complete them. What about finishing this process by your own? Following the same idea, In Business Central, you can reprove the request, reject the order and reach the end of this process instance. Conclusion Congratulations, you successfully concluded a Hello World in IBM Business Automation Open Edition 8.0. In this guide, we installed Red Hat PAM, imported a project directly from GitHub, checked out the a process definition modeled and an automation decision. We wrapped up our tutorial by deploying and testing our services using Business Central UI. If you want to know more about the Order Management demo, we recommend you take a look at the project's instructions at the github repository located here .","title":"Introduction"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/#getting-started-with-ibamoe","text":"This guide shows you the experience of using IBM Business Automation Open Edition 8.0 to author, deploy, and execute your business automation applications. With three steps, this guide will get you from installation to deployment and testing of a business application: We will install IBAMOE locally, and it will run on top of Red Hat JBoss EAP (a.k.a. WildFly). Once we have it up and running, we will import an existing application, so that we have an overview of some capabilities by exploring the tool and the project itself. Finally, we'll wrap up by deploying the project and testing it.","title":"Getting started with IBAMOE"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/#pre-requisites","text":"We expect you to have installed in your machine: Java JDK 11 ( if you don't have it yet, you can download OpenJDK built by Red Hat https://developers.redhat.com/openjdk-install ) GIT client (https://git-scm.com/) IBM Business Automation Open Edition 8.0 Installation Demo : NOTE : You should use this installer to quickly install EAP, PAM and pre-configure the environment and user access you'll need. $ git clone https://github.com/timwuthenow/ibamoe-setup.git You should now have successfully installed IBM Business Automation Open Edition 8.0. You have two key components deployed in your Red Hat EAP right now: Business Central and KIE Server . Business Central is the component that allows you to develop business assets like processes and decisions, to manage projects, build and package them. Finally, you can deploy it in KIE Server. KIE Server is a lightweight engine capable of executing business assets like processes, cases and decisions. It can be easily integrated with your services, for example via REST or JMS. Luckily, IBM Business Automation Open Edition 8.0 comes with a number of out-of-the-box template and example applications that can be used to quickly build and deploy a process microservice.","title":"Pre-requisites"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/#explore-the-asset","text":"Let's start by accessing Business Central. In your browser, access Business Central by navigating to http://localhost:8080/business-central Log in with the credentials: User: bamAdmin Password: ibmpam1! \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use pamadmin : pamadm1n as the username password Click on \"Design, create and modify projects and pages\" Select \"MySpace\", and next, click on \"Import Project\": Insert the following repository URL, and click on Import. https://github.com/jbossdemocentral/rhpam7-order-management-demo-repo.git Select the Order-Management project and click on OK. Once the project has been imported, notice it has 27 assets. Click on the filter button \"All\" and select Process. Open the order-management process. This is the automated process that determines the approval or denial of an order request. As you see below, it is implemented with the BPMN2 standard. The final element of this process, is a sub-process \"Place Order in ERP\". This subprocess includes advanced bpmn2 modeling concepts like compensation and event based gateways. Have in mind that PAM supports the modeling of advanced flows using the bpmn2 specification, but don't worry if you don't fully get what is happening in this subprocess. Notice this process tasks are aggregated in three lanes: Manager, Purchase and Supplier. The approval decision will be made based on multiple authors, but, in this process we even have the support of automated decision. The automated decision is made on the node \"Auto Approve Decision\", that references a DMN Model that is also part of this business project. Close the process modeler. Now, filter the assets by Decision. You should see a Test Scenario and a DMN model. Open the order-approval. It is a simple decision model that can define the \"Approve\" decision based on the data input \"Order Information\" and on the \"Price Tolerance\" business rules. Now, close the decision asset. In your project page, click on the Deploy button. Business Central will trigger the build of this maven project, that will be packaged in a KJAR (the deployment unit which contains the assets) and will be deployed on the KIE server. Once the build and deployment has finished, you'll see a successful deployment message. Click on the \"View deployment details\" link. The page will show a running \u201cdefault-kieserver\u201d with the \u201corder-management_1.1-SNAPSHOT\u201d container deployed. Our business project is now available to be consumed by client applications! Let's have a look at how we can consume this business application.","title":"Explore the Asset"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/#experience","text":"The engine, KIE Server, is the service which exposes the business project and also the one we use when integrating with client applications. It comes with a Swagger UI that allows us to test the RESTful endpoints of the engine and consume rules deployed on it. Another way to consume our business project is to use Business Central UI to interact with the engine and test our business assets. For this hello world, let's use Business Central process and task management capabilities. In Business Central, let's open the Menu in the top bar and navigate to \"Process Definitions\" We can see three different process definitions. We'll start a new process instance based on the \"order-management\" process. Click on the actions kebab, and select \"Start\" The form that opened is also part of our business process and we can customize it if needed. For now, let's just fill in the data required to start our process instance, and click the \"Submit\" button. Item Name: Laptop Dell XPS 15 Urgency: Medium A new process instance will start in the engine. In order to visualize the current status, click on \"Diagram\". Notice we currently have a Human Task named \"Request Offer\" waiting for human intervention. Now, let's work on this task. In the Menu, access the \"Task Inbox\": In the list you should see a list of tasks you have permission to see and work on. Let's claim the Request Offer task to our user, and start working on it. Click on the kebab and select the \"Claim and Work\" option: You'll see the task data available for your analysis, as a knowledge worker - someone responsible for executing the task. Click on the blue \"Start\" button to start working on the task.Based on this offer, we'll define our reply. Inform the following data and click on the blue \"Complete\" button: Category : optional Target Price: 250 Suplier list: supplier 1 According to our process, a new task will be created for the suppliers. The supplier should provide an offer - so let's do it. Still on the task list, claim and work the task \"Prepare Offer\": Click \"Start\" blue button, inform any date, and the best offer as 1000 . Click on complete. At this point, the automatic approval was already taken, and our request was not automatically approved. You can confirm this by visualizing the process instance. On the kebab, select \"View Process\" You'll be redirected to the list of process instances. Select the process instance with id 1, and then, choose the \"Diagram\" option: At this point, you have learned how you manage processes and tasks using Business Central. You know how to start new process instances, how to interact with the process tasks and how to complete them. What about finishing this process by your own? Following the same idea, In Business Central, you can reprove the request, reject the order and reach the end of this process instance.","title":"Experience"},{"location":"guided_exercises/04_order_management/01_try-order-management-app/#conclusion","text":"Congratulations, you successfully concluded a Hello World in IBM Business Automation Open Edition 8.0. In this guide, we installed Red Hat PAM, imported a project directly from GitHub, checked out the a process definition modeled and an automation decision. We wrapped up our tutorial by deploying and testing our services using Business Central UI. If you want to know more about the Order Management demo, we recommend you take a look at the project's instructions at the github repository located here .","title":"Conclusion"},{"location":"guided_exercises/04_order_management/02_create-order-management-app/","text":"Overview of Order Management Process This is a Process Management lab in which will implement an Order Management process. The process will use BPMN2 constructs like Swimlanes , User Tasks , Gateways , combined with decision-based routing based on a DMN Model (Decision Model & Notation). It also introduces more dynamic concepts of the IBM Business Automation Open Edition 8.0 process engine, like dynamic assignments of tasks based on process instance data. Goals Create an Order Management project in IBM Business Automation Open Edition 8.0 Define and create the process' domain model using the platform\u2019s Data Modeller. Implement an order management process in the process designer Implement decision logic in a DMN model. Create forms with the platform\u2019s Form Modeller. Deploy the project to the platform\u2019s Execution Server. Execute the end-to-end process. Pre-reqs Successful completion of the Environment Setup Lab or An existing, accessible, DM/PAM 7.3+ environment. Problem Statement In this lab we will create an Order Management process that manages the process of ordering a new phone or laptop. Start the process by providing the order information. The supplier sends an offer stating the expected delivery date and its best offer. Depending on the urgency of the urgency and the price, the order can be auto-approved by a DMN decision. If the order is not auto-approved, the manager needs to complete an approval step.","title":"Order Management Background"},{"location":"guided_exercises/04_order_management/02_create-order-management-app/#overview-of-order-management-process","text":"This is a Process Management lab in which will implement an Order Management process. The process will use BPMN2 constructs like Swimlanes , User Tasks , Gateways , combined with decision-based routing based on a DMN Model (Decision Model & Notation). It also introduces more dynamic concepts of the IBM Business Automation Open Edition 8.0 process engine, like dynamic assignments of tasks based on process instance data.","title":"Overview of Order Management Process"},{"location":"guided_exercises/04_order_management/02_create-order-management-app/#goals","text":"Create an Order Management project in IBM Business Automation Open Edition 8.0 Define and create the process' domain model using the platform\u2019s Data Modeller. Implement an order management process in the process designer Implement decision logic in a DMN model. Create forms with the platform\u2019s Form Modeller. Deploy the project to the platform\u2019s Execution Server. Execute the end-to-end process.","title":"Goals"},{"location":"guided_exercises/04_order_management/02_create-order-management-app/#pre-reqs","text":"Successful completion of the Environment Setup Lab or An existing, accessible, DM/PAM 7.3+ environment.","title":"Pre-reqs"},{"location":"guided_exercises/04_order_management/02_create-order-management-app/#problem-statement","text":"In this lab we will create an Order Management process that manages the process of ordering a new phone or laptop. Start the process by providing the order information. The supplier sends an offer stating the expected delivery date and its best offer. Depending on the urgency of the urgency and the price, the order can be auto-approved by a DMN decision. If the order is not auto-approved, the manager needs to complete an approval step.","title":"Problem Statement"},{"location":"guided_exercises/04_order_management/lab-walkthrough/","text":"Create a Project To define and deploy a business process, we first need to create a new project in which we can store the BPMN2 model, our domain model and the forms required for user interaction. To create a new project: Navigate to Business Central Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name order-management , and the description \"Order Management\". With the project created, we can now start building our solution. Lab Walk through In this section we will first create the Domain Model within Business Central and then walk through the creation of the assets associated with the Process. The Domain Model The business process will collect and carry data through the execution of the process. This data is stored in a data model or domain model. In this lab, we collect two types of data: OrderInfo : contains information about the order, like the item and the price. SupplierInfo : contains information about the supplier, like the name and the expected delivery date. In your project, click on the Add Asset button in the middle of the screen. In the drop-down menu in the upper-left corner, select Model . Click on the Data Object tile. Give the Data Object the name OrderInfo . Leave the package set to default. Add the following fields to the OrderInfo data object: Identifier Label Type item item name String urgency urgency String targetPrice target price double managerApproval approved Boolean When you\u2019ve added the fields, save the data object by clicking on the Save button in the top menu. Use the _breadcrumb` navigator at the top-left of the screen to navigate back to our order-management project. Click on the blue Add Asset button in the top-right corner and create a new Data Object Give it the name SupplierInfo Give the SupplierInfo object the following fields: Identifier Label Type offer best offer double deliveryDate delivery date Date user user String We\u2019re done creating our data model. We can now start with our process design. Process Design With the domain model defined, we can now sketch out the main flow of the process, the actors, the user task nodes and the required automation decisions. Create a new Business Process asset. Name it OrderManagement . You can do this by clicking Add an Asset and then selecting Business Process and then setting the name as OrderManagement . When the process designer opens, scroll down in the property panel on the right side of the screen, until you see the section Process Data . Expand the Process Data section and add the following 3 Process Variables by clicking on the + sign. Name Data Type orderInfo OrderInfo supplierInfo SupplierInfo approved Boolean Prepare Offer In the palette on the left-side of the editor, select the Lane component: Create the following 3 swimlanes: Supplier , Purchase , Manager Create the Start Event node in the Purchase swimlane. Create the Prepare Offer User Task node in the Supplier swimlane and connect it to the Start Event node. Set the following properties on the node via the properties panel on the right side of the screen: Task Name: PrepareOffer Subject: Prepare Offer for #{orderInfo.item} Actors: #{supplierInfo.user} Input: Data Inputs and Assignments Name Data Type Source orderInfo OrderInfo orderInfo supplierInfo SupplierInfo supplierInfo Output Data Outputs and Assignments Name Data Type Source supplierInfo SupplierInfo supplierInfo Create the Auto Approve Order Business Rule node in the Purchase swimlane and connect it to the Prepare Offer node. Set the following properties: Rule language: DMN Assigments: Data Inputs and Assignments Name Data Type Source Order Information OrderInfo orderInfo Supplier Information SupplierInfo supplierInfo Data Outputs and Assignments Name Data Type Target Approve Boolean approved \ud83d\udcd8 INFO: After we've created our DMN Decision Model, we will revisit the configuration of this node to reference this DMN model via its name and namespace properties. Exclusive Gateway Create an X-OR Gateway / Exclusive Gateway in the Manager swimlane, below the Auto Approve Order node and connect it to that node. Create the Approve User Task in the Manager swimlane and connect it to the X-OR gateway. Set the following properties: Task Name: Approve Subject: Approve Order of #{orderInfo.item} group: rest-all Assignments: Data Inputs and Assignments Name Data Type Source orderInfo OrderInfo orderInfo supplierInfo SupplierInfo supplierInfo Data Outputs and Assignments Name Data Type Target orderInfo OrderInfo orderInfo Create an X-OR Gateway / Exclusive Gateway in the Manager swimlane, after the Approve node and connect it to that node. Create another X-OR Gateway / Exclusive Gateway under the Manager swimlane (so outside of the swimlane) and connect it to the two other X-OR Gateways / Exclusive Gateways as shown in image below: Create the Place Order in ERP Script Task under the Manager swimlane (so outside of the swimlanes) and connect it to the X-OR Gateway we created earlier. Set the following script in the node\u2019s properties properties: System . out . println ( \"Place Order in ERP\" ); Create an End Event node under the Manager swimlane (so outside of the swimlanes) and connect it to the Place Order in ERP node. Name it Approved . Create an End Event node in the Purchase swimlane and connect it to the X-OR Gateway . Name it Rejected . On the Sequence Flow from the X/OR Gateway before the Approve node that is connnected ot the other X/OR Gateway , set the following condition, which tells the process engine that this path should be taken when the order is not automatically approved: Process Variable: approved Condition: Is true On the Gateway before the Approve node , set the Default Route property to Approve . On the Sequence Flow from the X/OR Gateway after the Approve task, which is connected to the X/OR Gateway before the Place Order in ERP task, set the following condition: Process Variable: orderInfo.managerApproval Condition: Is true On the X/OR Gateway after the Approval node , set the Default Route to Rejected . Save the process definition. With the overall layout of the process definition complete, the routing logic implemented, and the I/O assignments defined, we can now implement the business rules of our automated approval decision. Business Rules and Decisions Our Order Management process contains a Business Rule Task , but we have not yet defined the Decision Model that will be used in the task. In this paragraph we will implement the automatic approval rules in the form of a DMN model. Creating the DMN Inputs and BKM In the main project page, the so called library view , click on the Add Asset button. In the next screen, set the drop-down filter to Decision . Select the DMN asset. Give it the name order-approval . In the DMN editor, open the property-panel on the right-side of the screen and set the Namespace property to: http://www.redhat.com/dmn/demo/order-management-dmn . First we need to import our data-model, so we can use it in our DMN decisions. In the DMN editor, click on the Data Types tab and click on the Import Data Object button at the right-hand side of the screen: Select both the OrderInfo and SupplierInfo objects and click on the Import button: \u200b With the 2 datatypes imported, we need to create a third type that will hold the possible values for the urgency field of our Order Information . Click on the blue Add button in the top-right corner. In the entry that opens, give the data type the Name Urgency and the Type string : Click on the Add Constraints button, select Enumeration as the constraint type , and set the values low and high`. Click on the blue checkmark button to save the type. Navigate back to the model via the Model tab. Add 2 Input nodes to the model and name them Order Information and Supplier Information Select the Order Information node. Open the properties panel on the right-hand side of the screen, and set the Data type to OrderInfo . Do the same for the Supplier Information node. Set the Data type to SupplierInfo . Create a new Business Knowledge Model node, name it Price Tolerance . Click on the node, and click on the Edit button to start editting the node: Click in the Edit parameters . An editor will open. Click on Add parameter . Name the parameter order information and set the type to OrderInfo . Right click in the empty white cell under the parameter definitions and select Clear . The text Select expression will appear in the cell. Click on the cell and select Decision Table . Add an input clause to the decision table. The name of the input clause is order information.urgency , which references the urgency attribute of the order information parameter. Set the type to Urgency , which references the Urgency enumeration we created earlier. Set the output clause data type to number . Leave the name empty. Click on the Price Tolerance cell (top cell of the table), and set the data type to number . Implement the rest of the decision table as shown below. And save the DMN model. Writing the DMN Decision In this section we will complete the writing of the DMN decision. Navigate back to the model by clicking on the Back to order-approval link at the top-left of the editor. Create a new Decision Node and name it Approve . Connect the 2 input nodes and out Price Tolerance busines knowledge model node to the new decision node. Select the Approve decision node and click on the edit button. Click on _Select Expression, and set the logic type to Literal Expression . Enter the following expression: Supplier Information.offer < Price Tolerance(Order Information) * Order Information.targetPrice Click on the Approve cell (top cell of the table), and set the data type to boolean . Connecting the Decision to the Process Navigate back to the model by clicking on the Back to order-approval link at the top-left of the editor. Our DMN model is now complete. Make sure to save your model. With our DMN model implemented, we can now revisit our Business Rules Task in our BPMN2 model. Open the order-management process definition and click on the Auto Approval Order node. Open the node\u2019s properties in the property-panel on the right side of the editor, open the Implementation/Execution section and set: Namespace: http://www.redhat.com/dmn/lab/order-approval-dmn Name: order-approval In the same properties panel, expand the Data Assignments section and open the Assignments editor Implement the following data input and output assignments. Our BPMN model is now complete. Make sure to save the model. Now, we should be able to create and implement our forms. Creating Forms In this section we are going to create the process start and user-task forms. We could simply generate these forms with the click of a button, which gives us some standard forms based on the process and task data. In this lab however, we will be creating these forms using the Form Modeler tool. This allows us to design these forms to our specific needs. Process Start Form Let\u2019s start with the process start form. We want to create the following form: In the project\u2019s library view, click on Add Asset . Filter on Form , click on the Form tile. Enter the details as shown in the screenshot below: On this form we want to specify the initial order. We therefore require fields from the orderInfo and supplierInfo process variable. When we expand the Model Fields section, we can see our 2 process variables ( orderInfo and supplierInfo ). These are both complex objects. To work with complex objects (as opposed to simple types like integers and booleans), we require a data-form for that specific object. We therefore first need to create a data-form for our OrderInfo and SupplierInfo objects. Go back to the project\u2019s library view, click again on Add Asset and create a new form. Use the following details: Using the Form Modeler constructs, create the following form: To create this form, drag both the item , urgency and targetPrice onto the canvas and configure them as follows. List Box: Radio Group: Decimal Box: Save the form and create another new form for our supplierInfo . Use the following details. . Using the Form Modeler constructs, create the following form: To create this form, drag the user field onto the canvas and configure it as follows. Save the form and open the OrderManagement form (the first form we created). Drag the orderInfo process variable onto the canvas. In the pop-up form, set the OrderManagement-Order form we just created as the Nested Form : Drag the supplierInfo process variable ontoo the canvas. In the pop-up form, set the OrderManagement-SupplierInfo form we just created as the Nested Form : Prepare Offer Form Next, we will create the form for the Prepare Offer User Task . Create a new form. Provide the following details: Our aim is to create a form that looks as such: As with the process start form, this user-task form operates on 2 variables, orderInfo and supplierInfo . And, as with the process start form, we need to create a data-object form for each of these variables. Technically, data-object forms for a certain data-object can be reused in multiple task-forms. However, creating a data-object form per task-form allows us to design these data-object forms aimed for that specific task. PrepareOffer-OrderInfo PrepareOffer-SupplierInfo Finally, we need to create the task form for the Approve task. Create a new form. Provide the following details. . Our aim is create a form that looks like this: As with the other forms, this user-task form operates on 2 variables, orderInfo , supplierInfo . And, as with the other forms, we need to create a data-object form for each of these variables. Approve-SupplierInfo Approve-OrderInfo Don\u2019t forget to save all your forms!!! The implementation of our process is complete. It\u2019s now time to deploy and test our application. Deploying the Process Service With our Order Management project\u2019s process, decisions and forms completed, we can now package our project in a Deployment Unit (KJAR) and deploy it on the Execution Server. To do this: Go back to our project\u2019s Library View (for example by clicking on the Order Management link in the breadcrumb navigation in the upper-left of the screen). Click on the Deploy button in the upper-right corner of the screen. This will package our project in a Deployment Unit (KJAR) and deploy it onto the Execution Server (KIE-Server). Go to the Execution Servers perspective by clicking on \"Menu \u2192 Deploy \u2192 Execution Servers\". You will see the Deployment Unit deployed on the Execution Server. Execute the process In this section, you will execute the process deployed on the Process Execution Server via the Business Central workbench. Navigate to Menu \u2192 Manage \u2192 Process Definitions . If everything is correct, the order-management process will be listed. Click on the kebab icon of the order-management process and click on Start . In the form that opens, pick the Huawei P10 Phone as the item and set the urgency to low . Set the target price to 700 and set the supplier name to the name of your own Business Central user (e.g. bamAmdmin ). Click on Submit . In the process instance details screen that opens, click on the Diagram tab to open the process instance diagram, which shows the current state of the process. The process is in a wait state at the Prepare Offer task. Navigateto Menu \u2192 Track Task Inbox**. Click on the Prepare Offer task to open its task window. Click on the Start button to start working on the task. Because the task has been assigned to a single user (via #{supplierInfo.user}), you don\u2019t have to first claim the task. Select a random delivery date. Set the best offer to 900 . Click on Complete . The process will continue to the Auto Approve Order decision node. Because of the target prices set, and the offered price, the decision will evaluale to false . Hence, the process will continue to the Approve task. Go back to the Task Inbox and open the Approve task. Click on Claim and on Start . In this form we can approve or disapprove the order via the approved checkbox, and specify a rejection reason if we reject the order. Approve the task by checking the approved checkbox and clicking on Complete : Go back to the process instance view and observe that the process instance is gone. Enable the Completed checkbox in the State filter on the left-hand-side of the screen. Observe that we can see our process instance in the list. Open the process instance, open it\u2019s Diagram tab. Observe that the order has been accepted: Run a couple more process instances with different values to test, for example, the functionality of the Automated Approval Rules . Correcting problems and errors During process instance execution, a lot of things can go wrong. Users might fill in incorrect data, remote services are not available, etc. In an ideal world, the process definition takes a lot of these possible problems into account in its design. E.g. the process definition might contain exception handling logic via boundary catching error events and retry-loops. However, there are situations in which an operator or administrator would like to manually change the process to another statem for example, restart an already completed User Task . In the latest version of IBM Business Automation Open Edition 8.0 this is now possible via the Process Instance interface in Business Central. Start a new process instance of our Order Management process. Complete the Prepare Offer task in such a way that the order is not automatically approved and the process will hit the Approve User Task wait state. Go to the Process Instances view and select the process instance. Navigate to the Diagram tab. Observe that the process is waiting in the Approve User Task . Click on the Prepare Offer node to select it. In the Node Actions panel on the left-hand-side of the screen, verify that the Prepare Offer node is selected and click on Trigger . Observe that the Prepare Offer User Task has been activated. Although we have re-activated the Prepare Offer node, we have not yet de-activated the Approve task. Click on the active Approve task and expand the Node Instances section in the Node Actions panel. Click on the kebab icon of the active Approve instance and click on Cancel : 6. Open the Task Inbox . Observe that the Approve User Task is gone and that we have a new Prepare Offer task. Open the Prepare Offer task, set the price to a price which will trigger the rules to automatically approve the order, and complete the task. Go to the process instances view and observe that the process instance has been completed. Enable the Completed filter in the State filter panel on the left-hand-side of the screen. Open the completed process instance and open its Diagram tab. Execute the process via APIs The Execution Server provides a rich RESTful API that allows user to interact with the process engine and deployed processes via a REST. This powerful feature allows users to create modern user interface and applications in their technology of choice (e.g. Entando DXP, ReactJS/Redux, AngularJS, etc.) and integrate these applications with the process engine to create modern, process driven, enterprise applications. The Swagger interface provides the description and documentation of the Execution Server\u2019s RESTful API. At the same time, it allows the APIs to be called from the UI. This enables developers and users to quickly test a, in this case, a deployed business process. Navigate to the KIE Server Swagger Page Locate the Process instances section. The Process Instances API provides a vast array of operations to interact with the process engine. Locate the POST operation for the resource /server/containers/{containerId}/processes/{processId}/instances . This is the RESTful operation with which we can start a new process instance. Expand the operation: Click on the Try it out button. Set the containerId to order-management (in this case we use the alias of the container). Set the processId to order-management.OrderManagement . Set Parameter content type to application/json . Set the Response content type to application/json . Set the body to: { \"orderInfo\" : { \"com.myspace.order\\_management.OrderInfo\" : { \"item\" : \"Huawei P10\" , \"urgency\" : \"low\" , \"price\" : 0.0 , \"targetPrice\" : \"700.0\" } }, \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"bamAdmin\" } } } Click on the Execute button. If requested, provide the username and password of your Business Central and KIE-Server user (in this example we have been using u: bamAdmin , p: ibmpam1! ). \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the pamadmin:pamadm1n information Inspect the response. Note that the operation returns the process instance id of the started process. Go back to the Business Central workbench. Go the process instances view and inspect the process instance we have just started. The RESTful API provides many more operations. Let\u2019s use the API to fetch our Task List and complete the Request Offer task. In the Swagger API, navigate to the Process queries section. Find the GET operation for the resource /server/queries/tasks/instances/pot-owners . Expand the operation and click on the Try it out button. Make sure the *Response content type is set to application/json . Leave all the other fields set to their default values. Click on the Execute button. This will return all the tasks for our user (in the case of this example this is the bamAdmin user). We can see the Prepare Offer task that is available in our inbox. Let\u2019s complete this task. Go to the Task Instances section in the Swagger interface and locate the PUT operation of the /server/containers/{containerId}/tasks/{taskInstanceId}/states/completed resource. This is the operation with which we can complete a task. Set the containerId to order-management . Set the taskInstanceId to the id of the task instance you want to complete. The task instance id an be found in the list of task instances we got back from our previous REST operation. Set auto-progress to true . This controls the auto progression of the taks through the various states of the task lifecycle (i.e. claimed, started, etc.) Set the Parameter content type to application/json . Set the Response content type to application/json . Set the body to: { \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"bamAdmin\" , \"offer\" : \"900\" , \"deliveryDate\" : \"2020-03-11T12:00:00.000Z\" } } } \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. { \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"pamadmin\" , \"offer\" : \"900\" , \"deliveryDate\" : \"2020-03-11T12:00:00.000Z\" } } } Click on the Execute button. If you\u2019ve entered everything correctly, the task will be completed and the process will move to the next wait state, the Prepare Offer task. . Go back to the Business Central workbench. Go to the process instances view. Select the process instance of the task you\u2019ve just completed. Observe that the Prepare Offer task has been completed and that the process is now waiting on the Approve User Task . The rest of the tasks can be completed in the same way via the API. Using the KIE-Server Client IBM Business Automation Open Edition 8.0 provides a KIE-Server Client API that allows the user to interact with the KIE-Server from a Java client using a higher level API. It abstracts the data marshalling and unmarshalling and the creation and execution of the RESTful commands from the developer, allowing him/her to focus on developing business logic. In this section we will create a simple Java client for our Order Management process. Create a new Maven Java JAR project in your favourite IDE (e.g. IntelliJ, Eclipse, Visual Studio Code). Add the following dependency to your project: <dependency> <groupid> org.kie.server </groupid> <artifactId> kie-server-client </artifactId> <version> 7.67.0.Final-redhat-00008 </version> <scope> compile </scope> </dependency> Create a Java package in your src/main/java folder with the name com.myspace.order_management . Download the OrderInfo.java file from this location and add it to the package you\u2019ve just created. Download the SupplierInfo.java file from this location and add it to the package. Create a new Java class called Main . Add a public static void main(String[] args) method to your main class. Before we implement our method, we first define a number of constants that we will need when implementing our method (note that the values of your constants can be different depending on your environment, model namespace, etc.): private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"order-management\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; private static final String PROCESS_ID = \"order-management.OrderManagement\" ; \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"order-management\" ; private static final String USERNAME = \"pamadmin\" ; private static final String PASSWORD = \"pamadm1n\" ; private static final String PROCESS_ID = \"order-management.OrderManagement\" ; KIE-Server client API classes can mostly be retrieved from the KieServicesFactory class. We first need to create a KieServicesConfiguration instance that will hold our credentials and defines how we want our client to communicate with the server: KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , new EnteredCredentialsProvider ( USERNAME , PASSWORD )); To allow the KIE-Server Client\u2019s marshaller to marshall and unmarshall instances of our domain model, we need to add our domain model classes to the KieServicesConfiguration . Set < Class <?>> extraClasses = new HashSet <> (); extraClasses . add ( OrderInfo . class ); extraClasses . add ( SupplierInfo . class ); kieServicesConfig . addExtraClasses ( extraClasses ); Next, we create the KieServicesClient : ~~~java KieServicesClient kieServicesClient = KieServicesFactory.newKieServicesClient(kieServicesConfig); ~~ From this client we retrieve our ProcessServicesClient : ProcessServicesClient processServicesClient = kieServicesClient . getServicesClient ( ProcessServicesClient . class ); We now create a Map which we will use to pass the process input variables. We create a new OrderInfo instance and SupplierInfo instance and put them in the Map . Map < String , Object > inputData = new HashMap <> (); OrderInfo orderInfo = new OrderInfo (); orderInfo . setItem ( \"Huawei P10\" ); orderInfo . setUrgency ( \"low\" ); inputData . put ( \"orderInfo\" , orderInfo ); SupplierInfo supplierInfo = new SupplierInfo (); supplierInfo . setUser ( \"bamAdmin\" ); inputData . put ( \"supplierInfo\" , supplierInfo ); \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use pamadmin : pamadm1n as the username password Map < String , Object > inputData = new HashMap <> (); OrderInfo orderInfo = new OrderInfo (); orderInfo . setItem ( \"Huawei P10\" ); orderInfo . setUrgency ( \"low\" ); inputData . put ( \"orderInfo\" , orderInfo ); SupplierInfo supplierInfo = new SupplierInfo (); supplierInfo . setUser ( \"pamadmin\" ); inputData . put ( \"supplierInfo\" , supplierInfo ); We can now start a new process instance via the ProcessServicesClient . Long processInstanceId = processServicesClient . startProcess ( CONTAINER_ID , PROCESS_ID , inputData ); Finally, we can print the process instance id to System.out . System . out . println ( \"New *Order Management* process instance started with instance-id: \" + processInstanceId ); Compile your project and run it. Observe the output in the console, which should say: New Order Management process instance started with instance-id The complete project can be found here: https://github.com/timwuthenow/rhpam7-order-management-demo-repo The KIE-Server Client provides more services to interact with the Execution Server: UserTaskServicesClient : provides functionality to interact with the UserTask services, for example to claim, start and complete a User Task . CaseServicesClient : provides functionality to interact with the Case Management features of the Execution Server. ProcessAdminServicesClient : provides the administration API for processes. etc. We leave as an exercise to the reader to try to complete a User Task , of the process instance we\u2019ve just created, using the UserTaskServicesClient .","title":"Lab Walk Through"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#create-a-project","text":"To define and deploy a business process, we first need to create a new project in which we can store the BPMN2 model, our domain model and the forms required for user interaction. To create a new project: Navigate to Business Central Login to the platform with the provided username and password. Click on Design to navigate to the Design perspective. In the Design perspective, create a new project. If your space is empty, this can be done by clicking on the blue Add Project button in the center of the page. If you already have projects in your space, you can click on the blue Add Project icon at the top right of the page. Give the project the name order-management , and the description \"Order Management\". With the project created, we can now start building our solution.","title":"Create a Project"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#lab-walk-through","text":"In this section we will first create the Domain Model within Business Central and then walk through the creation of the assets associated with the Process.","title":"Lab Walk through"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#the-domain-model","text":"The business process will collect and carry data through the execution of the process. This data is stored in a data model or domain model. In this lab, we collect two types of data: OrderInfo : contains information about the order, like the item and the price. SupplierInfo : contains information about the supplier, like the name and the expected delivery date. In your project, click on the Add Asset button in the middle of the screen. In the drop-down menu in the upper-left corner, select Model . Click on the Data Object tile. Give the Data Object the name OrderInfo . Leave the package set to default. Add the following fields to the OrderInfo data object: Identifier Label Type item item name String urgency urgency String targetPrice target price double managerApproval approved Boolean When you\u2019ve added the fields, save the data object by clicking on the Save button in the top menu. Use the _breadcrumb` navigator at the top-left of the screen to navigate back to our order-management project. Click on the blue Add Asset button in the top-right corner and create a new Data Object Give it the name SupplierInfo Give the SupplierInfo object the following fields: Identifier Label Type offer best offer double deliveryDate delivery date Date user user String We\u2019re done creating our data model. We can now start with our process design.","title":"The Domain Model"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#process-design","text":"With the domain model defined, we can now sketch out the main flow of the process, the actors, the user task nodes and the required automation decisions. Create a new Business Process asset. Name it OrderManagement . You can do this by clicking Add an Asset and then selecting Business Process and then setting the name as OrderManagement . When the process designer opens, scroll down in the property panel on the right side of the screen, until you see the section Process Data . Expand the Process Data section and add the following 3 Process Variables by clicking on the + sign. Name Data Type orderInfo OrderInfo supplierInfo SupplierInfo approved Boolean","title":"Process Design"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#prepare-offer","text":"In the palette on the left-side of the editor, select the Lane component: Create the following 3 swimlanes: Supplier , Purchase , Manager Create the Start Event node in the Purchase swimlane. Create the Prepare Offer User Task node in the Supplier swimlane and connect it to the Start Event node. Set the following properties on the node via the properties panel on the right side of the screen: Task Name: PrepareOffer Subject: Prepare Offer for #{orderInfo.item} Actors: #{supplierInfo.user} Input: Data Inputs and Assignments Name Data Type Source orderInfo OrderInfo orderInfo supplierInfo SupplierInfo supplierInfo Output Data Outputs and Assignments Name Data Type Source supplierInfo SupplierInfo supplierInfo Create the Auto Approve Order Business Rule node in the Purchase swimlane and connect it to the Prepare Offer node. Set the following properties: Rule language: DMN Assigments: Data Inputs and Assignments Name Data Type Source Order Information OrderInfo orderInfo Supplier Information SupplierInfo supplierInfo Data Outputs and Assignments Name Data Type Target Approve Boolean approved \ud83d\udcd8 INFO: After we've created our DMN Decision Model, we will revisit the configuration of this node to reference this DMN model via its name and namespace properties.","title":"Prepare Offer"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#exclusive-gateway","text":"Create an X-OR Gateway / Exclusive Gateway in the Manager swimlane, below the Auto Approve Order node and connect it to that node. Create the Approve User Task in the Manager swimlane and connect it to the X-OR gateway. Set the following properties: Task Name: Approve Subject: Approve Order of #{orderInfo.item} group: rest-all Assignments: Data Inputs and Assignments Name Data Type Source orderInfo OrderInfo orderInfo supplierInfo SupplierInfo supplierInfo Data Outputs and Assignments Name Data Type Target orderInfo OrderInfo orderInfo Create an X-OR Gateway / Exclusive Gateway in the Manager swimlane, after the Approve node and connect it to that node. Create another X-OR Gateway / Exclusive Gateway under the Manager swimlane (so outside of the swimlane) and connect it to the two other X-OR Gateways / Exclusive Gateways as shown in image below: Create the Place Order in ERP Script Task under the Manager swimlane (so outside of the swimlanes) and connect it to the X-OR Gateway we created earlier. Set the following script in the node\u2019s properties properties: System . out . println ( \"Place Order in ERP\" ); Create an End Event node under the Manager swimlane (so outside of the swimlanes) and connect it to the Place Order in ERP node. Name it Approved . Create an End Event node in the Purchase swimlane and connect it to the X-OR Gateway . Name it Rejected . On the Sequence Flow from the X/OR Gateway before the Approve node that is connnected ot the other X/OR Gateway , set the following condition, which tells the process engine that this path should be taken when the order is not automatically approved: Process Variable: approved Condition: Is true On the Gateway before the Approve node , set the Default Route property to Approve . On the Sequence Flow from the X/OR Gateway after the Approve task, which is connected to the X/OR Gateway before the Place Order in ERP task, set the following condition: Process Variable: orderInfo.managerApproval Condition: Is true On the X/OR Gateway after the Approval node , set the Default Route to Rejected . Save the process definition. With the overall layout of the process definition complete, the routing logic implemented, and the I/O assignments defined, we can now implement the business rules of our automated approval decision.","title":"Exclusive Gateway"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#business-rules-and-decisions","text":"Our Order Management process contains a Business Rule Task , but we have not yet defined the Decision Model that will be used in the task. In this paragraph we will implement the automatic approval rules in the form of a DMN model.","title":"Business Rules and Decisions"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#creating-the-dmn-inputs-and-bkm","text":"In the main project page, the so called library view , click on the Add Asset button. In the next screen, set the drop-down filter to Decision . Select the DMN asset. Give it the name order-approval . In the DMN editor, open the property-panel on the right-side of the screen and set the Namespace property to: http://www.redhat.com/dmn/demo/order-management-dmn . First we need to import our data-model, so we can use it in our DMN decisions. In the DMN editor, click on the Data Types tab and click on the Import Data Object button at the right-hand side of the screen: Select both the OrderInfo and SupplierInfo objects and click on the Import button: \u200b With the 2 datatypes imported, we need to create a third type that will hold the possible values for the urgency field of our Order Information . Click on the blue Add button in the top-right corner. In the entry that opens, give the data type the Name Urgency and the Type string : Click on the Add Constraints button, select Enumeration as the constraint type , and set the values low and high`. Click on the blue checkmark button to save the type. Navigate back to the model via the Model tab. Add 2 Input nodes to the model and name them Order Information and Supplier Information Select the Order Information node. Open the properties panel on the right-hand side of the screen, and set the Data type to OrderInfo . Do the same for the Supplier Information node. Set the Data type to SupplierInfo . Create a new Business Knowledge Model node, name it Price Tolerance . Click on the node, and click on the Edit button to start editting the node: Click in the Edit parameters . An editor will open. Click on Add parameter . Name the parameter order information and set the type to OrderInfo . Right click in the empty white cell under the parameter definitions and select Clear . The text Select expression will appear in the cell. Click on the cell and select Decision Table . Add an input clause to the decision table. The name of the input clause is order information.urgency , which references the urgency attribute of the order information parameter. Set the type to Urgency , which references the Urgency enumeration we created earlier. Set the output clause data type to number . Leave the name empty. Click on the Price Tolerance cell (top cell of the table), and set the data type to number . Implement the rest of the decision table as shown below. And save the DMN model.","title":"Creating the DMN Inputs and BKM"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#writing-the-dmn-decision","text":"In this section we will complete the writing of the DMN decision. Navigate back to the model by clicking on the Back to order-approval link at the top-left of the editor. Create a new Decision Node and name it Approve . Connect the 2 input nodes and out Price Tolerance busines knowledge model node to the new decision node. Select the Approve decision node and click on the edit button. Click on _Select Expression, and set the logic type to Literal Expression . Enter the following expression: Supplier Information.offer < Price Tolerance(Order Information) * Order Information.targetPrice Click on the Approve cell (top cell of the table), and set the data type to boolean .","title":"Writing the DMN Decision"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#connecting-the-decision-to-the-process","text":"Navigate back to the model by clicking on the Back to order-approval link at the top-left of the editor. Our DMN model is now complete. Make sure to save your model. With our DMN model implemented, we can now revisit our Business Rules Task in our BPMN2 model. Open the order-management process definition and click on the Auto Approval Order node. Open the node\u2019s properties in the property-panel on the right side of the editor, open the Implementation/Execution section and set: Namespace: http://www.redhat.com/dmn/lab/order-approval-dmn Name: order-approval In the same properties panel, expand the Data Assignments section and open the Assignments editor Implement the following data input and output assignments. Our BPMN model is now complete. Make sure to save the model. Now, we should be able to create and implement our forms.","title":"Connecting the Decision to the Process"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#creating-forms","text":"In this section we are going to create the process start and user-task forms. We could simply generate these forms with the click of a button, which gives us some standard forms based on the process and task data. In this lab however, we will be creating these forms using the Form Modeler tool. This allows us to design these forms to our specific needs.","title":"Creating Forms"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#process-start-form","text":"Let\u2019s start with the process start form. We want to create the following form: In the project\u2019s library view, click on Add Asset . Filter on Form , click on the Form tile. Enter the details as shown in the screenshot below: On this form we want to specify the initial order. We therefore require fields from the orderInfo and supplierInfo process variable. When we expand the Model Fields section, we can see our 2 process variables ( orderInfo and supplierInfo ). These are both complex objects. To work with complex objects (as opposed to simple types like integers and booleans), we require a data-form for that specific object. We therefore first need to create a data-form for our OrderInfo and SupplierInfo objects. Go back to the project\u2019s library view, click again on Add Asset and create a new form. Use the following details: Using the Form Modeler constructs, create the following form: To create this form, drag both the item , urgency and targetPrice onto the canvas and configure them as follows. List Box: Radio Group: Decimal Box: Save the form and create another new form for our supplierInfo . Use the following details. . Using the Form Modeler constructs, create the following form: To create this form, drag the user field onto the canvas and configure it as follows. Save the form and open the OrderManagement form (the first form we created). Drag the orderInfo process variable onto the canvas. In the pop-up form, set the OrderManagement-Order form we just created as the Nested Form : Drag the supplierInfo process variable ontoo the canvas. In the pop-up form, set the OrderManagement-SupplierInfo form we just created as the Nested Form :","title":"Process Start Form"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#prepare-offer-form","text":"Next, we will create the form for the Prepare Offer User Task . Create a new form. Provide the following details: Our aim is to create a form that looks as such: As with the process start form, this user-task form operates on 2 variables, orderInfo and supplierInfo . And, as with the process start form, we need to create a data-object form for each of these variables. Technically, data-object forms for a certain data-object can be reused in multiple task-forms. However, creating a data-object form per task-form allows us to design these data-object forms aimed for that specific task. PrepareOffer-OrderInfo PrepareOffer-SupplierInfo Finally, we need to create the task form for the Approve task. Create a new form. Provide the following details. . Our aim is create a form that looks like this: As with the other forms, this user-task form operates on 2 variables, orderInfo , supplierInfo . And, as with the other forms, we need to create a data-object form for each of these variables. Approve-SupplierInfo Approve-OrderInfo Don\u2019t forget to save all your forms!!! The implementation of our process is complete. It\u2019s now time to deploy and test our application.","title":"Prepare Offer Form"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#deploying-the-process-service","text":"With our Order Management project\u2019s process, decisions and forms completed, we can now package our project in a Deployment Unit (KJAR) and deploy it on the Execution Server. To do this: Go back to our project\u2019s Library View (for example by clicking on the Order Management link in the breadcrumb navigation in the upper-left of the screen). Click on the Deploy button in the upper-right corner of the screen. This will package our project in a Deployment Unit (KJAR) and deploy it onto the Execution Server (KIE-Server). Go to the Execution Servers perspective by clicking on \"Menu \u2192 Deploy \u2192 Execution Servers\". You will see the Deployment Unit deployed on the Execution Server.","title":"Deploying the Process Service"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#execute-the-process","text":"In this section, you will execute the process deployed on the Process Execution Server via the Business Central workbench. Navigate to Menu \u2192 Manage \u2192 Process Definitions . If everything is correct, the order-management process will be listed. Click on the kebab icon of the order-management process and click on Start . In the form that opens, pick the Huawei P10 Phone as the item and set the urgency to low . Set the target price to 700 and set the supplier name to the name of your own Business Central user (e.g. bamAmdmin ). Click on Submit . In the process instance details screen that opens, click on the Diagram tab to open the process instance diagram, which shows the current state of the process. The process is in a wait state at the Prepare Offer task. Navigateto Menu \u2192 Track Task Inbox**. Click on the Prepare Offer task to open its task window. Click on the Start button to start working on the task. Because the task has been assigned to a single user (via #{supplierInfo.user}), you don\u2019t have to first claim the task. Select a random delivery date. Set the best offer to 900 . Click on Complete . The process will continue to the Auto Approve Order decision node. Because of the target prices set, and the offered price, the decision will evaluale to false . Hence, the process will continue to the Approve task. Go back to the Task Inbox and open the Approve task. Click on Claim and on Start . In this form we can approve or disapprove the order via the approved checkbox, and specify a rejection reason if we reject the order. Approve the task by checking the approved checkbox and clicking on Complete : Go back to the process instance view and observe that the process instance is gone. Enable the Completed checkbox in the State filter on the left-hand-side of the screen. Observe that we can see our process instance in the list. Open the process instance, open it\u2019s Diagram tab. Observe that the order has been accepted: Run a couple more process instances with different values to test, for example, the functionality of the Automated Approval Rules .","title":"Execute the process"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#correcting-problems-and-errors","text":"During process instance execution, a lot of things can go wrong. Users might fill in incorrect data, remote services are not available, etc. In an ideal world, the process definition takes a lot of these possible problems into account in its design. E.g. the process definition might contain exception handling logic via boundary catching error events and retry-loops. However, there are situations in which an operator or administrator would like to manually change the process to another statem for example, restart an already completed User Task . In the latest version of IBM Business Automation Open Edition 8.0 this is now possible via the Process Instance interface in Business Central. Start a new process instance of our Order Management process. Complete the Prepare Offer task in such a way that the order is not automatically approved and the process will hit the Approve User Task wait state. Go to the Process Instances view and select the process instance. Navigate to the Diagram tab. Observe that the process is waiting in the Approve User Task . Click on the Prepare Offer node to select it. In the Node Actions panel on the left-hand-side of the screen, verify that the Prepare Offer node is selected and click on Trigger . Observe that the Prepare Offer User Task has been activated. Although we have re-activated the Prepare Offer node, we have not yet de-activated the Approve task. Click on the active Approve task and expand the Node Instances section in the Node Actions panel. Click on the kebab icon of the active Approve instance and click on Cancel : 6. Open the Task Inbox . Observe that the Approve User Task is gone and that we have a new Prepare Offer task. Open the Prepare Offer task, set the price to a price which will trigger the rules to automatically approve the order, and complete the task. Go to the process instances view and observe that the process instance has been completed. Enable the Completed filter in the State filter panel on the left-hand-side of the screen. Open the completed process instance and open its Diagram tab.","title":"Correcting problems and errors"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#execute-the-process-via-apis","text":"The Execution Server provides a rich RESTful API that allows user to interact with the process engine and deployed processes via a REST. This powerful feature allows users to create modern user interface and applications in their technology of choice (e.g. Entando DXP, ReactJS/Redux, AngularJS, etc.) and integrate these applications with the process engine to create modern, process driven, enterprise applications. The Swagger interface provides the description and documentation of the Execution Server\u2019s RESTful API. At the same time, it allows the APIs to be called from the UI. This enables developers and users to quickly test a, in this case, a deployed business process. Navigate to the KIE Server Swagger Page Locate the Process instances section. The Process Instances API provides a vast array of operations to interact with the process engine. Locate the POST operation for the resource /server/containers/{containerId}/processes/{processId}/instances . This is the RESTful operation with which we can start a new process instance. Expand the operation: Click on the Try it out button. Set the containerId to order-management (in this case we use the alias of the container). Set the processId to order-management.OrderManagement . Set Parameter content type to application/json . Set the Response content type to application/json . Set the body to: { \"orderInfo\" : { \"com.myspace.order\\_management.OrderInfo\" : { \"item\" : \"Huawei P10\" , \"urgency\" : \"low\" , \"price\" : 0.0 , \"targetPrice\" : \"700.0\" } }, \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"bamAdmin\" } } } Click on the Execute button. If requested, provide the username and password of your Business Central and KIE-Server user (in this example we have been using u: bamAdmin , p: ibmpam1! ). \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the pamadmin:pamadm1n information Inspect the response. Note that the operation returns the process instance id of the started process. Go back to the Business Central workbench. Go the process instances view and inspect the process instance we have just started. The RESTful API provides many more operations. Let\u2019s use the API to fetch our Task List and complete the Request Offer task. In the Swagger API, navigate to the Process queries section. Find the GET operation for the resource /server/queries/tasks/instances/pot-owners . Expand the operation and click on the Try it out button. Make sure the *Response content type is set to application/json . Leave all the other fields set to their default values. Click on the Execute button. This will return all the tasks for our user (in the case of this example this is the bamAdmin user). We can see the Prepare Offer task that is available in our inbox. Let\u2019s complete this task. Go to the Task Instances section in the Swagger interface and locate the PUT operation of the /server/containers/{containerId}/tasks/{taskInstanceId}/states/completed resource. This is the operation with which we can complete a task. Set the containerId to order-management . Set the taskInstanceId to the id of the task instance you want to complete. The task instance id an be found in the list of task instances we got back from our previous REST operation. Set auto-progress to true . This controls the auto progression of the taks through the various states of the task lifecycle (i.e. claimed, started, etc.) Set the Parameter content type to application/json . Set the Response content type to application/json . Set the body to: { \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"bamAdmin\" , \"offer\" : \"900\" , \"deliveryDate\" : \"2020-03-11T12:00:00.000Z\" } } } \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. { \"supplierInfo\" : { \"com.myspace.order\\_management.SupplierInfo\" : { \"user\" : \"pamadmin\" , \"offer\" : \"900\" , \"deliveryDate\" : \"2020-03-11T12:00:00.000Z\" } } } Click on the Execute button. If you\u2019ve entered everything correctly, the task will be completed and the process will move to the next wait state, the Prepare Offer task. . Go back to the Business Central workbench. Go to the process instances view. Select the process instance of the task you\u2019ve just completed. Observe that the Prepare Offer task has been completed and that the process is now waiting on the Approve User Task . The rest of the tasks can be completed in the same way via the API.","title":"Execute the process via APIs"},{"location":"guided_exercises/04_order_management/lab-walkthrough/#using-the-kie-server-client","text":"IBM Business Automation Open Edition 8.0 provides a KIE-Server Client API that allows the user to interact with the KIE-Server from a Java client using a higher level API. It abstracts the data marshalling and unmarshalling and the creation and execution of the RESTful commands from the developer, allowing him/her to focus on developing business logic. In this section we will create a simple Java client for our Order Management process. Create a new Maven Java JAR project in your favourite IDE (e.g. IntelliJ, Eclipse, Visual Studio Code). Add the following dependency to your project: <dependency> <groupid> org.kie.server </groupid> <artifactId> kie-server-client </artifactId> <version> 7.67.0.Final-redhat-00008 </version> <scope> compile </scope> </dependency> Create a Java package in your src/main/java folder with the name com.myspace.order_management . Download the OrderInfo.java file from this location and add it to the package you\u2019ve just created. Download the SupplierInfo.java file from this location and add it to the package. Create a new Java class called Main . Add a public static void main(String[] args) method to your main class. Before we implement our method, we first define a number of constants that we will need when implementing our method (note that the values of your constants can be different depending on your environment, model namespace, etc.): private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"order-management\" ; private static final String USERNAME = \"bamAdmin\" ; private static final String PASSWORD = \"ibmpam1!\" ; private static final String PROCESS_ID = \"order-management.OrderManagement\" ; \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use the following. private static final String KIE_SERVER_URL = \"http://localhost:8080/kie-server/services/rest/server\" ; private static final String CONTAINER_ID = \"order-management\" ; private static final String USERNAME = \"pamadmin\" ; private static final String PASSWORD = \"pamadm1n\" ; private static final String PROCESS_ID = \"order-management.OrderManagement\" ; KIE-Server client API classes can mostly be retrieved from the KieServicesFactory class. We first need to create a KieServicesConfiguration instance that will hold our credentials and defines how we want our client to communicate with the server: KieServicesConfiguration kieServicesConfig = KieServicesFactory . newRestConfiguration ( KIE_SERVER_URL , new EnteredCredentialsProvider ( USERNAME , PASSWORD )); To allow the KIE-Server Client\u2019s marshaller to marshall and unmarshall instances of our domain model, we need to add our domain model classes to the KieServicesConfiguration . Set < Class <?>> extraClasses = new HashSet <> (); extraClasses . add ( OrderInfo . class ); extraClasses . add ( SupplierInfo . class ); kieServicesConfig . addExtraClasses ( extraClasses ); Next, we create the KieServicesClient : ~~~java KieServicesClient kieServicesClient = KieServicesFactory.newKieServicesClient(kieServicesConfig); ~~ From this client we retrieve our ProcessServicesClient : ProcessServicesClient processServicesClient = kieServicesClient . getServicesClient ( ProcessServicesClient . class ); We now create a Map which we will use to pass the process input variables. We create a new OrderInfo instance and SupplierInfo instance and put them in the Map . Map < String , Object > inputData = new HashMap <> (); OrderInfo orderInfo = new OrderInfo (); orderInfo . setItem ( \"Huawei P10\" ); orderInfo . setUrgency ( \"low\" ); inputData . put ( \"orderInfo\" , orderInfo ); SupplierInfo supplierInfo = new SupplierInfo (); supplierInfo . setUser ( \"bamAdmin\" ); inputData . put ( \"supplierInfo\" , supplierInfo ); \ud83d\udcd8 INFO: If you're using the Linux environment on Skytap use pamadmin : pamadm1n as the username password Map < String , Object > inputData = new HashMap <> (); OrderInfo orderInfo = new OrderInfo (); orderInfo . setItem ( \"Huawei P10\" ); orderInfo . setUrgency ( \"low\" ); inputData . put ( \"orderInfo\" , orderInfo ); SupplierInfo supplierInfo = new SupplierInfo (); supplierInfo . setUser ( \"pamadmin\" ); inputData . put ( \"supplierInfo\" , supplierInfo ); We can now start a new process instance via the ProcessServicesClient . Long processInstanceId = processServicesClient . startProcess ( CONTAINER_ID , PROCESS_ID , inputData ); Finally, we can print the process instance id to System.out . System . out . println ( \"New *Order Management* process instance started with instance-id: \" + processInstanceId ); Compile your project and run it. Observe the output in the console, which should say: New Order Management process instance started with instance-id The complete project can be found here: https://github.com/timwuthenow/rhpam7-order-management-demo-repo The KIE-Server Client provides more services to interact with the Execution Server: UserTaskServicesClient : provides functionality to interact with the UserTask services, for example to claim, start and complete a User Task . CaseServicesClient : provides functionality to interact with the Case Management features of the Execution Server. ProcessAdminServicesClient : provides the administration API for processes. etc. We leave as an exercise to the reader to try to complete a User Task , of the process instance we\u2019ve just created, using the UserTaskServicesClient .","title":"Using the KIE-Server Client"},{"location":"guided_exercises/05_bam_kafka/","text":"IBAMOE + Kafka Workshop A set of guided labs to get you up started on how to: Running locally Have docker or podman running locally: ~~~shell docker run -it --rm -p 8080:8080 -v $(pwd):/app-data -e CONTENT_URL_PREFIX=\"file:///app-data\" -e WORKSHOPS_URLS=\"file:///app-data/_bam_kafka_workshop.yml\" -e LOG_TO_STDOUT=true quay.io/osevg/workshopper ~~~ Running on ocp Login on ocp and: ~~~shell oc create -f support/ocp-provisioning.yml ~~~","title":"IBAMOE + Kafka Workshop"},{"location":"guided_exercises/05_bam_kafka/#ibamoe-kafka-workshop","text":"","title":"IBAMOE + Kafka Workshop"},{"location":"guided_exercises/05_bam_kafka/#a-set-of-guided-labs-to-get-you-up-started-on-how-to","text":"","title":"A set of guided labs to get you up started on how to:"},{"location":"guided_exercises/05_bam_kafka/#running-locally","text":"Have docker or podman running locally: ~~~shell docker run -it --rm -p 8080:8080 -v $(pwd):/app-data -e CONTENT_URL_PREFIX=\"file:///app-data\" -e WORKSHOPS_URLS=\"file:///app-data/_bam_kafka_workshop.yml\" -e LOG_TO_STDOUT=true quay.io/osevg/workshopper ~~~","title":"Running locally"},{"location":"guided_exercises/05_bam_kafka/#running-on-ocp","text":"Login on ocp and: ~~~shell oc create -f support/ocp-provisioning.yml ~~~","title":"Running on ocp"},{"location":"guided_exercises/05_bam_kafka/00_introduction/","text":"Introduction In this guided lab let\u2019s see in practice how we can use process automation applications that fits within event-driven architectures. We can list at least three ways to adjust our business application fit within EDA: We can build processes that can react to events that happen in the ecosystem; From within the process, we can emit events to notify the ecosystem about key activities in the business process and interact with external services via events; We can track every transaction committed either for business processes, cases (case management), or human tasks by publishing events for. The alignment of tech evolution and business standards like BMPN When providing an implementation for a specification, each provider has the opportunity to deliver the solution of choice. It is not different for the BPMN specification. It allows different implementations for its diagram elements, and this is how IBAMOE delivers the most recent tech concepts by still allowing business users to use the modeling notation they are used to. In IBAMOE (a.k.a. jBPM), it is possible to make use of message events (starting, intermediate or ending) to interact via events. In this case, the KIE Server Kafka extension makes sure the communication occurs effectively with the event streaming brokers. In the upcoming labs we will learn how to model the processes and when and how to add configurations to the business project and KIE Server.","title":"Introduction"},{"location":"guided_exercises/05_bam_kafka/00_introduction/#introduction","text":"In this guided lab let\u2019s see in practice how we can use process automation applications that fits within event-driven architectures. We can list at least three ways to adjust our business application fit within EDA: We can build processes that can react to events that happen in the ecosystem; From within the process, we can emit events to notify the ecosystem about key activities in the business process and interact with external services via events; We can track every transaction committed either for business processes, cases (case management), or human tasks by publishing events for.","title":"Introduction"},{"location":"guided_exercises/05_bam_kafka/00_introduction/#the-alignment-of-tech-evolution-and-business-standards-like-bmpn","text":"When providing an implementation for a specification, each provider has the opportunity to deliver the solution of choice. It is not different for the BPMN specification. It allows different implementations for its diagram elements, and this is how IBAMOE delivers the most recent tech concepts by still allowing business users to use the modeling notation they are used to. In IBAMOE (a.k.a. jBPM), it is possible to make use of message events (starting, intermediate or ending) to interact via events. In this case, the KIE Server Kafka extension makes sure the communication occurs effectively with the event streaming brokers. In the upcoming labs we will learn how to model the processes and when and how to add configurations to the business project and KIE Server.","title":"The alignment of tech evolution and business standards like BMPN"},{"location":"guided_exercises/05_bam_kafka/01_lab-one-setup/","text":"IBAMOE 8.0 Kafka extension In order to be able to start processes based on new events, we will need to configure the IBAMOEKafka extension. The IBAMOE Kafka extension allows the KIE Server (process and decision engine) to react to events and publish events to kafka topics. \ud83d\udcd8 INFO: There are several options in IBAMOE to customize the Kafka address, topic names, etc. In our case, we\u2019re using the default Kafka address, which is, localhost:9092. More customization information can be found in the official Red Hat product documentation: Configuring a KIE Server to send and receive Kafka messages from the process. In this setup steps, we will configure IBAMOE only in the server level - we are not yet configuring the business project . We will see how to configure the project as we move forward on the labs. Enabling the Kafka extension We can configure the engine to support different capabilities. In order to enable processes to be started through eventing, we only need to enable the extension via system property. With IBAMOE up and running, execute the following, where $JBOSS_HOME is the installation directory of the JBoss EAP instance you're running KIE Server from: $JBOSS_HOME /bin/jboss-cli.sh -c [ standalone@localhost:9990 / ] /system-property = org.kie.kafka.server.ext.disabled:add ( value = false ) [ standalone@localhost:9990 / ] :shutdown ( restart = true ) The first command will enable the Kafka extension. Next, we're restarting EAP so that the new configuration is active. You can check EAP logs to confirm it is restarting. The following output will show up in IBAMOE logs: INFO [org.kie.server.services.impl.KieServerImpl] (ServerService Thread Pool -- 74) Kafka KIE Server extension has been successfully registered as server extension This is the only configuration you will need in a server level to be able to start processes using events.","title":"Setup - Starting Processes with Events"},{"location":"guided_exercises/05_bam_kafka/01_lab-one-setup/#ibamoe-80-kafka-extension","text":"In order to be able to start processes based on new events, we will need to configure the IBAMOEKafka extension. The IBAMOE Kafka extension allows the KIE Server (process and decision engine) to react to events and publish events to kafka topics. \ud83d\udcd8 INFO: There are several options in IBAMOE to customize the Kafka address, topic names, etc. In our case, we\u2019re using the default Kafka address, which is, localhost:9092. More customization information can be found in the official Red Hat product documentation: Configuring a KIE Server to send and receive Kafka messages from the process. In this setup steps, we will configure IBAMOE only in the server level - we are not yet configuring the business project . We will see how to configure the project as we move forward on the labs.","title":"IBAMOE 8.0 Kafka extension"},{"location":"guided_exercises/05_bam_kafka/01_lab-one-setup/#enabling-the-kafka-extension","text":"We can configure the engine to support different capabilities. In order to enable processes to be started through eventing, we only need to enable the extension via system property. With IBAMOE up and running, execute the following, where $JBOSS_HOME is the installation directory of the JBoss EAP instance you're running KIE Server from: $JBOSS_HOME /bin/jboss-cli.sh -c [ standalone@localhost:9990 / ] /system-property = org.kie.kafka.server.ext.disabled:add ( value = false ) [ standalone@localhost:9990 / ] :shutdown ( restart = true ) The first command will enable the Kafka extension. Next, we're restarting EAP so that the new configuration is active. You can check EAP logs to confirm it is restarting. The following output will show up in IBAMOE logs: INFO [org.kie.server.services.impl.KieServerImpl] (ServerService Thread Pool -- 74) Kafka KIE Server extension has been successfully registered as server extension This is the only configuration you will need in a server level to be able to start processes using events.","title":"Enabling the Kafka extension"},{"location":"guided_exercises/05_bam_kafka/02_start-event/","text":"The Credit Card Raise Approval Project In this use case we would like to handle the automation of a credit limit increase approval process. Most card issuers allow customers to request an increased credit limit through various entry points such as: websites, their mobile applications or over the phone with customer service. Let\u2019s consider we need to deliver this automation for a bank that wants to achieve a similar use case within an event-driven architecture. The existing process is started via REST. It has a step for automatic request validation using DMN, and if the request not approved, it goes to a manual review queue. If approved, the service responsible for updating the cc limit is invoked via REST (the diagram only represents this REST call with a script task since this is not relevant for this lab's scenario). Finally, the process ends either with an approved or denied request. Now, with the architecture shift, the service responsible for increasing the credit card limit should not be invoked via REST anymore. The external service now listens to the topic \u201crequest-approved\u201d in order to track when to execute the limit raise. The business process should get started based on events, and whenever the process finishes, it should post a message to a specific topic depending on whether the request was approved or not. Process v2. Whenever a new event happens in a topic, a new instance will be triggered. Depending on how this process ends, an event is published in a different topic, therefore, different services can react based on the approval status . In this strategy we have a resilient way of communication between services where the broker is responsible for storing and providing the events. Adding to that, the tech team can evolve the solutions by using the features available in Kafka itself, like the possibility to replay all the events that happened in a specific time, in chronological order. Importing the project Let's import the existing project so we can start implementing the eventing capabilities. Access Business Central, and import the following project: https://github.com/kmacedovarela/cc-limit-approval-app-step1 Let's check the existing project. Open the cc-limit-raise-approval process. Notice that currently it is a traditional process, with a standard start and stop node. Processes like this can be started either via REST or JMS. Reacting to events The first task we'll do, is to enable the existing process to react to events that are published in a specific topic. Whenever a new event is published, a new process instance should be created. To allow this process definition to be started with events, the first step is to change the start event to a Start Message Event by clicking the node in the editor and selecting the envelope icon to \"Convert into Start Message\": Whenever a customer make a new request (independently of the channel used) an event should be published on the incoming-requests Kafka topic . This way as new channels are ready to be added, they just need to point to this request instead of modifying for the process end point. With that, a new process instance should be started whenever a new event is published in this topic. Let's configure the Start Message Event with the incoming-requests topic: \ud83d\udea7 WARN: we need to receive the data that is the event data. The KIE Server provides automatic marshalling to help us mapping the input directly to a Data Object (a POJO). This project has an object named LimitRaiseRequest.java which we will use to receive the incoming data and feed it in the process. On the properties panel of the Start Message Event , configure the input data: Name: request Data Type: LimitRaiseRequest Target: request Save the process. Your process should now look like this: Deploying the project Now, let's deploy and test the project. On the breadcrumb, click on \"cc-limit-approval-app-step1\" to go back to the Project Explorer view. Click on the \"Deploy\" button. Testing the project Let's publish a new event in the incoming-requests topic using the Kafka producer CLI tool. Open a new tab in your terminal and access the strimzi-all-in-one project folder. cd ~/enablement/amq-examples/strimzi-all-in-one Next, use the Kafka producer to publish new messages on the topic incoming-requests . docker-compose exec kafka bin/kafka-console-producer.sh --topic incoming-requests --bootstrap-server localhost:9092 You can send the following data, and press enter: { \"customerId\" : 1 , \"customerScore\" : 250 , \"requestedValue\" : 1500 } Back to the browser, open Business Central. On the top menu, go to Menu -> Manage -> Process Instances . On the left column, filter by \"Completed\" State. You should see as many instances as the number of events you published on Kafka: Select a process instance, and next, select the tab Diagram . You should see something like:","title":"Starting Processes with Events"},{"location":"guided_exercises/05_bam_kafka/02_start-event/#the-credit-card-raise-approval-project","text":"In this use case we would like to handle the automation of a credit limit increase approval process. Most card issuers allow customers to request an increased credit limit through various entry points such as: websites, their mobile applications or over the phone with customer service. Let\u2019s consider we need to deliver this automation for a bank that wants to achieve a similar use case within an event-driven architecture. The existing process is started via REST. It has a step for automatic request validation using DMN, and if the request not approved, it goes to a manual review queue. If approved, the service responsible for updating the cc limit is invoked via REST (the diagram only represents this REST call with a script task since this is not relevant for this lab's scenario). Finally, the process ends either with an approved or denied request. Now, with the architecture shift, the service responsible for increasing the credit card limit should not be invoked via REST anymore. The external service now listens to the topic \u201crequest-approved\u201d in order to track when to execute the limit raise. The business process should get started based on events, and whenever the process finishes, it should post a message to a specific topic depending on whether the request was approved or not. Process v2. Whenever a new event happens in a topic, a new instance will be triggered. Depending on how this process ends, an event is published in a different topic, therefore, different services can react based on the approval status . In this strategy we have a resilient way of communication between services where the broker is responsible for storing and providing the events. Adding to that, the tech team can evolve the solutions by using the features available in Kafka itself, like the possibility to replay all the events that happened in a specific time, in chronological order.","title":"The Credit Card Raise Approval Project"},{"location":"guided_exercises/05_bam_kafka/02_start-event/#importing-the-project","text":"Let's import the existing project so we can start implementing the eventing capabilities. Access Business Central, and import the following project: https://github.com/kmacedovarela/cc-limit-approval-app-step1 Let's check the existing project. Open the cc-limit-raise-approval process. Notice that currently it is a traditional process, with a standard start and stop node. Processes like this can be started either via REST or JMS.","title":"Importing the project"},{"location":"guided_exercises/05_bam_kafka/02_start-event/#reacting-to-events","text":"The first task we'll do, is to enable the existing process to react to events that are published in a specific topic. Whenever a new event is published, a new process instance should be created. To allow this process definition to be started with events, the first step is to change the start event to a Start Message Event by clicking the node in the editor and selecting the envelope icon to \"Convert into Start Message\": Whenever a customer make a new request (independently of the channel used) an event should be published on the incoming-requests Kafka topic . This way as new channels are ready to be added, they just need to point to this request instead of modifying for the process end point. With that, a new process instance should be started whenever a new event is published in this topic. Let's configure the Start Message Event with the incoming-requests topic: \ud83d\udea7 WARN: we need to receive the data that is the event data. The KIE Server provides automatic marshalling to help us mapping the input directly to a Data Object (a POJO). This project has an object named LimitRaiseRequest.java which we will use to receive the incoming data and feed it in the process. On the properties panel of the Start Message Event , configure the input data: Name: request Data Type: LimitRaiseRequest Target: request Save the process. Your process should now look like this:","title":"Reacting to events"},{"location":"guided_exercises/05_bam_kafka/02_start-event/#deploying-the-project","text":"Now, let's deploy and test the project. On the breadcrumb, click on \"cc-limit-approval-app-step1\" to go back to the Project Explorer view. Click on the \"Deploy\" button.","title":"Deploying the project"},{"location":"guided_exercises/05_bam_kafka/02_start-event/#testing-the-project","text":"Let's publish a new event in the incoming-requests topic using the Kafka producer CLI tool. Open a new tab in your terminal and access the strimzi-all-in-one project folder. cd ~/enablement/amq-examples/strimzi-all-in-one Next, use the Kafka producer to publish new messages on the topic incoming-requests . docker-compose exec kafka bin/kafka-console-producer.sh --topic incoming-requests --bootstrap-server localhost:9092 You can send the following data, and press enter: { \"customerId\" : 1 , \"customerScore\" : 250 , \"requestedValue\" : 1500 } Back to the browser, open Business Central. On the top menu, go to Menu -> Manage -> Process Instances . On the left column, filter by \"Completed\" State. You should see as many instances as the number of events you published on Kafka: Select a process instance, and next, select the tab Diagram . You should see something like:","title":"Testing the project"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/","text":"Emmitting Events from Business Processes In order to be able to finish processes based on new events, we will need to set up our environment. In this setup we will: Check the required configuration in the business project Add bpmn components to emit messages Emitting events in a business process Now, we need to End Message Events instead of two End Events . In Business Central, open the cc-limit-approval-app process. Convert the two End Events to End Message Events . It should look like this: Next, configure the Kafka topic name in the message name for both nodes as following: Raise Approved message name: requests-approved Raise Denied message name: requests-denied See below one of the nodes, the Raise Denied node configuration: Save the process definition. On the breadcrumb, click on \"cc-limit-approval-app-step1\" to go back to the Project Explorer view. Click on the \"Deploy\" button. Configuring the business application In Business Central, navigate to the Project Settings -> Deployments -> Work Item handlers : Observe that there is a task configured named Send Task . In IBAMOE {{ version }} you need this configuration to be able to use any Message Events (ending and throwing) that would emit events. Consuming the events from Kafka topic using Kafka Consumer CLI In order to validate if our process is emitting processes as we expect, we need to listen to the Kafka topics requests-approved and requests-denied to validate if the messages were emitted correctly. Open a new terminal tab, and navigate to the Kafka project folder. cd ~/enablement/amq-examples/strimzi-all-in-one/ Start the Kafka command line tool that allows us to consume events that happen in a topic, and therefore, will allow us to know if IBAMOE published the events when the process ended. The tool is kafka-console-consumer.sh . Let's check if the process emitted events on the topic requests-approved . docker-compose exec kafka bin/kafka-console-consumer.sh --topic requests-approved --from-beginning --bootstrap-server localhost:9092 Testing the solution To test the solution, we will start a new process instance that will start, be automatically approved, and end without any human interaction. A new process instance should get started whenever you publish a new event on the incoming-requests topic, and, when there is an automatic approval, the process will end and publish an event to the requests-approved topic. Let's see this in action: Like we did on the first lab, let's start a new process instance by publishing a message in the incoming-requests topic. If you canceled the execution of the kafka producer, here's how you can start it: docker-compose exec kafka bin/kafka-console-producer.sh --topic incoming-requests --bootstrap-server localhost:9092 You can use the following data in your event: {\"data\" : {\"customerId\": 1, \"customerScore\": 250, \"requestedValue\":1500}} When you hit return, the data is published to the incoming-requests topic Kafka reads the event from the incoming-requests and automatically instantiates a new process is with this data. Now check the terminal where you are consuming the messages in the requests-approved topic. You should see a new event published by your process. The event will look like this (though not on multiple lines): { \"specversion\" : \"1.0\" , \"time\" : \"2021-04-14T18:04:42.532-0300\" , \"id\" : \"25ba2dd0-a8d0-4cfc-9ba4-d2e556ffb4d0\" , \"type\" : \"empty\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval/5\" , \"data\" : null } Identify the process ID on the event above. In this example, the process instance that emitted this event was process of ID 5 . Let's check this same process instance in Business Central. In Business Central, open the Menu -> Manage -> Process Instances . On the left column, filter by \"Completed\" State. You should see as many instances as the number of events you published on Kafka. Identify your process instance ID. In this example, instance with id 5 . Select the process instance. Next, select the tab Diagram . You should see something like:","title":"Emitting Events in Processes"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/#emmitting-events-from-business-processes","text":"In order to be able to finish processes based on new events, we will need to set up our environment. In this setup we will: Check the required configuration in the business project Add bpmn components to emit messages","title":"Emmitting Events from Business Processes"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/#emitting-events-in-a-business-process","text":"Now, we need to End Message Events instead of two End Events . In Business Central, open the cc-limit-approval-app process. Convert the two End Events to End Message Events . It should look like this: Next, configure the Kafka topic name in the message name for both nodes as following: Raise Approved message name: requests-approved Raise Denied message name: requests-denied See below one of the nodes, the Raise Denied node configuration: Save the process definition. On the breadcrumb, click on \"cc-limit-approval-app-step1\" to go back to the Project Explorer view. Click on the \"Deploy\" button.","title":"Emitting events in a business process"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/#configuring-the-business-application","text":"In Business Central, navigate to the Project Settings -> Deployments -> Work Item handlers : Observe that there is a task configured named Send Task . In IBAMOE {{ version }} you need this configuration to be able to use any Message Events (ending and throwing) that would emit events.","title":"Configuring the business application"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/#consuming-the-events-from-kafka-topic-using-kafka-consumer-cli","text":"In order to validate if our process is emitting processes as we expect, we need to listen to the Kafka topics requests-approved and requests-denied to validate if the messages were emitted correctly. Open a new terminal tab, and navigate to the Kafka project folder. cd ~/enablement/amq-examples/strimzi-all-in-one/ Start the Kafka command line tool that allows us to consume events that happen in a topic, and therefore, will allow us to know if IBAMOE published the events when the process ended. The tool is kafka-console-consumer.sh . Let's check if the process emitted events on the topic requests-approved . docker-compose exec kafka bin/kafka-console-consumer.sh --topic requests-approved --from-beginning --bootstrap-server localhost:9092","title":"Consuming the events from Kafka topic using Kafka Consumer CLI"},{"location":"guided_exercises/05_bam_kafka/03_emitting-events/#testing-the-solution","text":"To test the solution, we will start a new process instance that will start, be automatically approved, and end without any human interaction. A new process instance should get started whenever you publish a new event on the incoming-requests topic, and, when there is an automatic approval, the process will end and publish an event to the requests-approved topic. Let's see this in action: Like we did on the first lab, let's start a new process instance by publishing a message in the incoming-requests topic. If you canceled the execution of the kafka producer, here's how you can start it: docker-compose exec kafka bin/kafka-console-producer.sh --topic incoming-requests --bootstrap-server localhost:9092 You can use the following data in your event: {\"data\" : {\"customerId\": 1, \"customerScore\": 250, \"requestedValue\":1500}} When you hit return, the data is published to the incoming-requests topic Kafka reads the event from the incoming-requests and automatically instantiates a new process is with this data. Now check the terminal where you are consuming the messages in the requests-approved topic. You should see a new event published by your process. The event will look like this (though not on multiple lines): { \"specversion\" : \"1.0\" , \"time\" : \"2021-04-14T18:04:42.532-0300\" , \"id\" : \"25ba2dd0-a8d0-4cfc-9ba4-d2e556ffb4d0\" , \"type\" : \"empty\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval/5\" , \"data\" : null } Identify the process ID on the event above. In this example, the process instance that emitted this event was process of ID 5 . Let's check this same process instance in Business Central. In Business Central, open the Menu -> Manage -> Process Instances . On the left column, filter by \"Completed\" State. You should see as many instances as the number of events you published on Kafka. Identify your process instance ID. In this example, instance with id 5 . Select the process instance. Next, select the tab Diagram . You should see something like:","title":"Testing the solution"},{"location":"guided_exercises/05_bam_kafka/04_auditing-events/","text":"Auditing with Kafka When using the Kafka extension in IBM Business Automation Open Edition 8.0, every transaction for processes, cases and tasks execution can be tracked via events. For each of these categories, we'll have an event emitted to a Kafka topic, in other words, we'll have three topics here: jbpm-processes-events , jbpm-tasks-events , jbpm-cases-events . To enable this feature, you need to add the jbpm-event-emitters-kafka library to the engine, KIE Server. This can either be downloaded in the community repository for jBPM or via the Red Hat customer portal: rhpam-7.10.0-maven-repository.zip . The maven repository have ~1.5GB. In order to facilitate the execution of this lab, you can download the jbpm-event-emmiters-kafka for IBAMOE {{ version }} here ; FIGURE OUT THE REPLACEMENT Stop IBAMOE. Download the jbpm-event-emitters-kafka . It's name will be similar to jbpm-event-emitters-kafka-7.x.x.Final-redhat-x.jar . Since this is a behavior only needed by the engine, place the library inside the kie-server.war folder, inside the WEB-INF directory. TIP: If you downloaded the maven repository zip file in the Red Hat Customer Portal, you can find the jar inside the folder maven-repository/org/jbpm/jbpm-event-emitters-kafka/7.67.0.Final-redhat-00008/jbpm-event-emitters-kafka-7.67.0.Final-redhat-00008.jar cp jbpm-event-emitters-kafka-7.67.0.Final-redhat-00008.jar $JBOSS_EAP /standalone/deployments/kie-server.war/WEB-INF/lib/ Next,startIBAMOE server. Let's check the auditing behavior. Testing the feature To check the auditing capabilities you can start new processes, interact with human tasks and track the events that are being published on the jbpm-tasks-events and jbpm-processes-events topics. The event tracking are active also for processes that doesn't use message events elements. In this example we will check the behavior for our event driven business application. Start a new process by emitting an event. Let's start a process that will not be automatically approved. In this way, we will also have a human task created. You can emit the following event to the incoming-requests topic: { \"customerId\" : 1 , \"customerScore\" : 100 , \"requestedValue\" : 1200 } You should be able to see a new process instance can be seen in Business Central in the following status: You can use the kafka consumer CLI script to check the messages that were emitted on the topics: jbpm-processes-events and jbpm-tasks-events . You should be able to see an event like this published on the jbpm-process-events : { \"specversion\" : \"1.0\" , \"time\" : \"2022-09-15T10:00:05.609-0300\" , \"id\" : \"28e13bc0-1c92-42fd-8909-b48a206325d3\" , \"type\" : \"process\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval-with-end-events/2\" , \"data\" :{ \"compositeId\" : \"default-kieserver_2\" , \"id\" : 2 , \"processId\" : \"cc-limit-approval-app.cc-limit-raise-approval-with-end-events\" , \"processName\" : \"cc-limit-raise-approval-with-events\" , \"processVersion\" : \"1.0\" , \"state\" : 1 , \"containerId\" : \"cc-limit-approval-app_1.0.0-SNAPSHOT\" , \"initiator\" : \"unknown\" , \"date\" : \"2021-04-15T10:00:05.608-0300\" , \"processInstanceDescription\" : \"cc-limit-raise-approval-with-events\" , \"correlationKey\" : \"2\" , \"parentId\" : -1 , \"variables\" :{ \"request\" :{ \"customerId\" : 1 , \"requestedValue\" : 1200 , \"customerScore\" : 100 , \"denyReason\" : null }, \"approval\" : false , \"initiator\" : \"unknown\" }}} You should be able to see an event like this published on the jbpm-tasks-events : { \"specversion\" : \"1.0\" , \"time\" : \"2022-09-15T10:00:05.612-0300\" , \"id\" : \"2ac83d91-40d7-49f3-a114-2b72816a20a4\" , \"type\" : \"task\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval-with-end-events/2\" , \"data\" :{ \"compositeId\" : \"default-kieserver_2\" , \"id\" : 2 , \"priority\" : 0 , \"name\" : \"Analyst validation\" , \"subject\" : \"\" , \"description\" : \"\" , \"taskType\" : null , \"formName\" : \"Task\" , \"status\" : \"Ready\" , \"actualOwner\" : null , \"createdBy\" : null , \"createdOn\" : \"2021-04-15T10:00:05.590-0300\" , \"activationTime\" : \"2021-04-15T10:00:05.590-0300\" , \"expirationDate\" : null , \"skipable\" : false , \"workItemId\" : 2 , \"processInstanceId\" : 2 , \"parentId\" : -1 , \"processId\" : \"cc-limit-approval-app.cc-limit-raise-approval-with-end-events\" , \"containerId\" : \"cc-limit-approval-app_1.0.0-SNAPSHOT\" , \"potentialOwners\" :[ \"kie-server\" ], \"excludedOwners\" :[], \"businessAdmins\" :[ \"Administrator\" , \"Administrators\" ], \"inputData\" :{ \"Skippable\" : \"false\" , \"request\" :{ \"customerId\" : 1 , \"requestedValue\" : 1200 , \"customerScore\" : 100 , \"denyReason\" : null }, \"TaskName\" : \"Task\" , \"NodeName\" : \"Analyst validation\" , \"GroupId\" : \"kie-server\" }, \"outputData\" : null }} Using Business Central, tnteract with the human task Analyst Validation , and check the events emitted on the jbpm-tasks-events . You should be able to see at every task change, a new event in the jbpm-tasks-events . Also, for every transaction commited for the process, you should see new events on the jbpm-process-events . By now, you have an event-driven process, that can be integrated within an event driven architecture, and furthermore, can be tracked and monitored in an asyncronous way by the usage of events. The complete project can be found at: https://github.com/timwuthenow/cc-limit-approval-app","title":"Auditing through events"},{"location":"guided_exercises/05_bam_kafka/04_auditing-events/#auditing-with-kafka","text":"When using the Kafka extension in IBM Business Automation Open Edition 8.0, every transaction for processes, cases and tasks execution can be tracked via events. For each of these categories, we'll have an event emitted to a Kafka topic, in other words, we'll have three topics here: jbpm-processes-events , jbpm-tasks-events , jbpm-cases-events . To enable this feature, you need to add the jbpm-event-emitters-kafka library to the engine, KIE Server. This can either be downloaded in the community repository for jBPM or via the Red Hat customer portal: rhpam-7.10.0-maven-repository.zip . The maven repository have ~1.5GB. In order to facilitate the execution of this lab, you can download the jbpm-event-emmiters-kafka for IBAMOE {{ version }} here ; FIGURE OUT THE REPLACEMENT Stop IBAMOE. Download the jbpm-event-emitters-kafka . It's name will be similar to jbpm-event-emitters-kafka-7.x.x.Final-redhat-x.jar . Since this is a behavior only needed by the engine, place the library inside the kie-server.war folder, inside the WEB-INF directory. TIP: If you downloaded the maven repository zip file in the Red Hat Customer Portal, you can find the jar inside the folder maven-repository/org/jbpm/jbpm-event-emitters-kafka/7.67.0.Final-redhat-00008/jbpm-event-emitters-kafka-7.67.0.Final-redhat-00008.jar cp jbpm-event-emitters-kafka-7.67.0.Final-redhat-00008.jar $JBOSS_EAP /standalone/deployments/kie-server.war/WEB-INF/lib/ Next,startIBAMOE server. Let's check the auditing behavior.","title":"Auditing with Kafka"},{"location":"guided_exercises/05_bam_kafka/04_auditing-events/#testing-the-feature","text":"To check the auditing capabilities you can start new processes, interact with human tasks and track the events that are being published on the jbpm-tasks-events and jbpm-processes-events topics. The event tracking are active also for processes that doesn't use message events elements. In this example we will check the behavior for our event driven business application. Start a new process by emitting an event. Let's start a process that will not be automatically approved. In this way, we will also have a human task created. You can emit the following event to the incoming-requests topic: { \"customerId\" : 1 , \"customerScore\" : 100 , \"requestedValue\" : 1200 } You should be able to see a new process instance can be seen in Business Central in the following status: You can use the kafka consumer CLI script to check the messages that were emitted on the topics: jbpm-processes-events and jbpm-tasks-events . You should be able to see an event like this published on the jbpm-process-events : { \"specversion\" : \"1.0\" , \"time\" : \"2022-09-15T10:00:05.609-0300\" , \"id\" : \"28e13bc0-1c92-42fd-8909-b48a206325d3\" , \"type\" : \"process\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval-with-end-events/2\" , \"data\" :{ \"compositeId\" : \"default-kieserver_2\" , \"id\" : 2 , \"processId\" : \"cc-limit-approval-app.cc-limit-raise-approval-with-end-events\" , \"processName\" : \"cc-limit-raise-approval-with-events\" , \"processVersion\" : \"1.0\" , \"state\" : 1 , \"containerId\" : \"cc-limit-approval-app_1.0.0-SNAPSHOT\" , \"initiator\" : \"unknown\" , \"date\" : \"2021-04-15T10:00:05.608-0300\" , \"processInstanceDescription\" : \"cc-limit-raise-approval-with-events\" , \"correlationKey\" : \"2\" , \"parentId\" : -1 , \"variables\" :{ \"request\" :{ \"customerId\" : 1 , \"requestedValue\" : 1200 , \"customerScore\" : 100 , \"denyReason\" : null }, \"approval\" : false , \"initiator\" : \"unknown\" }}} You should be able to see an event like this published on the jbpm-tasks-events : { \"specversion\" : \"1.0\" , \"time\" : \"2022-09-15T10:00:05.612-0300\" , \"id\" : \"2ac83d91-40d7-49f3-a114-2b72816a20a4\" , \"type\" : \"task\" , \"source\" : \"/process/cc-limit-approval-app.cc-limit-raise-approval-with-end-events/2\" , \"data\" :{ \"compositeId\" : \"default-kieserver_2\" , \"id\" : 2 , \"priority\" : 0 , \"name\" : \"Analyst validation\" , \"subject\" : \"\" , \"description\" : \"\" , \"taskType\" : null , \"formName\" : \"Task\" , \"status\" : \"Ready\" , \"actualOwner\" : null , \"createdBy\" : null , \"createdOn\" : \"2021-04-15T10:00:05.590-0300\" , \"activationTime\" : \"2021-04-15T10:00:05.590-0300\" , \"expirationDate\" : null , \"skipable\" : false , \"workItemId\" : 2 , \"processInstanceId\" : 2 , \"parentId\" : -1 , \"processId\" : \"cc-limit-approval-app.cc-limit-raise-approval-with-end-events\" , \"containerId\" : \"cc-limit-approval-app_1.0.0-SNAPSHOT\" , \"potentialOwners\" :[ \"kie-server\" ], \"excludedOwners\" :[], \"businessAdmins\" :[ \"Administrator\" , \"Administrators\" ], \"inputData\" :{ \"Skippable\" : \"false\" , \"request\" :{ \"customerId\" : 1 , \"requestedValue\" : 1200 , \"customerScore\" : 100 , \"denyReason\" : null }, \"TaskName\" : \"Task\" , \"NodeName\" : \"Analyst validation\" , \"GroupId\" : \"kie-server\" }, \"outputData\" : null }} Using Business Central, tnteract with the human task Analyst Validation , and check the events emitted on the jbpm-tasks-events . You should be able to see at every task change, a new event in the jbpm-tasks-events . Also, for every transaction commited for the process, you should see new events on the jbpm-process-events . By now, you have an event-driven process, that can be integrated within an event driven architecture, and furthermore, can be tracked and monitored in an asyncronous way by the usage of events. The complete project can be found at: https://github.com/timwuthenow/cc-limit-approval-app","title":"Testing the feature"},{"location":"guided_exercises/operator/configmaps-deleteapp/","text":"ConfigMaps The Operator stores its configuration in a number of ConfigurationMaps . These ConfigurationMaps can be used to change more advanced configurations that can not be configured in the KieApp YAML. In the case the Operator upgrades the version of your IBAMOE environment, the Operator is aware that one of the ConfigMaps has changed and will make a backup of it during the upgrade. Viewing and editing ConfigMaps A powerful feature of OpenShift are ConfigMaps which provide mechanisms to inject containers with configuration data while keeping containers agnostic of OpenShift Container Platform. A ConfigMap can be used to store fine-grained information like individual properties or coarse-grained information like entire configuration files or JSON blobs. In this section, we will modify some of the default health properties that are different than the defaults provided with IBAMOE and we want them to roll out to every container that gets created with the operator. In the OpenShift Console, open Workloads \u2192 Config Maps . Note that the Operator keeps the current ConfigMaps, and the ones of the last 2 versions. Click on the kieconfigs-7.10.1 ConfigMap and open the YAML tab. Explore the configuration options. Set the initialDelaySeconds of the livenessProbe of the Business Central console from 180 to 240. Click the Save button to save the configuration. Go to \"Workloads \u2192 Deployment Configs\", open the rhpam-trial-rhpamcentr Deployment Config and open the YAML tab. Find the LivenessProbe initialDelaySeconds configuration and notice that it\u2019s still set to 180. Delete the DeploymentConfig. This will have the Operator reconciliation recreate the DC. Open the YAML configuation of this recreated DeploymentConfig. Find the LivenessProbe initialDelaySeconds configuration and note that this time it has been set to 240, the value set in the ConfigMap. Deleting an application Apart from provisioning an IBAMOE application, the Operator also allows us to easily delete an application. Navigate to Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp . Click on the kebab icon of the rhpam-trial KieApp and click Delete . Navigate back to Workloads \u2192 Deployment Configs and note that the IBAMOE Deployment Configs have been removed.","title":"ConfigMaps and Deleteting Projects"},{"location":"guided_exercises/operator/configmaps-deleteapp/#configmaps","text":"The Operator stores its configuration in a number of ConfigurationMaps . These ConfigurationMaps can be used to change more advanced configurations that can not be configured in the KieApp YAML. In the case the Operator upgrades the version of your IBAMOE environment, the Operator is aware that one of the ConfigMaps has changed and will make a backup of it during the upgrade.","title":"ConfigMaps"},{"location":"guided_exercises/operator/configmaps-deleteapp/#viewing-and-editing-configmaps","text":"A powerful feature of OpenShift are ConfigMaps which provide mechanisms to inject containers with configuration data while keeping containers agnostic of OpenShift Container Platform. A ConfigMap can be used to store fine-grained information like individual properties or coarse-grained information like entire configuration files or JSON blobs. In this section, we will modify some of the default health properties that are different than the defaults provided with IBAMOE and we want them to roll out to every container that gets created with the operator. In the OpenShift Console, open Workloads \u2192 Config Maps . Note that the Operator keeps the current ConfigMaps, and the ones of the last 2 versions. Click on the kieconfigs-7.10.1 ConfigMap and open the YAML tab. Explore the configuration options. Set the initialDelaySeconds of the livenessProbe of the Business Central console from 180 to 240. Click the Save button to save the configuration. Go to \"Workloads \u2192 Deployment Configs\", open the rhpam-trial-rhpamcentr Deployment Config and open the YAML tab. Find the LivenessProbe initialDelaySeconds configuration and notice that it\u2019s still set to 180. Delete the DeploymentConfig. This will have the Operator reconciliation recreate the DC. Open the YAML configuation of this recreated DeploymentConfig. Find the LivenessProbe initialDelaySeconds configuration and note that this time it has been set to 240, the value set in the ConfigMap.","title":"Viewing and editing ConfigMaps"},{"location":"guided_exercises/operator/configmaps-deleteapp/#deleting-an-application","text":"Apart from provisioning an IBAMOE application, the Operator also allows us to easily delete an application. Navigate to Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp . Click on the kebab icon of the rhpam-trial KieApp and click Delete . Navigate back to Workloads \u2192 Deployment Configs and note that the IBAMOE Deployment Configs have been removed.","title":"Deleting an application"},{"location":"guided_exercises/operator/configuration/","text":"KIE App Configuration The definition of the expected state of KIE-App environment is defined in the YAML definition the KIE-App. In this section we will slightly change this configuration to see how the Operator applies changes in the configuration of your IBM Business Automation Open Edition 8.0 environment. Changing Credentials Go back to the YAML definition of your rhpam-trial KieApp. Add a commonConfig section, with the adminUser to the value bamAdmin , and the adminPassword to ibmpam1! . Click on the Save button. spec : commonConfig : adminPassword : bamAdmin adminUser : ibmpam1! Click the Reload button to reload the YAML view. Click on the Overview tab. Notice the deployments re-deploying. Click on the Business/Central Central URL to open the Business Central console. Log in with the new username and password: bamAdmin / ibmpam1! . Adding a KIE-Server Apart from changing some configuration parameters, we can also change the topology our deployment in the KieApp YAML file. Go back to the YAML definition of your rhpam-trial KieApp. Add a servers section and set the replicas parameter of the rhpam-trial-kieserver to 2 . objects : servers : - deployments : 1 name : rhpam-trial-kieserver replicas : 2 Click the Save button. Go to Workloads \u2192 Deployment Configs . Note that there are now 2 KIE-Server Deployment Configs. Go back to the YAML definition of your rhpam-trial KieApp. Navigate to the servers section and add the property deployments with the value 2 . objects : servers : - deployments : 2 name : rhpam-trial-kieserver replicas : 2 Click the Save button.","title":"Kie App Configuration"},{"location":"guided_exercises/operator/configuration/#kie-app-configuration","text":"The definition of the expected state of KIE-App environment is defined in the YAML definition the KIE-App. In this section we will slightly change this configuration to see how the Operator applies changes in the configuration of your IBM Business Automation Open Edition 8.0 environment.","title":"KIE App Configuration"},{"location":"guided_exercises/operator/configuration/#changing-credentials","text":"Go back to the YAML definition of your rhpam-trial KieApp. Add a commonConfig section, with the adminUser to the value bamAdmin , and the adminPassword to ibmpam1! . Click on the Save button. spec : commonConfig : adminPassword : bamAdmin adminUser : ibmpam1! Click the Reload button to reload the YAML view. Click on the Overview tab. Notice the deployments re-deploying. Click on the Business/Central Central URL to open the Business Central console. Log in with the new username and password: bamAdmin / ibmpam1! .","title":"Changing Credentials"},{"location":"guided_exercises/operator/configuration/#adding-a-kie-server","text":"Apart from changing some configuration parameters, we can also change the topology our deployment in the KieApp YAML file. Go back to the YAML definition of your rhpam-trial KieApp. Add a servers section and set the replicas parameter of the rhpam-trial-kieserver to 2 . objects : servers : - deployments : 1 name : rhpam-trial-kieserver replicas : 2 Click the Save button. Go to Workloads \u2192 Deployment Configs . Note that there are now 2 KIE-Server Deployment Configs. Go back to the YAML definition of your rhpam-trial KieApp. Navigate to the servers section and add the property deployments with the value 2 . objects : servers : - deployments : 2 name : rhpam-trial-kieserver replicas : 2 Click the Save button.","title":"Adding a KIE-Server"},{"location":"guided_exercises/operator/installer/","text":"Operator Wizard Walkthrough The Business Automation Operator contains an Operator Installer Console . This console gives you a wizard experience to deploy IBM Business Automation Open Edition 8.0 environments. Wizard walk through Go the Business Automation Operator and click in Installer link. Login with Openshift. A page will show up asking for authorization. Select all options and click on \"Allow selected permissions\". Give the application the name my-rhpam-prod . Select the rhpam-production for the Enviroment. Check the Enable Upgrades checkbox. Scroll down and set the Username and Password to bamAdmin : ibmpam1 . Click the Next button. Don\u2019t change any values in the Security section. Click on Next . Go through the Components section of the installer and observe the possible options. Don\u2019t change any values for now. Keep clicking next until you reach the Confirmation screen and click Deploy . Go back to the OpenShift Console. Navigate to Workloads \u2192 Deployment Configs and observe that a new IBM Business Automation Open Edition 8.0 production environment has been deployed. Note that this environment has a PostgreSQL database deployed. Also note that both the Business Central and KIE-Server Deployment Configs have their ReplicationController set to 3 pods. Go back to the Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp . Delete the my-rhpam-prod we\u2019ve just deployed with the Installer. How to KIE Server with more replicas We will now deploy a new production environment using the installer, but this time we will configure our KIE-Server in the wizard and set the replications of the KIE-Server to 2 instead of 3. Go back to the Business Automation Operator, and open the Wizard. Create a new IBAMOE Production Environment. Continue until you reach the KIE Servers screen. Click Add new KIE Server and use the following configuration for your KIE-Server. Click through the rest of the screens until you can press the Deploy button to deploy the environment. Navigate to the Workloads \u2192 Deployment Configs screen to see your IBAMOE production environment, including the KIE-Server you configured. Conclusion This concludes the lab on the Business Automation Operator. If you have time left, feel free to explore more features of the operator.","title":"Operator Wizard"},{"location":"guided_exercises/operator/installer/#operator-wizard-walkthrough","text":"The Business Automation Operator contains an Operator Installer Console . This console gives you a wizard experience to deploy IBM Business Automation Open Edition 8.0 environments.","title":"Operator Wizard Walkthrough"},{"location":"guided_exercises/operator/installer/#wizard-walk-through","text":"Go the Business Automation Operator and click in Installer link. Login with Openshift. A page will show up asking for authorization. Select all options and click on \"Allow selected permissions\". Give the application the name my-rhpam-prod . Select the rhpam-production for the Enviroment. Check the Enable Upgrades checkbox. Scroll down and set the Username and Password to bamAdmin : ibmpam1 . Click the Next button. Don\u2019t change any values in the Security section. Click on Next . Go through the Components section of the installer and observe the possible options. Don\u2019t change any values for now. Keep clicking next until you reach the Confirmation screen and click Deploy . Go back to the OpenShift Console. Navigate to Workloads \u2192 Deployment Configs and observe that a new IBM Business Automation Open Edition 8.0 production environment has been deployed. Note that this environment has a PostgreSQL database deployed. Also note that both the Business Central and KIE-Server Deployment Configs have their ReplicationController set to 3 pods. Go back to the Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp . Delete the my-rhpam-prod we\u2019ve just deployed with the Installer.","title":"Wizard walk through"},{"location":"guided_exercises/operator/installer/#how-to-kie-server-with-more-replicas","text":"We will now deploy a new production environment using the installer, but this time we will configure our KIE-Server in the wizard and set the replications of the KIE-Server to 2 instead of 3. Go back to the Business Automation Operator, and open the Wizard. Create a new IBAMOE Production Environment. Continue until you reach the KIE Servers screen. Click Add new KIE Server and use the following configuration for your KIE-Server. Click through the rest of the screens until you can press the Deploy button to deploy the environment. Navigate to the Workloads \u2192 Deployment Configs screen to see your IBAMOE production environment, including the KIE-Server you configured.","title":"How to KIE Server with more replicas"},{"location":"guided_exercises/operator/installer/#conclusion","text":"This concludes the lab on the Business Automation Operator. If you have time left, feel free to explore more features of the operator.","title":"Conclusion"},{"location":"guided_exercises/operator/introduction/","text":"Red Hat PAM Operator on OpenShift 4 In this lab we will use the enhanced Business Automation Operator 7.10+ to deploy a number of IBM Business Automation Open Edition 8.0 environments on OpenShift 4. Goal Install the Business Automation Operator on OCP 4. Use the Business Automation Operator 7.10 to deploy a number of Process Automation Manager environments. Change the KIE-App deployment CRDs to show reconciliation. Change Operator ConfigMaps to make advanced configuration changes to the KIE-App. Problem Statement In this lab, the goal is to provision and manage various IBM Business Automation Open Edition 8.0 architectures using the Business Automation Operator on OpenShift 4. We deploy an IBAMOE Trial environment, which is a basic ephemeral environment that does not require any form of storage (e.g. persistent volume, database). We explore Operator reconciliation features by removing provisioned resources like Services and Deployment Configs. We alter the deployment through the Operator to show how the provisioned environment changes. We change a KIE configuration parameter in the Business Automation Operator ConfigMap to demonstrate advanced configuration changes. We provision a more sophisticated Production environment, to show creation of PVCs, deployment of databases and integration with IBAMOE Smart Router. We use the Operator Installer console to install a new KIE-App deployment. First steps If you are using your own OpenShift environment , follow the steps below to create a project and install the operator. If you are trying this lab in an environment provisioned by the Red Hat team, skip to the section Inspect the Lab environment . Create a new project in OpenShift. We suggest the name rhpam710-operator-lab-user1 . Navigate to Operators , Operator Hub , and search for Business Automation : Click on the Business Automation and then, click Install . You can select the following options, and click on Submit : Once subscribed, you should wait for the operator to get provisioned. Then you can proceed with the lab. Inspect the Lab environment If you attending to an enablement with a provisioned environment. We provisioned an environment where each user already has a subscription to the Business Automation Operator. These Operator subscriptions are managed by the OpenShift cluster admin. Navigate to the OpenShift Master url. Login to the platform with the provided username and password. Open the project rhpam710-operator-lab-userX . Open the Workloads tab. Observe that the business-automation-operator has already been provisioned to your project. This has been done by the cluster-admin by subscribing your project to the Business Automation Operator. Expand the Operators menu group in the left-hand-side of the screen and click on Installed Operators . This will show the installed Operators, or Operator Subscriptions, in your OpenShift namespace. Click on Business Automation to access the Business Automation Operator instance in your project.","title":"Using the Business Automation Operator"},{"location":"guided_exercises/operator/introduction/#red-hat-pam-operator-on-openshift-4","text":"In this lab we will use the enhanced Business Automation Operator 7.10+ to deploy a number of IBM Business Automation Open Edition 8.0 environments on OpenShift 4.","title":"Red Hat PAM Operator on OpenShift 4"},{"location":"guided_exercises/operator/introduction/#goal","text":"Install the Business Automation Operator on OCP 4. Use the Business Automation Operator 7.10 to deploy a number of Process Automation Manager environments. Change the KIE-App deployment CRDs to show reconciliation. Change Operator ConfigMaps to make advanced configuration changes to the KIE-App.","title":"Goal"},{"location":"guided_exercises/operator/introduction/#problem-statement","text":"In this lab, the goal is to provision and manage various IBM Business Automation Open Edition 8.0 architectures using the Business Automation Operator on OpenShift 4. We deploy an IBAMOE Trial environment, which is a basic ephemeral environment that does not require any form of storage (e.g. persistent volume, database). We explore Operator reconciliation features by removing provisioned resources like Services and Deployment Configs. We alter the deployment through the Operator to show how the provisioned environment changes. We change a KIE configuration parameter in the Business Automation Operator ConfigMap to demonstrate advanced configuration changes. We provision a more sophisticated Production environment, to show creation of PVCs, deployment of databases and integration with IBAMOE Smart Router. We use the Operator Installer console to install a new KIE-App deployment.","title":"Problem Statement"},{"location":"guided_exercises/operator/introduction/#first-steps","text":"If you are using your own OpenShift environment , follow the steps below to create a project and install the operator. If you are trying this lab in an environment provisioned by the Red Hat team, skip to the section Inspect the Lab environment . Create a new project in OpenShift. We suggest the name rhpam710-operator-lab-user1 . Navigate to Operators , Operator Hub , and search for Business Automation : Click on the Business Automation and then, click Install . You can select the following options, and click on Submit : Once subscribed, you should wait for the operator to get provisioned. Then you can proceed with the lab.","title":"First steps"},{"location":"guided_exercises/operator/introduction/#inspect-the-lab-environment","text":"If you attending to an enablement with a provisioned environment. We provisioned an environment where each user already has a subscription to the Business Automation Operator. These Operator subscriptions are managed by the OpenShift cluster admin. Navigate to the OpenShift Master url. Login to the platform with the provided username and password. Open the project rhpam710-operator-lab-userX . Open the Workloads tab. Observe that the business-automation-operator has already been provisioned to your project. This has been done by the cluster-admin by subscribing your project to the Business Automation Operator. Expand the Operators menu group in the left-hand-side of the screen and click on Installed Operators . This will show the installed Operators, or Operator Subscriptions, in your OpenShift namespace. Click on Business Automation to access the Business Automation Operator instance in your project.","title":"Inspect the Lab environment"},{"location":"guided_exercises/operator/reconciliation/","text":"Reconciliation The OpenShift Operators provide functionality to reconciliate an existing environment in order to bring it back to its expected state. We will now test this feature by removing one of the required resources from our deployment. Open the Resources tab. This will show all the resources of the application deployed and managed by the Operator. On the fourth row, we can see the rhpam-trial-kieserver Service resource. In the left menu, go to Networking \u2192 Services . Open rhpam-trial-kieserver . Delete the Service by clicking on the Actions button at the upper right of the screen and clicking on Delete . Notice the Service disappearing and immediately reappearing. This is the Operators reconciliation logic at work, bringing the environment back in its expected state.","title":"Automatic reconciliation"},{"location":"guided_exercises/operator/reconciliation/#reconciliation","text":"The OpenShift Operators provide functionality to reconciliate an existing environment in order to bring it back to its expected state. We will now test this feature by removing one of the required resources from our deployment. Open the Resources tab. This will show all the resources of the application deployed and managed by the Operator. On the fourth row, we can see the rhpam-trial-kieserver Service resource. In the left menu, go to Networking \u2192 Services . Open rhpam-trial-kieserver . Delete the Service by clicking on the Actions button at the upper right of the screen and clicking on Delete . Notice the Service disappearing and immediately reappearing. This is the Operators reconciliation logic at work, bringing the environment back in its expected state.","title":"Reconciliation"},{"location":"guided_exercises/operator/trial-environment/","text":"Deploying an IBAMOE Trial Environment From the Business Automation page in your OpenShift Console, open the KieApp tab and click on Create KieApp . A form will be displayed for you to choose which instalation option you want to have. notice the environment field. In this field we define the type of the environment we want to provision. In this case we want to provision the Trial environment, so we accept the default values. TIP: You also have the YAML definition option if you want to do customizations that are not available in the form above. Click on the Create button at the bottom of the page. In the KieApp tab, we can see our new rhpam-trial environment being listed. Expand the Workloads menu on the left side of the screen. Click on Deployment Configs . Observe that the Operator has created 2 Deployment Configs, one for Business Central and one for KIE-Server. Open the Developer Console by clicking on the link in the dropdown box at the top left of the screen. Click on the Topology link to show a graphical representation of the topology of our namespace, which includes an Operator DC, a Business Central DC, and a KIE-Server DC. Go back to the Adminstrator console. Open Networking \u2192 Routes menu to see all the available routes to our KIE application deployed in this namespace. Identify the Business/Decision Central URL link to navigate to the IBAMOE Business Central workbench. It should be named rhpam-trial-rhpamcentr-http for the http option, or rhpam-trial-rhpamcentr for https. As the Operator is responsible for deployment and configuration of the IBAMOE environment, we can find the details if this deployment in the KieApp instance details screen. Open your KieApp in Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp \u2192 rhpam-trial , and click on the YAML tab. We can see in the YAML description that the adminPassword has been set to RedHat . Navigate back to the Business Central workbench and login with u: adminUser p: RedHat . Explore the Business Central application. In particular, go to Menu \u2192 Deploy \u2192 Execution Servers to see the Execution Server connected to the workbench.","title":"Deploying a trial environment"},{"location":"guided_exercises/operator/trial-environment/#deploying-an-ibamoe-trial-environment","text":"From the Business Automation page in your OpenShift Console, open the KieApp tab and click on Create KieApp . A form will be displayed for you to choose which instalation option you want to have. notice the environment field. In this field we define the type of the environment we want to provision. In this case we want to provision the Trial environment, so we accept the default values. TIP: You also have the YAML definition option if you want to do customizations that are not available in the form above. Click on the Create button at the bottom of the page. In the KieApp tab, we can see our new rhpam-trial environment being listed. Expand the Workloads menu on the left side of the screen. Click on Deployment Configs . Observe that the Operator has created 2 Deployment Configs, one for Business Central and one for KIE-Server. Open the Developer Console by clicking on the link in the dropdown box at the top left of the screen. Click on the Topology link to show a graphical representation of the topology of our namespace, which includes an Operator DC, a Business Central DC, and a KIE-Server DC. Go back to the Adminstrator console. Open Networking \u2192 Routes menu to see all the available routes to our KIE application deployed in this namespace. Identify the Business/Decision Central URL link to navigate to the IBAMOE Business Central workbench. It should be named rhpam-trial-rhpamcentr-http for the http option, or rhpam-trial-rhpamcentr for https. As the Operator is responsible for deployment and configuration of the IBAMOE environment, we can find the details if this deployment in the KieApp instance details screen. Open your KieApp in Operators \u2192 Installed Operators \u2192 Business Automation \u2192 KieApp \u2192 rhpam-trial , and click on the YAML tab. We can see in the YAML description that the adminPassword has been set to RedHat . Navigate back to the Business Central workbench and login with u: adminUser p: RedHat . Explore the Business Central application. In particular, go to Menu \u2192 Deploy \u2192 Execution Servers to see the Execution Server connected to the workbench.","title":"Deploying an IBAMOE Trial Environment"},{"location":"guided_exercises/operator/version-upgrade/","text":"Version Upgrades The Operator of IBAMOE is also capable of doing both patch and minor upgrades. This means that, for example, the Operation can upgrade an IBAMOE environment from 7.10.0 to 7.10.1 or from 7.10.1 to 7.11.0. When creating a new KieApp, you can find the option to enable the version updates. If both the Enable Upgrades and Include minor version upgrades settings are set to true, the KieApp YAML configuration will include the following spec: With this configuration the version upgrade mechanism of the Operator should be enabled for the given KieApp.","title":"Versions and the Operator - Starting Processes with Events"},{"location":"guided_exercises/operator/version-upgrade/#version-upgrades","text":"The Operator of IBAMOE is also capable of doing both patch and minor upgrades. This means that, for example, the Operation can upgrade an IBAMOE environment from 7.10.0 to 7.10.1 or from 7.10.1 to 7.11.0. When creating a new KieApp, you can find the option to enable the version updates. If both the Enable Upgrades and Include minor version upgrades settings are set to true, the KieApp YAML configuration will include the following spec: With this configuration the version upgrade mechanism of the Operator should be enabled for the given KieApp.","title":"Version Upgrades"},{"location":"guided_exercises/operator/support/readme/","text":"To provision this guide in ocp: 1. Login to the cluster on your terminal 2. run oc create -f ocp-provisioning.yml","title":"Readme"},{"location":"guided_exercises/tools/authoring-decisions/","text":"Authoring a Decision Let's author a simple decision and test it. The use case we'll try out is the automation of a repeated decision for requests approval. Create a new Decision In the project we've just created: Select the folder where you want to create the new file. Click on resources . Next, click on the new folder icon: Name the file automated-request-approval.dmn and press enter. The file should open in the DMN Editor. Create the following DMN: This DRD contains: A decision node Approval of type boolean ; Two inputs: A request type , which is string A request price , that is a number . Implement the following decision table, in the decision node: Save the diagram Testing the decision Before deploying the decision service in KIE Server, let's do some unit testing using the Test Scenario Simulation tooling. Configuring the project In order to use test scenarios, you need to add at least three dependencies to your project: junit:junit org.drools:drools-scenario-simulation-backend org.drools:drools-scenario-simulation-api Open the pom.xml file and add the following dependencies: <dependencies> <dependency> <groupId>org.drools</groupId> <artifactId>drools-scenario-simulation-api</artifactId> <version>${version.org.kie}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.drools</groupId> <artifactId>drools-scenario-simulation-backend</artifactId> <version>${version.org.kie}</version> <scope>test</scope> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.12</version> <scope>test</scope> </dependency> </dependencies> Adding the JUnit Runner In this scenario, the version.org.kie should be compatible with the product version you want to use. In this scenario, we are using IBAMOE 7.10, which would be <version.org.kie>7.67.0.Final-redhat-00008</version.org.kie> . Create a new folder testscenario: /src/test/java/testscenario In the folder you just created, add a file and name it ScenarioJunitActivatorTest.java In this class, you should add a the Scenario Activator. This class allows the test scenarios to run along with the junit tests. package testscenario; /** * Do not remove this file */ @org.junit.runner.RunWith(org.drools.scenariosimulation.backend.runner.ScenarioJunitActivator.class) public class ScenarioJunitActivatorTest { } It should look like this: Creating the test scenario On the folder src/test/resources/org/kie/businessapp create a new file named ValidateAutomaticDecision.scesim . The editor should open up with the option to choose the Source type . This is the type of rule you want to test. Select DMN, next, choose your DMN file and click on the create button. The tool will already bring the inputs and expected result columns based on your DMN. Now, implement the following test: Runing the tests You can run the tests in two ways: using maven or the JUnit activator class. To run the test with maven you can for example run: mvn test If you want to run the tests using the activator class: Right click the ScenarioJunitActivatorTest.java file and select Run Java : The execution results should show up: Try changing the line one expected result from true to false . Click on the re-run button to see the results. Finally, adjust the tests and make sure that your project can compile when you run: mvn clean install Next Steps Now it's time to deploy our project to KIE Server and test it out.","title":"Authoring and testing decisions"},{"location":"guided_exercises/tools/authoring-decisions/#authoring-a-decision","text":"Let's author a simple decision and test it. The use case we'll try out is the automation of a repeated decision for requests approval.","title":"Authoring a Decision"},{"location":"guided_exercises/tools/authoring-decisions/#create-a-new-decision","text":"In the project we've just created: Select the folder where you want to create the new file. Click on resources . Next, click on the new folder icon: Name the file automated-request-approval.dmn and press enter. The file should open in the DMN Editor. Create the following DMN: This DRD contains: A decision node Approval of type boolean ; Two inputs: A request type , which is string A request price , that is a number . Implement the following decision table, in the decision node: Save the diagram","title":"Create a new Decision"},{"location":"guided_exercises/tools/authoring-decisions/#testing-the-decision","text":"Before deploying the decision service in KIE Server, let's do some unit testing using the Test Scenario Simulation tooling.","title":"Testing the decision"},{"location":"guided_exercises/tools/authoring-decisions/#configuring-the-project","text":"In order to use test scenarios, you need to add at least three dependencies to your project: junit:junit org.drools:drools-scenario-simulation-backend org.drools:drools-scenario-simulation-api Open the pom.xml file and add the following dependencies: <dependencies> <dependency> <groupId>org.drools</groupId> <artifactId>drools-scenario-simulation-api</artifactId> <version>${version.org.kie}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.drools</groupId> <artifactId>drools-scenario-simulation-backend</artifactId> <version>${version.org.kie}</version> <scope>test</scope> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.12</version> <scope>test</scope> </dependency> </dependencies>","title":"Configuring the project"},{"location":"guided_exercises/tools/authoring-decisions/#adding-the-junit-runner","text":"In this scenario, the version.org.kie should be compatible with the product version you want to use. In this scenario, we are using IBAMOE 7.10, which would be <version.org.kie>7.67.0.Final-redhat-00008</version.org.kie> . Create a new folder testscenario: /src/test/java/testscenario In the folder you just created, add a file and name it ScenarioJunitActivatorTest.java In this class, you should add a the Scenario Activator. This class allows the test scenarios to run along with the junit tests. package testscenario; /** * Do not remove this file */ @org.junit.runner.RunWith(org.drools.scenariosimulation.backend.runner.ScenarioJunitActivator.class) public class ScenarioJunitActivatorTest { } It should look like this:","title":"Adding the JUnit Runner"},{"location":"guided_exercises/tools/authoring-decisions/#creating-the-test-scenario","text":"On the folder src/test/resources/org/kie/businessapp create a new file named ValidateAutomaticDecision.scesim . The editor should open up with the option to choose the Source type . This is the type of rule you want to test. Select DMN, next, choose your DMN file and click on the create button. The tool will already bring the inputs and expected result columns based on your DMN. Now, implement the following test:","title":"Creating the test scenario"},{"location":"guided_exercises/tools/authoring-decisions/#runing-the-tests","text":"You can run the tests in two ways: using maven or the JUnit activator class. To run the test with maven you can for example run: mvn test If you want to run the tests using the activator class: Right click the ScenarioJunitActivatorTest.java file and select Run Java : The execution results should show up: Try changing the line one expected result from true to false . Click on the re-run button to see the results. Finally, adjust the tests and make sure that your project can compile when you run: mvn clean install","title":"Runing the tests"},{"location":"guided_exercises/tools/authoring-decisions/#next-steps","text":"Now it's time to deploy our project to KIE Server and test it out.","title":"Next Steps"},{"location":"guided_exercises/tools/deploying-project/","text":"Deploying the project in KIE Server It's time to deploy our business application in KIE Server. Deployment We can deploy the project directly in KIE Server without the need to use Business Central. To do so, we can use the available REST API. Open KIE Server REST API. (i.e. http://localhost:8080/kie-server/docs) Under \u201cKIE Server and KIE container\" category select the following: PUT /server/containers/{containerId} Creates a new KIE container in the KIE Server with a specified KIE container ID Click on \"Try it out\" Insert your project details. The GAV can be found for example, in your pom.xml . See an example: containerId : mybusinessapp body : {\"container-id\" : \"mybusinessapp\", \"release-id\" : { \"group-id\" : \"org.kie.businessapp\", \"artifact-id\" : \"mybusinessapp\", \"version\" : \"1.0\" } } Click on the blue button \"Execute\". You should get a 201 result as follows: Testing the Automated Approval Decision Now, using the KIE server REST API, we'll consume the decision we've just deployed. Under the section DMN Models locate: POST /server/containers/{containerId}/dmn Evaluates decisions for given input Click on try it out Use the following data: ContainerID : mybusinessapp Body: { \"dmn-context\": { \"request type\": \"urgent\", \"request price\": \"250\" } } Extra Lab: Business Central Finally, you can import this project in Business Central. In order to do so, this needs to be a git-based project and Business Central needs to have access to the git repository where the project is stored. The following steps consider a local environment scenario. Access your application folder in the terminal. Initialize the git repository and do the first commit git init git add -A git commit -m \"first commit\" With this you can already import the project in Business Central. Open Business Central and select the import the project option. In the pop-up, in the Repository URL field, you should insert the git repository. If it is on your local machine you can inform something like: /$PROJECT_DIR/tooling-labs/mybusinessapp . Confirm the operation. You should see the project. Select it and click the Ok button. Feel free to explore the project and validate the test scenario and deployment through Business Central.","title":"Deploying and consuming services"},{"location":"guided_exercises/tools/deploying-project/#deploying-the-project-in-kie-server","text":"It's time to deploy our business application in KIE Server.","title":"Deploying the project in KIE Server"},{"location":"guided_exercises/tools/deploying-project/#deployment","text":"We can deploy the project directly in KIE Server without the need to use Business Central. To do so, we can use the available REST API. Open KIE Server REST API. (i.e. http://localhost:8080/kie-server/docs) Under \u201cKIE Server and KIE container\" category select the following: PUT /server/containers/{containerId} Creates a new KIE container in the KIE Server with a specified KIE container ID Click on \"Try it out\" Insert your project details. The GAV can be found for example, in your pom.xml . See an example: containerId : mybusinessapp body : {\"container-id\" : \"mybusinessapp\", \"release-id\" : { \"group-id\" : \"org.kie.businessapp\", \"artifact-id\" : \"mybusinessapp\", \"version\" : \"1.0\" } } Click on the blue button \"Execute\". You should get a 201 result as follows:","title":"Deployment"},{"location":"guided_exercises/tools/deploying-project/#testing-the-automated-approval-decision","text":"Now, using the KIE server REST API, we'll consume the decision we've just deployed. Under the section DMN Models locate: POST /server/containers/{containerId}/dmn Evaluates decisions for given input Click on try it out Use the following data: ContainerID : mybusinessapp Body: { \"dmn-context\": { \"request type\": \"urgent\", \"request price\": \"250\" } }","title":"Testing the Automated Approval Decision"},{"location":"guided_exercises/tools/deploying-project/#extra-lab-business-central","text":"Finally, you can import this project in Business Central. In order to do so, this needs to be a git-based project and Business Central needs to have access to the git repository where the project is stored. The following steps consider a local environment scenario. Access your application folder in the terminal. Initialize the git repository and do the first commit git init git add -A git commit -m \"first commit\" With this you can already import the project in Business Central. Open Business Central and select the import the project option. In the pop-up, in the Repository URL field, you should insert the git repository. If it is on your local machine you can inform something like: /$PROJECT_DIR/tooling-labs/mybusinessapp . Confirm the operation. You should see the project. Select it and click the Ok button. Feel free to explore the project and validate the test scenario and deployment through Business Central.","title":"Extra Lab: Business Central"},{"location":"guided_exercises/tools/getting-started/","text":"Business Automation projects in VSCode To start working with business automation projects in VSCode, you'll need to install the VSCode Extension that allows you to work with BPMN, DMN and Test Scenarios through graphical editors. Installing the VSCode extension To install the extension in VSCode, open the extensions menu, and search for Business Automation. You should find the Red Hat Business Automation Bundle. Click on install. If this is the first time you are using VSCode, it would be interesting to also install the code command in path, so that you can open projects directly from the terminal. To do so, press cmd+shift+p (or ctrl+shift+p ) to launch VSCode Quick Open menu. And next, search for Instal code command in PATH : Create new a project Let's create a new project using the maven archetype. This project should contain the structure and files that Business Central expects, so this project should be editable and authored in both VScode and Business Central. Now we will use the terminal. You can either use your terminal or use the built-in terminal In VScode. To use the terminal in VSCode you can press cmd+shift+p (or ctrl+shift+p ) to launch VSCode Quick Open menu. And next, open a new intergrated terminal : Next, in the terminal navigate to the directory where you would like to create the new project. Let's call it $PROJECT_DIR from now on. Create a new folder named tooling-labs . $ cd $PROJECT_DIR $ mkdir tooling-labs $ cd tooling-labs Now, use the maven archetype to create a new project in the tooling-labs directory: mvn archetype:generate \\ -DarchetypeGroupId=org.kie \\ -DarchetypeArtifactId=kie-kjar-archetype \\ -DarchetypeVersion=7.67.0.Final-redhat-00008 TIP: If you need to create a case project, you can use the parameter -DcaseProject=true . Maven will download the libraries, and once it finishes, it will confirm if you want to create the project using the default GAV (group:artifact:version). Type \"Y\" and press enter. You should get a new project named mybusinessapp . If you are in VSCode built-in terminal, you can open the project with: $ code -r mybusinessapp/ In VSCode, navigate through the project structure and confirm that it has a kie-deployment-descriptor.xml and a kmodule.xml . These are the files that Business Central needs to understand that this is a business project that should be packaged in a kjar. These files are also needed by KIE Server. Next Steps Now, let's author a DMN file, test it and deploy it to KIE Server.","title":"Getting Started"},{"location":"guided_exercises/tools/getting-started/#business-automation-projects-in-vscode","text":"To start working with business automation projects in VSCode, you'll need to install the VSCode Extension that allows you to work with BPMN, DMN and Test Scenarios through graphical editors.","title":"Business Automation projects in VSCode"},{"location":"guided_exercises/tools/getting-started/#installing-the-vscode-extension","text":"To install the extension in VSCode, open the extensions menu, and search for Business Automation. You should find the Red Hat Business Automation Bundle. Click on install. If this is the first time you are using VSCode, it would be interesting to also install the code command in path, so that you can open projects directly from the terminal. To do so, press cmd+shift+p (or ctrl+shift+p ) to launch VSCode Quick Open menu. And next, search for Instal code command in PATH :","title":"Installing the VSCode extension"},{"location":"guided_exercises/tools/getting-started/#create-new-a-project","text":"Let's create a new project using the maven archetype. This project should contain the structure and files that Business Central expects, so this project should be editable and authored in both VScode and Business Central. Now we will use the terminal. You can either use your terminal or use the built-in terminal In VScode. To use the terminal in VSCode you can press cmd+shift+p (or ctrl+shift+p ) to launch VSCode Quick Open menu. And next, open a new intergrated terminal : Next, in the terminal navigate to the directory where you would like to create the new project. Let's call it $PROJECT_DIR from now on. Create a new folder named tooling-labs . $ cd $PROJECT_DIR $ mkdir tooling-labs $ cd tooling-labs Now, use the maven archetype to create a new project in the tooling-labs directory: mvn archetype:generate \\ -DarchetypeGroupId=org.kie \\ -DarchetypeArtifactId=kie-kjar-archetype \\ -DarchetypeVersion=7.67.0.Final-redhat-00008 TIP: If you need to create a case project, you can use the parameter -DcaseProject=true . Maven will download the libraries, and once it finishes, it will confirm if you want to create the project using the default GAV (group:artifact:version). Type \"Y\" and press enter. You should get a new project named mybusinessapp . If you are in VSCode built-in terminal, you can open the project with: $ code -r mybusinessapp/ In VSCode, navigate through the project structure and confirm that it has a kie-deployment-descriptor.xml and a kmodule.xml . These are the files that Business Central needs to understand that this is a business project that should be packaged in a kjar. These files are also needed by KIE Server.","title":"Create new a project"},{"location":"guided_exercises/tools/getting-started/#next-steps","text":"Now, let's author a DMN file, test it and deploy it to KIE Server.","title":"Next Steps"},{"location":"guided_exercises/tools/introduction/","text":"Introduction This is a series of guided exercises that will allow you to experiment the authoring tools in VSCode and deployment in IBAMOE engine, KIE Server. Tooling Set In Red Hat PAM and DM you can author decisions using: Business Central (in IBAMOE) or Decision Central (in RHDM) A more business friendly UI; Business Automation VSCode Extension A developer IDE ( Visual Studio Code ) extension that allows the visualization and editing of BPMN, DMN and Test Scenarios inside VSCode. There is also a set of community tooling that's also available for use. All the tools below are backed by IBM and Red Hat: DMN FEEL Handbook A handbook for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine. Learn DMN in 15 minutes A guided tour in a website through the elements of DMN GitHub Chrome Extension A browser extension that allows you to visualize and edit BPMN, DMN and Test Scenario files directly in GitHub. Online Editors BPMN.new - A free online editor for business processes; DMN.new - A free online editor for decision models; PMML.new - A free online editor for scorecards; Business Modeler Hub Allows for the download of the: VSCode extension, GitHub Chrome Extension, and Desktop App","title":"Introduction"},{"location":"guided_exercises/tools/introduction/#introduction","text":"This is a series of guided exercises that will allow you to experiment the authoring tools in VSCode and deployment in IBAMOE engine, KIE Server.","title":"Introduction"},{"location":"guided_exercises/tools/introduction/#tooling-set","text":"In Red Hat PAM and DM you can author decisions using: Business Central (in IBAMOE) or Decision Central (in RHDM) A more business friendly UI; Business Automation VSCode Extension A developer IDE ( Visual Studio Code ) extension that allows the visualization and editing of BPMN, DMN and Test Scenarios inside VSCode. There is also a set of community tooling that's also available for use. All the tools below are backed by IBM and Red Hat: DMN FEEL Handbook A handbook for the FEEL expression language from the DMN specification, as implemented by the Drools DMN open source engine. Learn DMN in 15 minutes A guided tour in a website through the elements of DMN GitHub Chrome Extension A browser extension that allows you to visualize and edit BPMN, DMN and Test Scenario files directly in GitHub. Online Editors BPMN.new - A free online editor for business processes; DMN.new - A free online editor for decision models; PMML.new - A free online editor for scorecards; Business Modeler Hub Allows for the download of the: VSCode extension, GitHub Chrome Extension, and Desktop App","title":"Tooling Set"}]}